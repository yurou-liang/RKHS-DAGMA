{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        # Define the three layers\n",
    "        self.layer1 = nn.Linear(10, 5)  # Example sizes, adjust as needed\n",
    "        self.layer2 = nn.Linear(5, 3)   # Example sizes, adjust as needed\n",
    "        self.layer3 = nn.Linear(3, 1)   # Example sizes, adjust as needed\n",
    "\n",
    "        # Freeze the first layer by setting requires_grad to False for its parameters\n",
    "        for param in self.layer1.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn([100, 10])\n",
    "target = torch.randn([100, 10])\n",
    "\n",
    "model = MyModel()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "mse_loss = nn.MSELoss()\n",
    "optimizer.zero_grad()  # Clear gradients for the next train step\n",
    "outputs = model(x)  # Get model predictions\n",
    "loss = mse_loss(outputs, target)  # Compute the mean squared error loss\n",
    "loss.backward()  # Backpropagate the loss\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
