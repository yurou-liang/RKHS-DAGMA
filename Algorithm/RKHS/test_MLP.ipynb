{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from notears.locally_connected import LocallyConnected\n",
    "from lbfgsb_scipy import LBFGSBScipy\n",
    "from trace_expm import trace_expm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from locally_connected import LocallyConnected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotearsMLP(nn.Module):\n",
    "    def __init__(self, dims, bias=True):\n",
    "        super(NotearsMLP, self).__init__()\n",
    "        assert len(dims) >= 2\n",
    "        assert dims[-1] == 1 # output dim of last layer must be 1\n",
    "        d = dims[0]\n",
    "        self.dims = dims\n",
    "        # fc1: variable splitting for l1\n",
    "        self.fc1_pos = nn.Linear(d, d * dims[1], bias=bias)\n",
    "        self.fc1_neg = nn.Linear(d, d * dims[1], bias=bias)\n",
    "        \n",
    "        self.fc1_pos.weight.bounds = self._bounds()\n",
    "        self.fc1_neg.weight.bounds = self._bounds()\n",
    "        # fc2: local linear layers\n",
    "        layers = []\n",
    "        for l in range(len(dims) - 2):\n",
    "            layers.append(LocallyConnected(d, dims[l + 1], dims[l + 2], bias=bias))\n",
    "        self.fc2 = nn.ModuleList(layers)\n",
    "\n",
    "    def _bounds(self):\n",
    "        d = self.dims[0]\n",
    "        bounds = []\n",
    "        for j in range(d):\n",
    "            for m in range(self.dims[1]):\n",
    "                for i in range(d):\n",
    "                    if i == j:\n",
    "                        bound = (0, 0) # self loop not allowed\n",
    "                    else:\n",
    "                        bound = (0, None)\n",
    "                    bounds.append(bound)\n",
    "        return bounds\n",
    "\n",
    "    def forward(self, x):  # [n, d] -> [n, d]\n",
    "        x = self.fc1_pos(x) - self.fc1_neg(x)  # [n, d * m1] pass linear layer through input x\n",
    "        #print(x.shape)\n",
    "        x = x.view(-1, self.dims[0], self.dims[1])  # [n, d, m1]\n",
    "        for fc in self.fc2:\n",
    "            x = torch.sigmoid(x)  # [n, d, m1]\n",
    "            x = fc(x)  # [n, d, m2]\n",
    "            #print(x.shape)\n",
    "        x = x.squeeze(dim=2)  # [n, d]\n",
    "        return x\n",
    "\n",
    "    def h_func(self):\n",
    "        \"\"\"Constrain 2-norm-squared of fc1 weights along m1 dim to be a DAG\"\"\"\n",
    "        d = self.dims[0]\n",
    "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j * m1, i]\n",
    "        fc1_weight = fc1_weight.view(d, -1, d)  # [j, m1, i]\n",
    "        A = torch.sum(fc1_weight * fc1_weight, dim=1).t()  # [i, j]\n",
    "        h = trace_expm(A) - d  # (Zheng et al. 2018)\n",
    "        # A different formulation, slightly faster at the cost of numerical stability\n",
    "        # M = torch.eye(d) + A / d  # (Yu et al. 2019)\n",
    "        # E = torch.matrix_power(M, d - 1)\n",
    "        # h = (E.t() * M).sum() - d\n",
    "        return h\n",
    "\n",
    "    def l2_reg(self):\n",
    "        \"\"\"Take 2-norm-squared of all parameters\"\"\"\n",
    "        reg = 0.\n",
    "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j * m1, i]\n",
    "        reg += torch.sum(fc1_weight ** 2)\n",
    "        for fc in self.fc2:\n",
    "            reg += torch.sum(fc.weight ** 2)\n",
    "        return reg\n",
    "\n",
    "    def fc1_l1_reg(self):\n",
    "        \"\"\"Take l1 norm of fc1 weight\"\"\"\n",
    "        reg = torch.sum(self.fc1_pos.weight + self.fc1_neg.weight)\n",
    "        return reg\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def fc1_to_adj(self) -> np.ndarray:  # [j * m1, i] -> [i, j]\n",
    "        \"\"\"Get W from fc1 weights, take 2-norm over m1 dim\"\"\"\n",
    "        d = self.dims[0]\n",
    "        fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight  # [j * m1, i]\n",
    "        fc1_weight = fc1_weight.view(d, -1, d)  # [j, m1, i]\n",
    "        A = torch.sum(fc1_weight * fc1_weight, dim=1).t()  # [i, j]\n",
    "        W = torch.sqrt(A)  # [i, j]\n",
    "        W = W.cpu().detach().numpy()  # [i, j]\n",
    "        return W\n",
    "\n",
    "\n",
    "def squared_loss(output, target):\n",
    "    n = target.shape[0]\n",
    "    loss = 0.5 / n * torch.sum((output - target) ** 2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def dual_ascent_step(model, X, lambda1, lambda2, rho, alpha, h, rho_max):\n",
    "    \"\"\"Perform one step of dual ascent in augmented Lagrangian.\"\"\"\n",
    "    h_new = None\n",
    "    optimizer = LBFGSBScipy(model.parameters())\n",
    "    for idx, param in enumerate(model.parameters()):\n",
    "        print('layer:',idx,'shape:',param.shape)\n",
    "    #print(model.parameters().shape)\n",
    "    #print(\"model.parameters(): \", model.parameters().data())\n",
    "    X_torch = torch.from_numpy(X)\n",
    "    while rho < rho_max:\n",
    "        print(\"rho: \", rho)\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            X_hat = model(X_torch)\n",
    "            loss = squared_loss(X_hat, X_torch)\n",
    "            h_val = model.h_func()\n",
    "            print(\"h_val: \", h_val)\n",
    "            penalty = 0.5 * rho * h_val * h_val + alpha * h_val\n",
    "            l2_reg = 0.5 * lambda2 * model.l2_reg()\n",
    "            l1_reg = lambda1 * model.fc1_l1_reg()\n",
    "            primal_obj = loss + penalty + l2_reg + l1_reg\n",
    "            primal_obj.backward()\n",
    "            print('squared loss:', loss.item())\n",
    "            print('loss:', primal_obj.item())\n",
    "            print('penalty:', penalty.item())\n",
    "            return primal_obj\n",
    "        optimizer.step(closure)  # NOTE: updates model in-place\n",
    "        with torch.no_grad():\n",
    "            h_new = model.h_func().item()\n",
    "            print(\"h_new: \", h_new)\n",
    "        if h_new > 0.25 * h:\n",
    "            rho *= 10\n",
    "        else:\n",
    "            break\n",
    "    alpha += rho * h_new\n",
    "    return rho, alpha, h_new\n",
    "\n",
    "\n",
    "def notears_nonlinear(model: nn.Module,\n",
    "                      X: np.ndarray,\n",
    "                      lambda1: float = 0.,\n",
    "                      lambda2: float = 0.,\n",
    "                      max_iter: int = 100,\n",
    "                      h_tol: float = 1e-8,\n",
    "                      rho_max: float = 1e+16,\n",
    "                      w_threshold: float = 0.3):\n",
    "    rho, alpha, h = 1.0, 0.0, np.inf\n",
    "    for _ in range(max_iter):\n",
    "        rho, alpha, h = dual_ascent_step(model, X, lambda1, lambda2,\n",
    "                                         rho, alpha, h, rho_max)\n",
    "        if h <= h_tol or rho >= rho_max:\n",
    "            break\n",
    "    W_est = model.fc1_to_adj()\n",
    "    print(W_est)\n",
    "    W_est[np.abs(W_est) < w_threshold] = 0\n",
    "    W_est[np.abs(W_est) >= w_threshold] = 1\n",
    "    X_torch = torch.from_numpy(X)\n",
    "    output = model.forward(X_torch)\n",
    "    return W_est, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_true:  [[0. 1. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]]\n",
      "X.dtype:  float64\n",
      "layer: 0 shape: torch.Size([40, 4])\n",
      "layer: 1 shape: torch.Size([40])\n",
      "layer: 2 shape: torch.Size([40, 4])\n",
      "layer: 3 shape: torch.Size([40])\n",
      "layer: 4 shape: torch.Size([4, 10, 1])\n",
      "layer: 5 shape: torch.Size([4, 1])\n",
      "layer: 0 shape: torch.Size([40, 4])\n",
      "layer: 1 shape: torch.Size([40])\n",
      "layer: 2 shape: torch.Size([40, 4])\n",
      "layer: 3 shape: torch.Size([40])\n",
      "layer: 4 shape: torch.Size([4, 10, 1])\n",
      "layer: 5 shape: torch.Size([4, 1])\n",
      "layer: 0 shape: torch.Size([40, 4])\n",
      "layer: 1 shape: torch.Size([40])\n",
      "layer: 2 shape: torch.Size([40, 4])\n",
      "layer: 3 shape: torch.Size([40])\n",
      "layer: 4 shape: torch.Size([4, 10, 1])\n",
      "layer: 5 shape: torch.Size([4, 1])\n",
      "layer: 0 shape: torch.Size([40, 4])\n",
      "layer: 1 shape: torch.Size([40])\n",
      "layer: 2 shape: torch.Size([40, 4])\n",
      "layer: 3 shape: torch.Size([40])\n",
      "layer: 4 shape: torch.Size([4, 10, 1])\n",
      "layer: 5 shape: torch.Size([4, 1])\n",
      "layer: 0 shape: torch.Size([40, 4])\n",
      "layer: 1 shape: torch.Size([40])\n",
      "layer: 2 shape: torch.Size([40, 4])\n",
      "layer: 3 shape: torch.Size([40])\n",
      "layer: 4 shape: torch.Size([4, 10, 1])\n",
      "layer: 5 shape: torch.Size([4, 1])\n",
      "layer: 0 shape: torch.Size([40, 4])\n",
      "layer: 1 shape: torch.Size([40])\n",
      "layer: 2 shape: torch.Size([40, 4])\n",
      "layer: 3 shape: torch.Size([40])\n",
      "layer: 4 shape: torch.Size([4, 10, 1])\n",
      "layer: 5 shape: torch.Size([4, 1])\n",
      "layer: 0 shape: torch.Size([40, 4])\n",
      "layer: 1 shape: torch.Size([40])\n",
      "layer: 2 shape: torch.Size([40, 4])\n",
      "layer: 3 shape: torch.Size([40])\n",
      "layer: 4 shape: torch.Size([4, 10, 1])\n",
      "layer: 5 shape: torch.Size([4, 1])\n",
      "layer: 0 shape: torch.Size([40, 4])\n",
      "layer: 1 shape: torch.Size([40])\n",
      "layer: 2 shape: torch.Size([40, 4])\n",
      "layer: 3 shape: torch.Size([40])\n",
      "layer: 4 shape: torch.Size([4, 10, 1])\n",
      "layer: 5 shape: torch.Size([4, 1])\n",
      "layer: 0 shape: torch.Size([40, 4])\n",
      "layer: 1 shape: torch.Size([40])\n",
      "layer: 2 shape: torch.Size([40, 4])\n",
      "layer: 3 shape: torch.Size([40])\n",
      "layer: 4 shape: torch.Size([4, 10, 1])\n",
      "layer: 5 shape: torch.Size([4, 1])\n",
      "layer: 0 shape: torch.Size([40, 4])\n",
      "layer: 1 shape: torch.Size([40])\n",
      "layer: 2 shape: torch.Size([40, 4])\n",
      "layer: 3 shape: torch.Size([40])\n",
      "layer: 4 shape: torch.Size([4, 10, 1])\n",
      "layer: 5 shape: torch.Size([4, 1])\n",
      "layer: 0 shape: torch.Size([40, 4])\n",
      "layer: 1 shape: torch.Size([40])\n",
      "layer: 2 shape: torch.Size([40, 4])\n",
      "layer: 3 shape: torch.Size([40])\n",
      "layer: 4 shape: torch.Size([4, 10, 1])\n",
      "layer: 5 shape: torch.Size([4, 1])\n",
      "[[0.000e+00 2.911e+00 0.000e+00 0.000e+00]\n",
      " [1.657e-05 0.000e+00 0.000e+00 0.000e+00]\n",
      " [3.007e+00 1.860e-01 0.000e+00 0.000e+00]\n",
      " [2.108e-01 2.370e+00 0.000e+00 0.000e+00]]\n",
      "W_est [[0. 1. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "torch.set_default_dtype(torch.double)\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "import utils as ut\n",
    "ut.set_random_seed(123)\n",
    "\n",
    "n, d, s0, graph_type, sem_type = 100, 4, 3, 'ER', 'mim'\n",
    "B_true = ut.simulate_dag(d, s0, graph_type)\n",
    "#np.savetxt('W_true.csv', B_true, delimiter=',')\n",
    "print(\"W_true: \", B_true)\n",
    "\n",
    "X = ut.simulate_nonlinear_sem(B_true, n, sem_type)\n",
    "#np.savetxt('X.csv', X, delimiter=',')\n",
    "print(\"X.dtype: \", X.dtype)\n",
    "\n",
    "model = NotearsMLP(dims=[d, 10, 1], bias=True)\n",
    "W_est = notears_nonlinear(model, X, lambda1=0.01, lambda2=0.01)\n",
    "#np.savetxt('W_est.csv', W_est, delimiter=',')\n",
    "print(\"W_est\", W_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "3\n",
      "model.parameters():  <generator object Module.parameters at 0x00000183D91CB3E0>\n",
      "model.parameters():  <generator object Module.parameters at 0x00000183D91CB3E0>\n",
      "model.parameters():  <generator object Module.parameters at 0x00000183D91CB3E0>\n",
      "model.parameters():  <generator object Module.parameters at 0x00000183D91CB3E0>\n",
      "model.parameters():  <generator object Module.parameters at 0x00000183D91CB3E0>\n",
      "model.parameters():  <generator object Module.parameters at 0x00000183D91CB3E0>\n",
      "model.parameters():  <generator object Module.parameters at 0x00000183D91CB3E0>\n",
      "model.parameters():  <generator object Module.parameters at 0x00000183D91CB3E0>\n",
      "model.parameters():  <generator object Module.parameters at 0x00000183D91CB3E0>\n",
      "model.parameters():  <generator object Module.parameters at 0x00000183D91CB3E0>\n",
      "[[0.000e+00 3.445e+00 1.147e+00]\n",
      " [1.803e-05 0.000e+00 4.444e-02]\n",
      " [5.936e-05 4.221e-04 0.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "x = np.random.normal(10,2, 100)\n",
    "\n",
    "np.random.seed(24)\n",
    "l = np.random.normal(0,1, 100) \n",
    "\n",
    "\n",
    "np.random.seed(25)\n",
    "b = np.random.normal(0,1, 100) \n",
    "\n",
    "\n",
    "y = [8*x + l for x, l in zip(x, l)]\n",
    "z = [3*x + b for x, b in zip(x, b)]\n",
    "\n",
    "X = np.column_stack((x, y, z))\n",
    "n = X.shape[0]\n",
    "d = X.shape[1]\n",
    "\n",
    "print(n)\n",
    "print(d)\n",
    "\n",
    "model = NotearsMLP(dims=[d, 10, 1], bias=True)\n",
    "W_est = notears_nonlinear(model, X, lambda1=0.01, lambda2=0.01)\n",
    "# worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "2\n",
      "layer: 0 shape: torch.Size([10, 2])\n",
      "layer: 1 shape: torch.Size([10])\n",
      "layer: 2 shape: torch.Size([10, 2])\n",
      "layer: 3 shape: torch.Size([10])\n",
      "layer: 4 shape: torch.Size([2, 5, 7])\n",
      "layer: 5 shape: torch.Size([2, 7])\n",
      "layer: 6 shape: torch.Size([2, 7, 20])\n",
      "layer: 7 shape: torch.Size([2, 20])\n",
      "layer: 8 shape: torch.Size([2, 20, 5])\n",
      "layer: 9 shape: torch.Size([2, 5])\n",
      "layer: 10 shape: torch.Size([2, 5, 8])\n",
      "layer: 11 shape: torch.Size([2, 8])\n",
      "layer: 12 shape: torch.Size([2, 8, 10])\n",
      "layer: 13 shape: torch.Size([2, 10])\n",
      "layer: 14 shape: torch.Size([2, 10, 15])\n",
      "layer: 15 shape: torch.Size([2, 15])\n",
      "layer: 16 shape: torch.Size([2, 15, 1])\n",
      "layer: 17 shape: torch.Size([2, 1])\n",
      "rho:  1.0\n",
      "h_val:  tensor(0.0383, grad_fn=<SubBackward0>)\n",
      "squared loss: 627.3311259082818\n",
      "loss: 627.5852912149091\n",
      "penalty: 0.0007316723465475266\n",
      "h_val:  tensor(0.0380, grad_fn=<SubBackward0>)\n",
      "squared loss: 572.5653574672637\n",
      "loss: 572.8234880131163\n",
      "penalty: 0.00072380719263927\n",
      "h_val:  tensor(0.0372, grad_fn=<SubBackward0>)\n",
      "squared loss: 405.32268418866704\n",
      "loss: 405.6771571768222\n",
      "penalty: 0.0006930528504858909\n",
      "h_val:  tensor(0.0340, grad_fn=<SubBackward0>)\n",
      "squared loss: 892.822471005425\n",
      "loss: 894.2943846377996\n",
      "penalty: 0.0005766812784593784\n",
      "h_val:  tensor(0.0363, grad_fn=<SubBackward0>)\n",
      "squared loss: 315.7423938253893\n",
      "loss: 316.2710030292837\n",
      "penalty: 0.0006600532610317499\n",
      "h_val:  tensor(0.0350, grad_fn=<SubBackward0>)\n",
      "squared loss: 313.84527146228316\n",
      "loss: 314.43433783480504\n",
      "penalty: 0.0006137598115051457\n",
      "h_val:  tensor(0.0348, grad_fn=<SubBackward0>)\n",
      "squared loss: 312.9995553320063\n",
      "loss: 313.56597735768406\n",
      "penalty: 0.0006058131844626662\n",
      "h_val:  tensor(0.0342, grad_fn=<SubBackward0>)\n",
      "squared loss: 312.94482618602547\n",
      "loss: 313.5111825806647\n",
      "penalty: 0.0005835454693364812\n",
      "h_val:  tensor(0.0317, grad_fn=<SubBackward0>)\n",
      "squared loss: 312.9131312938043\n",
      "loss: 313.4788285859674\n",
      "penalty: 0.0005029333042590526\n",
      "h_val:  tensor(0.0303, grad_fn=<SubBackward0>)\n",
      "squared loss: 312.91311502337226\n",
      "loss: 313.4774704579538\n",
      "penalty: 0.000457607509194946\n",
      "h_val:  tensor(0.0249, grad_fn=<SubBackward0>)\n",
      "squared loss: 312.91305995192255\n",
      "loss: 313.4720999216382\n",
      "penalty: 0.00030918609983851943\n",
      "h_val:  tensor(0.0098, grad_fn=<SubBackward0>)\n",
      "squared loss: 312.91297544209357\n",
      "loss: 313.45139106869976\n",
      "penalty: 4.767779149810358e-05\n",
      "h_val:  tensor(0.0018, grad_fn=<SubBackward0>)\n",
      "squared loss: 312.9135929946811\n",
      "loss: 313.3753966184363\n",
      "penalty: 1.589736173649102e-06\n",
      "h_val:  tensor(0., grad_fn=<SubBackward0>)\n",
      "squared loss: 313.9298309773678\n",
      "loss: 314.1155038293033\n",
      "penalty: 0.0\n",
      "h_val:  tensor(0.0004, grad_fn=<SubBackward0>)\n",
      "squared loss: 312.9253912353156\n",
      "loss: 313.26739309562157\n",
      "penalty: 8.592133450281602e-08\n",
      "h_val:  tensor(0., grad_fn=<SubBackward0>)\n",
      "squared loss: 313.04286518331827\n",
      "loss: 313.271645865563\n",
      "penalty: 0.0\n",
      "h_val:  tensor(0.0001, grad_fn=<SubBackward0>)\n",
      "squared loss: 312.95254323856363\n",
      "loss: 313.2290095762641\n",
      "penalty: 9.463485055361341e-09\n",
      "h_val:  tensor(0., grad_fn=<SubBackward0>)\n",
      "squared loss: 312.92965560091653\n",
      "loss: 313.1751277297756\n",
      "penalty: 0.0\n",
      "h_val:  tensor(0., grad_fn=<SubBackward0>)\n",
      "squared loss: 312.91297996183675\n",
      "loss: 313.15899041383625\n",
      "penalty: 0.0\n",
      "h_val:  tensor(0., grad_fn=<SubBackward0>)\n",
      "squared loss: 312.91315422888005\n",
      "loss: 313.1489459252121\n",
      "penalty: 0.0\n",
      "h_val:  tensor(0., grad_fn=<SubBackward0>)\n",
      "squared loss: 312.96050889402113\n",
      "loss: 313.15747124384933\n",
      "penalty: 0.0\n",
      "h_val:  tensor(0., grad_fn=<SubBackward0>)\n",
      "squared loss: 312.9177629544249\n",
      "loss: 313.12610998068874\n",
      "penalty: 0.0\n",
      "h_val:  tensor(0., grad_fn=<SubBackward0>)\n",
      "squared loss: 312.9160707629595\n",
      "loss: 313.1184504553169\n",
      "penalty: 0.0\n",
      "h_val:  tensor(0., grad_fn=<SubBackward0>)\n",
      "squared loss: 312.9130561333558\n",
      "loss: 313.11398443606237\n",
      "penalty: 0.0\n",
      "h_val:  tensor(0., grad_fn=<SubBackward0>)\n",
      "squared loss: 312.91310077597177\n",
      "loss: 313.1132348114942\n",
      "penalty: 0.0\n",
      "h_val:  tensor(0., grad_fn=<SubBackward0>)\n",
      "squared loss: 312.9131067781973\n",
      "loss: 313.1130065248874\n",
      "penalty: 0.0\n",
      "h_new:  0.0\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "x1 = np.random.uniform(low=0, high=10, size=50)\n",
    "\n",
    "epsilon = np.random.normal(0,1, 50) \n",
    "\n",
    "y1 = [x1**2 + epsilon for x1, epsilon in zip(x1, epsilon)]\n",
    "\n",
    "y1 = np.array(y1)  # your y data here\n",
    "\n",
    "x2 = np.random.uniform(low=0, high=10, size=20)\n",
    "e = np.random.normal(10,8, 20) \n",
    "y2 = [x2**2 + e for x2, e in zip(x2, e)]\n",
    "y2 = np.array(y2)\n",
    "\n",
    "x3 = np.random.uniform(low=0, high=10, size=20)\n",
    "e3 = np.random.normal(-20,8, 20) \n",
    "y3 = [x3**2 + e3 for x3, e3 in zip(x3, e3)]\n",
    "y3 = np.array(y3)\n",
    "\n",
    "x = np.concatenate((x1, x2, x3))\n",
    "y = np.concatenate((y1, y2, y3))\n",
    "X = np.column_stack((x, y))\n",
    "\n",
    "n = X.shape[0]\n",
    "d = X.shape[1]\n",
    "\n",
    "print(n)\n",
    "print(d)\n",
    "\n",
    "model = NotearsMLP(dims=[d, 5, 7, 20, 5, 8, 10, 15, 1], bias=True)\n",
    "W_est, output = notears_nonlinear(model, X, lambda1=0.01, lambda2=0.01)\n",
    "print(W_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1f33362ff90>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0EAAAH5CAYAAABH+zXoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/SrBM8AAAACXBIWXMAAA9hAAAPYQGoP6dpAABCF0lEQVR4nO3df5hbZZ3//1eadqazpQm0pfOjkzqFZUUBFSiWgnHbdT4WP+qWKx1YKO4W9St7uQVmKOAWvQrXrkilqzDlZ61fF/a6ZArab1hcXX90awdHqAVRVFQK6LBMh5mhICS1yBQz+f5xk+lkJpk5mZyTc07O83FdudKcnGTuJKdJXrnv+32HstlsVgAAAAAQEDPcbgAAAAAAVBIhCAAAAECgEIIAAAAABAohCAAAAECgEIIAAAAABAohCAAAAECgEIIAAAAABMpMtxtQrpGREb344ouaO3euQqGQ280BAAAA4JJsNqtDhw6pqalJM2YU7+/xfQh68cUXFYvF3G4GAAAAAI/o6+tTc3Nz0et9H4Lmzp0ryTzQSCTicmsAAAAAuCWdTisWi41mhGJ8H4JyQ+AikQghCAAAAMCU02QojAAAAAAgUAhBAAAAAAKFEAQAAAAgUAhBAAAAAAKFEAQAAAAgUAhBAAAAAAKFEAQAAAAgUAhBAAAAAAKFEAQAAAAgUAhBAAAAAAKFEAQAAAAgUAhBAAAAAAKFEAQAAAAgUGa63QAAAAAA/pTJSD090sCA1NgoxeNSOOx2q6ZGCAIAAABQsmRSam+XDhw4uq25Wdq6VUok3GuXFQyHAwAAAFCSZFJqa8sPQJLU32+2J5PutMsqQhAAAAAAyzIZ0wOUzU68Lreto8Ps51WEIAAAAACW9fRM7AEaK5uV+vrMfl5FCAIAAABg2cCAvfu5gRAEAAAAwLLGRnv3cwPV4QAAAAAUVKgEdjxuqsD19xeeFxQKmevj8cq31yp6ggAAAABMkExKLS3SypXS2rXmvKVFeughUwZbMoFnrNzlzk5vrxdECAIAAACQZ6oS2JK0c6e0aFH+9c3NZrvX1wkKZbOFOrH8I51OKxqNKpVKKRKJuN0cAAAAwNcyGdPjU6wCXG64W2+vuTx+uJybPUBWswFzggAAAACMKqUE9ooV5uQ3DIcDAAAAMKoaSmBPhRAEAAAAYFQ1lMCeCiEIAAAAwKhcCezxld9yQiEpFvN2CeypEIIAAAAAjAqH/V8CeyqEIAAAAAB5Egl/l8CeCtXhAAAAAEyQSEirV3urBLZdCEEAAAAACgqH/VkCeyoMhwMAAAAQKIQgAAAAAIFCCAIAAAAQKIQgAAAAAIFCCAIAAAAQKIQgAAAAAIFCCAIAAAAQKIQgAAAAAIFCCAIAAAAQKIQgAAAAAIFCCAIAAAAQKIQgAAAAAIFCCAIAAAAQKIQgAAAAAIFCCAIAAAAQKIQgAAAAAIFCCAIAAAAQKIQgAAAAAIFCCAIAAAAQKIQgAAAAAIFCCAIAAAAQKIQgAAAAAIFCCAIAAAAQKIQgAAAAAIFCCAIAAAAQKIQgAAAAAIFCCAIAAAAQKIQgAAAAAIFCCAIAAAAQKIQgAAAAAIHiWAjKZDLatGmTlixZorq6Op144on6/Oc/r2w2O7pPNpvV9ddfr8bGRtXV1am1tVXPPvusU00CAAAAKiqTkbq7pR07zHkm43aLIDkYgm6++WbdfffduuOOO/Tb3/5WN998s7Zs2aLbb799dJ8tW7botttu07Zt27Rv3z7NmTNHq1at0htvvOFUswAAAICKSCallhZp5Upp7Vpz3tJitsNdoezYrhkbfeQjH1F9fb2+9rWvjW5bs2aN6urq9PWvf13ZbFZNTU26+uqrdc0110iSUqmU6uvrde+99+qiiy4qeL/Dw8MaHh4evZxOpxWLxZRKpRSJRJx4KAAAAEBJkkmprU0a/007FDLnO3dKiUTl21Xt0um0otHolNnAsZ6gc845R7t379YzzzwjSfrFL36hH//4x/rQhz4kSert7dXg4KBaW1tHbxONRrVs2TLt3bu36P1u3rxZ0Wh09BSLxZx6CAAAAEDJMhmpvX1iAJKObuvoYGicmxwLQRs3btRFF12kk08+WbNmzdLpp5+ujo4OXXLJJZKkwcFBSVJ9fX3e7err60evK+S6665TKpUaPfX19Tn1EAAAAICS9fRIBw4Uvz6blfr6zH5wx0yn7vgb3/iG7rvvPnV1demUU07Rk08+qY6ODjU1NWndunXTvt/a2lrV1tba2FIAAADAPgMD9u4H+zkWgq699trR3iBJOu200/S///u/2rx5s9atW6eGhgZJ0tDQkBobG0dvNzQ0pPe85z1ONQsAAABw1JivtrbsB/s5Nhzu9ddf14wZ+XcfDoc1MjIiSVqyZIkaGhq0e/fu0evT6bT27dun5cuXO9UsAAAAwFHxuNTcfLQIwnihkBSLmf3gDsd6gj760Y/qC1/4ghYvXqxTTjlFP//5z3XLLbfoE5/4hCQpFAqpo6NDN954o0466SQtWbJEmzZtUlNTk84//3ynmgUAAAA4KhyWtm411eFCofwCCblg1Nlp9oM7HCuRfejQIW3atEkPPvigXnrpJTU1Neniiy/W9ddfr5qaGklmsdQbbrhB27dv12uvvab3ve99uuuuu/RXf/VXlv+O1TJ4AAAAQCUlk6ZK3NgiCbGYCUCUx3aG1WzgWAiqFEIQAAAAvCqTMVXgBgbMHKB4nB4gJ1nNBo4NhwMAAACCLhyWVqxwuxUYz7HCCAAAAADgRYQgAAAAAIFCCAIAAAAQKIQgAAAAAIFCCAIAAAAQKIQgAAAAAIFCCAIAAAAQKIQgAAAAAIFCCAIAAAAQKIQgAAAAAIFCCAIAAAAQKIQgAAAAAIEy0+0GAAAAwHsyGamnRxoYkBobpXhcCofdbhVgD0IQAAAA8iSTUnu7dODA0W3NzdLWrVIi4V67ALswHA4AAACjkkmprS0/AElSf7/Znky60y7AToQgAAAASDJD4NrbpWx24nW5bR0dZj/AzwhBAAAAkGTmAI3vARorm5X6+sx+gJ8RggAAACDJFEGwcz/AqwhBAAAAkGSqwNm5H+BVhCAAAABIMmWwm5ulUKjw9aGQFIuZ/QA/IwQBAABAklkHaOtW8+/xQSh3ubPTf+sFZTJSd7e0Y4c5p7ADWCcIAAAAoxIJaefOwusEdXb6b52gYmse3XqrtGABi8EGVSibLVQE0T/S6bSi0ahSqZQikYjbzQEAAKgKmYypAufnkJBb88jKt10Wg60OVrMBIQgAAABVJ5ORWlomL/k9Vm64386dBCE/s5oNmBMEAACAqjPVmkfjsRhssBCCAAAAUHWms5YRi8EGByEIAAAAVaectYxYDLb6EYIAAABQdaZa82gyLAZb/QhBAAAAJWDNGX+YbM2jYlgMNjgIQQAAABYlk6bi2MqV0tq15rylxWyH9+TWPFq0aOp9/bwYrJOqNfRTIhsAAMCCYmvOUFrZ+8aveXTwoLRhQ371uFjMn4vBOqnYQrNeXk+JdYIAAABsMtWaM6GQ+XLY20svgl9Uw2KwTvJr6CcEAQAA2KS72wx9m8qePdKKFU63BnCWn0M/i6UCAADYxGrJZEoroxpMtdBsNaynNNPtBgAAAO8L+tAhqyWTKa2MahCE0E9PEAAAmBQV0aZec4bSyqgmQQj9hCAAAFBUbnL0+KEx/f1me1CC0GRrzlBaGdUmCKGfEAQAAArKZEx53EIllHLbOjqqZ92QqRRbc6a52buVsqpJta5X40VBCP2EIAAAUFAQJkeXKpGQnn/eVIHr6jLnvb0EIKcxJLPyqj30UxgBAAAUFITJ0dMRDlMGu5KKrVeTG5JZDV/IvSqRkFavrs6iKIQgAABQUBAmR8PbphqSGQqZIZmrV1fHF3MvqtbQz3A4AABQUBAmR8PbGJIJpxCCAABAQUGYHA1vY0gmnEIIAgAARVX75Gh4G0My4ZRQNltolKV/pNNpRaNRpVIpRSIRt5sDAEBVymSqc3I0vC2TMVXg+vsLzwsKhUwg7+3leIRhNRtQGAEAAEypWidHw9tyQzLb2kzgGRuEGJKJcjAcDgAAAJ7FkEw4gZ4gAAAAeFo1r1cDdxCCAAAA4HkMyYSdGA4HAAAAIFAIQQAAAAAChRAEAAAAIFCYEwQAAICysI4U/IYQBAAAgGlLJqX2dunAgaPbmpvN+j6Ur4ZXEYIAAABgyfgen5dfli68MH8RU0nq7zcLnLKOD7yKEAQAAIApFerxCYcnBiDJbAuFpI4Os74PQ+PgNRRGAAAAwKSSSdOzMzYASaZnqJhsVurrMz1HgNfQEwQAAIA8Y4e9LVwoXXll4R4fKwYG7G0bYAdCEAAAAEYVGvZWjsZGe+4HsBMhCAAAAJKODnubbq/PWKGQqRIXj5d/X4DdmBMEAAAAZTKmB8iuACRJnZ0URYA3EYIAAACgnp7pD4EbH3SamymPDW9jOBwAAACmVcAg1+OzY4d0/PFH1w+Kx+kBgrc52hPU39+vj33sY5o/f77q6up02mmn6ac//eno9dlsVtdff70aGxtVV1en1tZWPfvss042CQAAAAVMp4BBrsfnggukFSukiy825wQgeJ1jIejVV1/Vueeeq1mzZum73/2ufvOb3+jLX/6yjjvuuNF9tmzZottuu03btm3Tvn37NGfOHK1atUpvvPGGU80CAABAAfG4CTW53p3xcoUO/ud/pK4uac8eqbeXIW/wp1A2a8f0t4k2btyoRx55RD1FVsjKZrNqamrS1VdfrWuuuUaSlEqlVF9fr3vvvVcXXXSRpb+TTqcVjUaVSqUUiURsaz8AAEDQ5KrDSfkFEnLBiHk+8Dqr2cCxnqBvfetbWrp0qS644AItXLhQp59+ur761a+OXt/b26vBwUG1traObotGo1q2bJn27t1b9H6Hh4eVTqfzTgAAAChfImGCzqJF+dspdIBq41gI+v3vf6+7775bJ510kr7//e/r05/+tK688kr9x3/8hyRpcHBQklRfX593u/r6+tHrCtm8ebOi0ejoKRaLOfUQAAAAAieRkJ5/3gx3Y9gbqpVj1eFGRka0dOlS3XTTTZKk008/XU899ZS2bdumdevWTft+r7vuOm3YsGH0cjqdJggBAADYKBw2BQ6AauVYT1BjY6Pe+c535m17xzveoRdeeEGS1NDQIEkaGhrK22doaGj0ukJqa2sViUTyTgAAAABglWMh6Nxzz9X+/fvztj3zzDN629veJklasmSJGhoatHv37tHr0+m09u3bp+XLlzvVLAAAAAAB59hwuKuuukrnnHOObrrpJl144YV67LHHtH37dm3fvl2SFAqF1NHRoRtvvFEnnXSSlixZok2bNqmpqUnnn3++U80CAAAAEHCOhaCzzjpLDz74oK677jr967/+q5YsWaLOzk5dcsklo/t85jOf0eHDh3XZZZfptdde0/ve9z5973vf0+zZs51qFgAAAICAc2ydoEphnSAAAAAAkgfWCQIAAAAALyIEAQAAAAgUQhAAAACAQCEEAQAAAAgUx6rDAQAAAKgOmYzU0yMNDEiNjVI8LoXDbrdq+ghBAAAAAIpKJqX2dunAgaPbmpulrVulRMK9dpWD4XAAAAAACkompba2/AAkSf39Znsy6U67ykUIAgAAADBBJmN6gAqtKprb1tFh9vMbQhAAAACACXp6JvYAjZXNSn19Zj+/IQQBAAAAmGBgwN79vIQQBAAAAGCCxkZ79/MSQhAAAACACeJxUwUuFCp8fSgkxWJmP78hBAEAAACYIBw2ZbCliUEod7mz05/rBRGCAAAAABSUSEg7d0qLFuVvb2422/26ThCLpQIAAAAoKpGQVq82VeAGBswcoHjcnz1AOYQgAAAAAJMKh6UVK9xuhX0YDgcAAAAgUAhBAAAAAAKFEAQAAAAgUAhBAAAAAAKFEAQAAAAgUAhBAAAAAAKFEAQAAAAgUAhBAAAAAAKFxVIBAABKlMlIPT3SwIDU2CjF42YxSQD+QAgCAAAoQTIptbdLBw4c3dbcLG3dKiUS7rULgHUMhwMAALAomZTa2vIDkCT195vtyaQ77QJQGkIQAACABZmM6QHKZidel9vW0WH2A+BthCAAAAALenom9gCNlc1KfX1mPwDeRggCAACwYGDA3v0AuIcQBAAAYEFjo737AXAPIQgAAMCCeNxUgQuFCl8fCkmxmNkPgLcRggAAACwIh00ZbGliEMpd7uxkvSDADwhBAAAAFiUS0s6d0qJF+dubm8121gkC/IHFUgEAAEqQSEirV5sqcAMDZg5QPE4PEOAnhCAAAIAShcPSihVutwLAdDEcDgAAAECg0BMEAACAPJkMw/1Q3QhBAAAAGJVMSu3t0oEDR7c1N5vKeBR+QLVgOBwAABWWyUjd3dKOHeY8k3G7RYCRTEptbfkBSJL6+832ZNKddgF2IwQBAFBByaTU0iKtXCmtXWvOW1r4cgn3ZTKmByibnXhdbltHB6Ed1YEQBABAhfArO7ysp2fisTlWNiv19Zn9AL8jBAEAUAFB/pWd4X/+MDBg736AlxGCAACogKD+ys7wP/9obLR3P8DLCEEAAFRAEH9lZ/ifv8TjpgpcKFT4+lBIisXMfoDfEYIAAKiAoP3KHuThf34VDpsy2NLEIJS73NnJekGoDoQgAAAqIGi/sgd1+J/fJRLSzp3SokX525ubzXbWCUK1YLFUAAAqIPcre1ubCTxje0iq8Vf2IA7/qxaJhLR6tQmoAwOmdzIer55jE5AIQQAAVEzuV/b29vxekuZmE4Cq6Vf2oA3/qzbhsLRihdutAJwTymYLjdb1j3Q6rWg0qlQqpUgk4nZzAACYUiZT/b+yZzKmClx/f+F5QaGQCX+9vdX32AG4x2o2oCcIAIAKC8Kv7EEb/gfAXyiMAAAAHMEkewBeRU8QAABwDJPsAXgRIQgAADgqCMP/APgLIQgAgIAJQmEGAJgMIQgAgABJJguX6N66lTk6AIKDwggAAAREMmmqtY0NQJIpY93WZq4HgCAgBAEAEACZjOkBKrRmT25bR4fZDwCqHSEIAIAA6OmZ2AM0VjYr9fWZ/QCg2hGCAAAIgIEBe/cDAD+jMAIAAAHQ2GjvfpMptfoc1eoAVBo9QQAABEA8bqrAhUKFrw+FpFjM7FeOZFJqaZFWrpTWrjXnLS3Fiy6Uuj8A2IEQBABAAITDpgy2NDEI5S53dpbXA1Nq9Tmq1QFwCyEIAICASCSknTulRYvytzc3m+3lrBNUavU5qtUBcFPFQtAXv/hFhUIhdXR0jG574403tH79es2fP1/HHHOM1qxZo6GhoUo1CQCAwEkkpOefl/bskbq6zHlvb/kLpXZ3l1Z9jmp1ANxUkcIIjz/+uL7yla/oXe96V972q666St/5znf0zW9+U9FoVJdffrkSiYQeeeSRSjQLAIBACoelFSvsu79kUvrUp6ztm6s+R7U6AG5yvCfoj3/8oy655BJ99atf1XHHHTe6PZVK6Wtf+5puueUW/c3f/I3OPPNM3XPPPXr00Uf1k5/8xOlmAQAAG+Tm9fzhD9b2z1Wfq2S1OgAYz/EQtH79en34wx9Wa2tr3vYnnnhCb775Zt72k08+WYsXL9bevXuL3t/w8LDS6XTeCQAAVN5k83rGG199rlLV6gCgEEdD0P3336+f/exn2rx584TrBgcHVVNTo2OPPTZve319vQYHB4ve5+bNmxWNRkdPsVjM7mYDAAALpprXM97Y6nOVqFYHAMU4FoL6+vrU3t6u++67T7Nnz7btfq+77jqlUqnRU19fn233DQAArLM6X+eYYwpXn3OyWh0ATMaxwghPPPGEXnrpJZ1xxhmj2zKZjH70ox/pjjvu0Pe//30dOXJEr732Wl5v0NDQkBoaGoreb21trWpra51qNgAAsMjqfJ3Zs6XVqwtfl0iY63p6TKhqbDRD4OgBAuAkx0LQBz7wAf3qV7/K2/bxj39cJ598sv75n/9ZsVhMs2bN0u7du7VmzRpJ0v79+/XCCy9o+fLlTjULAADYJB6XFiyQXn558v1eftmEnGIV6eyuVgcAU3EsBM2dO1ennnpq3rY5c+Zo/vz5o9s/+clPasOGDZo3b54ikYiuuOIKLV++XGeffbZTzQIAADYJh6WPfczM3ZkKpa4BeElF1gkq5tZbb9WMGTO0Zs0aDQ8Pa9WqVbrrrrvcbBIAwKcyGYZUuWH1amshiFLXALwklM1aKWzpXel0WtFoVKlUSpFIxO3mAABckEyaUs1jK5U1N5vqY0yud1YmI7W0SP39hUtlh0LmtejtJZQCcJ7VbOD4OkEAADgpt1jn+FLN/f1mezLpTruCglLXAPyIEAQA8K3JFuvMbevoMPvBOZS6BuA3rs4JAgCgHFMt1pnNSn19k1cmgz0odQ3ATwhBAADfslpxjMpklUGpawB+QQgCAPiW1YpjVCabHJX1AAQNc4IAAL4Vj5t5J+Mn5OeEQlIsZvZDYcmkqe62cqW0dq05b2mhoASA6kYIAgD4FpXJykNlPQBBRQgCAPgalcmmh8p6AIKMOUEAAN+jMlnpqKwHIMgIQQCAqkBlstJQWQ9AkBGCAAAIICrr2Y8qe4B/MCcIAIAAorKevaiyB/gLIQgAgACisp59qLIH+A8hCACAgKKyXvmosgf4E3OCAAAIMCrrlYcqe4A/EYIAAAg4KutNH1X2AH8iBAEA4BNUH/MequwB/kQIAgDAB5JJM/dk7NCr5mZT3GD83B3CUuXkquz19xeeFxQKmeupsgd4C4URAADwuFKqj1GqubKosgf4EyEIAACHZDJSd7e0Y4c5n06FsFKqj1Gq2R1U2QP8J5TNFnpb9Y90Oq1oNKpUKqVIJOJ2cwAAkFTa8LXJdHeb3pyp/M//SJdeWrxSWW5YVm8vvRJOYRgi4D6r2YA5QQAA2CzXIzP+Z8Zcj8zOndbLUlutKtbdTalmt1FlD/APQhAAADaaavhaKCRddpl05ZUmFOUU6yWyu6oYpZoBgDlBAADYysrima+8kh+ApOLzdnLVx8ZPus8JhaRYzHoPBKWaAYAQBACArabb0zK+yEGO1epjK1ZYC0uUagYAQhAAALYqp6dl7LydsaxUH6NUMwBYRwgCAMBGUw1fs6JQb1IiIT3/vLRnj9TVZc57e/PnEFGqGQCsoTACAAA2yvXItLWZIDSdhSiK9SZZqT6WSFivPAcAQcU6QQAAOKDYOkF/+pP0hz8UDkes5QMA5WGdIAAAXJBbMHN4WLr3XrPtpZeO9sg89FDhXiLm7QBA5TAnCAAAmySTUkuLtHKltHat1NoqXXqpVFtrhrGFw8zbAQAvYDgcAAA2SCZND8/4T9VcD8/4gJPrMWLeDgDYx2o2IAQBAFCmTMb0ABVbJJW5PgBQGVazAcPhAAAoU09P8QAkFV//BwDgDkIQAABlKrSuTzn7AQCcRQgCAKBMxdb1me5+AABnEYIAAChTPG7m/OSKIIwXCkmxmNkPAOA+QhAAAGUKh6WtW82/xwehal7/J5ORurulHTvMeSbjdosAwBpCEAAANgja+j/j10RaudJcTibdbhkATI0S2QAqhnVR4CXFjsdyj9MgHOelrokEAJXCOkEAPCWZlNrb88sINzebIUR8WUKlFTseL77YDO2yepwGIfCMx5pIALyMEATAM/jVGF6STEpr1ljfv9hxGtRg391thr5NZc8eacUKp1sDAPlYLBWAJ2Qy5otioZ9bcts6OphQjcrIZKTLLivtNoWO01ywH98b0t9vtlfzvBjWRAJQDQhBABzV01N82IxkvmD29Zn9ACdlMqZC2yuvlH7bscdp0IM9ayIBqAaEIACO4ldjeEGuktk115R3PwMDBHvWRAJQDWa63QDfe/ZZ6dCh4tfPnSuddFLl9i+njdNp269/LR0+XHj/VEpqapIWL5ZeeGHifnPmmOuK3XehtuTuZ+xtJ2vjVHJ/p1D7xrZx7lxzuVibfvc76eBB6ZhjJl5fVyedeKJ0yikTn+fcbf/0p/z9GxqO/v1Ct8vddmybxz8nkz0X5R5XhW4/tj1j2nLSIelv9YIkqU9H2xfTC5ojs39EKb3711HpvreuHBw052OfB6uPbTJWXu+c8ceX3ceelXYWU+7fKvR/d3DQfLN/6SVzeeFCaf588+/nnjPdJ5FI/nbJbK+tNc9JNGrup9DxPPb/0vi2l/IeVeg9J/c36+rMKRrNu/rn3Sn9f/9vVBdoUFlJC3RQi9WnGh2RJB1RjVKKqk6v60/6C6UU0WM6O+94laRDmqsTMtKrjx7S3445fnPqNaiQpEE1KKKUar4ZlfrH7JA7vn73O3OeO741bp/c//npPkeT7ffKK/mvn5X3ESnvPsOSvrH2Bd2xxdzuRD2rqFKq1bCiSqkme0Rnv22Wwlccaz4HZs2STjst/zgo1p5ij8srrByDJ5448T3Ca49jMrt2Sb/8Zf7/Y8m8VsPD5n3g7LOPPsZKvDeW855YwueVpfsrhdPv5U7btct8Jox/X8/5859N+yvxueiErM+lUqmspGwqlar8H3/mmWzW/Og3+emZZyqzfzlt/MEPnGlbKSe77nuy52M6z42dJ6vPs123K/RclHtcufG8lfM6V7LdpbapnHZO92954fUb23a736Oq5cRzVPi5cFOlPpvc5NQxVM5jL+c9sZzXrNzXy+n3cqfZdSy48PisZgOGw5VjsnRfaD+n9y/1urFyv/5a/VtW77cUdt23Hc+bU6w+z3bdrtBjLPf5ceN5K6TUdlSi3Xb8DaePXy+8fmPbYPd7VLXgOTrKC8esVLnPJjc5dQyV89jLeU+sxN8t9/ZePS7sOha8+vjEnCAAAAAAAUMIAgAAABAohCAAAAAAgUIIAgAAABAohCAAAAAAgUIIAgAAABAohKBy5BaRs7qf0/uXet1YCxeW9res3m8p7LpvO543p1h9nu26XaHHWO7z48bzVkip7ahEu+34G04fv154/ca2we73qGrBc3SUF45ZqXKfTW5y6hgq57GX855Yib9b7u29elzYdSx49fFJCmWz2azbjShHOp1WNBpVKpVSJBKpfANKXQ3Y6f3LaeN02lZo5eycVEpqajKrCI9flVzKX2HY6uO2e2Xq3N8p1L6xbSywanpem373O+ngQemYYyZen1tBPLcK/NjHlrvt2JWY6+qOriQ/dvX48c+JlZXep1hBO5ORfv5z6eWXpQULpNNPl8LhKW475vYTnodiK3C/8II5H78tt38qJUWjR68bHDTnY58Hq49tMlZe75zxx1clVkUf385iyv1bb/3fzRw6rP37pddelRZmB7Vk1gGFX35rbYiFC6X5882/n3vOrBYfieRvl8z22lrznESjE1cWzx3PY/8vjWv79+94Vp+94pDGfxiF3jq/6fa5WnX5Sdr39Wd109//WnOU/9rVa1B1+pPeUJ1eV53+TyKqPXukP7xqro8opVnzovp/PjKod79L5v9qX5905IjZoabGtP3116W/+AvzOM8+u/Aq8lLxY2jscTv+mJaOHl+/+93R/cYb+39+LLvex195Jf/1s/I+IhX9//7oo9L37npWUaVUq2FFlVKNjuiIZimlYxVVSkc0S6evPU1n/N+Gifc/vj3FHpdXFPvcyx33uff7QseOlx7HZHbtkn75y/z/x5J5rYaHJ/7/qMR7YznviaV+Xk11f6Vw+r3cabt2mfWCxr+v5/z5z6b9lfhcLIHVbEAIAgIqmZTa26UDB45ua26Wtm6VEgn32oXK8Mrrn8lILS357RgrFDLt6u2VvvENae3aqe+zq0u68EKpp0caGJAaG6V4/K2AD1t1d0srV06935490ooVTrcGAAhBACaRTEptbdL4//2ht35637mTIFTNvPT6l/IlWuILt9fkQmx//8TjScoPsYRQAJVgNRswJwgImEzG9AAU+sKS29bRYfZD9fHa6z8wYH2/eNx8oc6FtfFCISkWM/uhMsJh03soTXxdcpc7OwlAALyHEAQETE9P8aFHkvki3Ndn9kP18drr39hofT++cHtTImF6Dxctyt/e3EyvMgDvIgQBAVPKL++oPl57/Uvt3eELtzclEtLzz5uhiF1d5ry3l9cDgHfNdLsBACqrlF/eUX289vrnenfa2kzgGTtMr1jvTiIhrV5N4QOvCYeZiwXAPyiMgIrKZPji4jYmMgebV1//QtXqYjETgOhNAABYRWEEeE4yab58rVxpytyuXGkuJ5NutyxYmFcRbF59/RlOBQCoJEdD0ObNm3XWWWdp7ty5Wrhwoc4//3zt378/b5833nhD69ev1/z583XMMcdozZo1GhoacrJZcEGuJO/4Cdn9/WY7QaiymFcRbFO9/qtXm9LVO3aY80pVissNp7r4YnNOEAcAOMXR4XDnnXeeLrroIp111ln685//rM9+9rN66qmn9Jvf/EZz3lo5+9Of/rS+853v6N5771U0GtXll1+uGTNm6JFHHrH0NxgO532lLIbIl57KYnhisBV6/R96yBuLqAIAMB2eXCz14MGDWrhwoR5++GG9//3vVyqV0vHHH6+uri61tbVJkp5++mm94x3v0N69e3X22WdPeZ+EIO9jRXHAH7y0iCoAANPhyTlBqVRKkjRv3jxJ0hNPPKE333xTra2to/ucfPLJWrx4sfbu3VvwPoaHh5VOp/NO8DavleQFMJHXFlEFAMBJFQtBIyMj6ujo0LnnnqtTTz1VkjQ4OKiamhode+yxefvW19drcHCw4P1s3rxZ0Wh09BSLxZxuOsrktZK8ACayuojq7bdXfq4QAAB2q1gIWr9+vZ566indf//9Zd3Pddddp1QqNXrq6+uzqYVwSqmLIfpdJuPOpHKgHFZ7Yq+6iuqOAAD/q0gIuvzyy/Xtb39be/bsUXNz8+j2hoYGHTlyRK+99lre/kNDQ2poaCh4X7W1tYpEInkneJtXS/I6gTLg8Kvp9MRS3REA4FeOhqBsNqvLL79cDz74oH74wx9qyZIledefeeaZmjVrlnbv3j26bf/+/XrhhRe0fPlyJ5uGCgtCSWbKgMPPpuqxLSSbNSfmCgEA/MbR6nD/9E//pK6uLj300EN6+9vfPro9Go2qrq5OkimR/d///d+69957FYlEdMUVV0iSHn30UUt/g+pw/lKtJZkpA45qUKw6nBV+qu5Yre9DAADr2WCmk424++67JUkrxn0y3nPPPbr00kslSbfeeqtmzJihNWvWaHh4WKtWrdJdd93lZLPgotxiiNXG6qTynp7qfPzwr/GB4OqrpS99qfT76e+3v21OSCZZBwkA4HAIstLJNHv2bN1555268847nWwK4CjKgMOPCgWC6faIHDxoT5ucVKynKzdktVqG5gIAplbRdYKAakUZcPhNsTls053b8/zzZTfJUayDBAAYixAElKhQCeyglQGHv00WCKarq8vbAaKUIasAgOpHCAJKUKwE9kMPBacMOCrHqTWnpgoE03HwoLcDBENWAQBjEYIAi6YqgS1VfxlwVI6Ta0459UXfywGCIasAgLEcLZFdCZTIRiWUUgJbovwuylNsAn+uV7HcUN3dbUKV3bxcJjv3f7i/v/AwQMrYA0B1sJoNCEGABVa/NHr5SyD8oRJrTk0VCCRz32OH342/bHebKiEXLqX8x21XuAQAuM9qNmA4HGAB8wlQKZWYwB8OTz6HLRQy85D27DEFD/bske6//+h14/eX/DHnLZFgyCoAwHB0nSCgWjCfAJVSqcCdCwSFFg7t7CwcCErd34sSCWn1aoasAkDQMRwOsID5BKiUSg+9zGRKCwSl7g8AQCUxJwiwGfMJUAkEbgAApo85QYDNmE+ASphqvo7k7fk3Tq1tBACAnegJAkrEcCBUQjI5cf5NLObt+TeF2tzcbEKdV9sMAKguDIcDAJ/zU+B2em0jAACsIAQBACqiEmsbAQBgBXOCAAAVUYm1jQAAsBMhCABQFhYTBgD4DSEIAFAWFhMGAPjNTLcbAABeVKwogZ+KFVRKPG7m/Ey1tlE8Xvm2AQBQCCEIAMYpVur54ovN+jeUgM6XW9uorc0EnkKLCXt5bSMAQPAwHA4AxsiVeh4/0f/AAenf/m3i9v5+s38yWbk2ehGLCQMA/IQS2QDwlqlKPRdDCeijGC4IAHCT1WzAcDgAeMtUpZ6LGVsCesUK25vlK+EwzwEAwPsIQQACp1hvRbklnCkBDQCAPxCCAARKsaIHW7eWX8KZEtAAAPgDhREAeEYmI3V3mwps3d3msp2KFT3IFTc4eNAEolxFM6tCISkWowQ0AAB+QQgC4AnJpClKsHKltHatOW9psa/qWiZjeoAKlYLJbbv6aunWW82/rQYhSkADAOA/hCAArpuqh8aOIDRV0YNccYMFCwqXeo7FpGuvNT1FY1ECGgAA/2FOEMpGSVyUY6oemlBI6uiQVq8u77iyWrRgYMAsirp6deHjevNmjncAAPyOEISyTDbJnF/GYYXVHppyy09bLVqQ269YqWdKQLuPH14AAOViOJxNnJ7Q7UWVGMKE6ldKD0054vHJix5Q3MAfnJ47BgAIBkKQDYL4oWxlknlHRzDCIMpTag/NdIXDpodSmhiEKG7gD/zwAgCwCyGoTEH9UC5lCBMwmUr20CQShYseUNzA+/jhBQBgJ0JQGYL8oVypIUyoflP10GSz0po1JlDb8X8pkZCef17as0fq6jLnvb0EIK/jhxcAgJ0IQWUI8odypYYwIRiK9dDMeOsdqrPT3mGmueIGF19szhkC53388AIAsBMhqAxB/lBmkjkkewuCjO2h6eg4ev9jVfswUxTHDy8AADsRgsoQ5A9lJpnDiYIg4bAJzjt3Fr6+2oeZojh+eAEA2IkQVIagfygzyTy4nCwIEuRhpn7m9DIB/PACALATIagMfCgzyTyInC4IEuRhpn5VqWUC+OEFAGCXUDZb6KuMf6TTaUWjUaVSKUUiEVfakEyaL4Vjf72OxUwA4kMZ1aa723zJncqePabogNfuH/bK9QqO/yTJ/RDkRDjJZExP4MCAGW4cj1f3j00AAOusZgNCkE34UEZQ7Nhhfu2fSleXqb5WqkzG9CL09xfubQqFzC//vb38H3Nb7rUqNnyR1woAUGlWs8HMCrapquVK7gLVzumCILlhpm1tR9cJygnKMFO/KGX+Fu+PAAAvYU4QgJJUoiAIcz/8gflbAAC/oicIsFEQhkVWqqcmkZBWr67+59PPgrxMAADA35gTBNikUIGM5mYTGKqx54KCIGD+FgDAayiMAFSQGxWyvCAIPV+YXO7Ylwr3ClbrsQ8A8CZCEFAhdlTIIkzAz+gVBAB4BdXhgAopt0JW0IbRofowfwsA4DeEIKBM5VTIKjaMrr/fbGcoEfyCZQIAAH5CCAJU3nC06VbIymRMD1ChAanZrBlG19FhfmGfrC0MpQMAACgN6wQh8JJJM6dn5Upp7Vpz3tJitlsx3XVzShlG51TbAQAAgogQhEDLDUcbH0Zyw9GshIncujmFTLZuTrkLTdrRdidkMlJ3t7RjhznPZNxpBwAAQDGEIATWVMPRJDMczeqX+HnzCm8rNq+nnIUm7W67XeiZAgAAfkAIQmDZMRxNOtoj88orE68rtC1nusPo7Gy7nbzaMwUAADAeIQiO8+rwqHKHo0mT98hIR4sbFHrMY4fRjQ9Ckw2jm6pN09mvXF7tmQIAACiEEARHeXl4VDnD0XLK7ZFJJMxwuUWL8rc3N09eHtuOttvJiz1TAAAAxVAiG47x+ho4ueFo/f2FezBCIXN9oeFoOeX0yORKWw8PS/fea7a99NLUZa4zGXOaN0/6wx8K72Ol7XbyWs8U3OeX0u1+aScAwF70BMEW44e8HTni/eFR5QxHy5luj8z4HrLWVunSS6XaWrPgZLG/mbtda+vkAchK2+3ktZ4puMvLPcBj+aWdAAD7hbLZYrMZ/CGdTisajSqVSikSibjdnEBKJk3gGTscasEC6eWXp77tnj3urzJfqP2xmAkRU/VUZTLmS9NUvUm9vUcDSbEeslx4KdZDVux241ltu52m8zygOk33+K40v7QTAFAaq9mAEBQgTgz7sPrFvJiuLunii4tfX6zNdj+Wcu4v9xxI+c9DoS9TubBQbP5MsbAw1e0kaf586YEHJu9JclIpzwOq03SP70rzSzsBAKWzmg0YDhcQTgz7mKoymhWTDY8q1ubPfMb+xxIOm/Bw8cWlh4hSihtMt4DAVLeTTDnucNi9L23TLfKA6uGXAhl+aScAwDkURggApwoUWPliXsxUE/eLtfnAAenf/m3i/m4XW0gkpNWrp+5Nmm4BAb8UHrD6PKA6+eU49Us7AQDOIQRVuanWb8mtY7N6delfVKf7BWGqifvT6WEq97HYIdebNJnpFhDwU+EBK88DqpNfjlO/tBMA4ByGw7mkUguIOjnsw+oXhOOPz7881fCo6fYw+WEIS64s9/hqdDmhkClsML6HbLq3AyrJL8epX9oJAHAOIcgFlSzL6uSwD6tfJA4cMFXgurrMeW/v5EPWyh2C4uUhLNMty21HOW/AaX45Tv3STgCAczwRgu688061tLRo9uzZWrZsmR577DG3m+SY3FyX8T0duTktdgchJ4d9WP0iUVNTWtGBcoegeH0Iy3QLCFB4AH7gl+PUL+0EADjD9RLZDzzwgP7hH/5B27Zt07Jly9TZ2alvfvOb2r9/vxYuXDjl7f1UItuNsqyVWL+lnHV2ptPmYvxW1na6ZblZ4R5+4Jfj1C/tBABY45t1gpYtW6azzjpLd9xxhyRpZGREsVhMV1xxhTZu3Djl7f0Ugrq7zdC3qdi9gGip67dM50uB3V8kirW5GNaiAQAAgC/WCTpy5IieeOIJtba2jm6bMWOGWltbtXfv3oK3GR4eVjqdzjv5hVtlWUsZ9jHd+UrlrLNTSptjMenaa03bp3osAAAAQCGulsh++eWXlclkVF9fn7e9vr5eTz/9dMHbbN68Wf/yL/9SiebZzs2yrFbWb3FqPSEn2rx5M0NYAAAAMD2uDod78cUXtWjRIj366KNavnz56PbPfOYzevjhh7Vv374JtxkeHtbw8PDo5XQ6rVgs5ovhcJWYn1Nu2yo5XwkAAACwky+Gwy1YsEDhcFhDQ0N524eGhtTQ0FDwNrW1tYpEInknv/ByWVYn1xMCAAAAvMTVEFRTU6MzzzxTu3fvHt02MjKi3bt35/UMVROvlmV1a74SAAAAUGmuzgmSpA0bNmjdunVaunSp3vve96qzs1OHDx/Wxz/+cbeb5hgr83Mqzc35SgDgN5TWBgB/cz0E/d3f/Z0OHjyo66+/XoODg3rPe96j733vexOKJVSbXDU1r4jHTW/UVPOV4vHKtw0AvKTQ2mjNzWa4MxUqAcAfXF8nqFx+WifI60pdTwgAgqZYFU3eJwHAG3xRGAHe4tX5SgDgBZmM6QEq9NNhbltHh9kPAOBtrg+Hg7d4cb4SAHhBKVU0vTTcGQAwESEIE3htvhIAeAFVNAGgejAcDgAAC6iiCQDVgxAEAIAFuSqa4xe7zgmFpFiMKpoA4AeEIAAALAiHTRlsaWIQyl3u7GQOJQD4ASEIAACLqKIJANWBwggAAJSAKpoA4H+EIAAASkQVTQDwN4bDAQAAAAgUQhAAAACAQGE4HIDAymSY1wEAQBARggAEUjIptbdLBw4c3dbcbEogU+ELAIDqxnA4AIGTTEptbfkBSJL6+832ZNKddgEAgMogBAEIlEzG9ABlsxOvy23r6DD7AQCA6kQIAhAoPT0Te4DGymalvj6zHwAAqE6EIACBMjBg734AAMB/CEEAAqWx0d79AACA/xCCAARKPG6qwIVCha8PhaRYzOwHAACqEyEIgKsyGam7W9qxw5w7XZAgHDZlsKWJQSh3ubOT9YIAAKhmhCAArkkmpZYWaeVKae1ac97S4nyJ6kRC2rlTWrQof3tzs9nOOkEAAFS3UDZbqFCsf6TTaUWjUaVSKUUiEbebA8Ci3Fo949+Bcr0xlQgjmYypAjcwYOYAxeP0AAEA4GdWswEhCEDFZTKmx6dYqepQyPTK9PYSSgAAgHVWswHD4QBUHGv1AAAANxGCAFQca/UAAAA3EYIAVBxr9QAAADcRggBUHGv1AAAANxGCAFQca/UAAAA3EYIAuIK1egAAgFtmut0AAMGVSEirV7NWDwAAqCxCEDAGi2dWXjgsrVjhdisAAECQEIKAtySTUnt7/vo1zc1m7gpDswAAAKoHc4IAmQDU1jZxAc/+frM9mXSnXQAAALAfIQiBl8mYHqBsduJ1uW0dHWY/oNplMlJ3t7RjhznnuAcAVCNCEAKvp2diD9BY2azU12f2A6pZMim1tEgrV0pr15rzlhZ6QgEA1YcQhMAbGLB3P8CPGBIKAAgSQhACr7HR3v0Av2FIKAAgaAhBCLx43FSBC4UKXx8KSbGY2Q+oRgwJBQAEDSEIgRcOmzLY0sQglLvc2cl6QaheDAkFAAQNIQiQWQdo505p0aL87c3NZjvrBKGaMSQUABA0oWy20Chw/0in04pGo0qlUopEIm43Bz6XyZghPwMD5gtfPE4PEKpfJmOqwPX3F54XFAqZHwR6e/n/AADwNqvZYGYF2wR4XjgsrVjhdiuAysoNCW1rM4FnbBBiSCgAoBoxHA4AwJBQAECg0BMEAJBkgs7q1QwJBQBUP0IQAGAUQ0IBAEHAcDgAAAAAgUIIAgAAABAohCAAAAAAgUIIAgAAABAohCAAAAAAgUIIAgAAABAohCAAAAAAgUIIAgAAABAohCAAAAAAgUIIAgAAABAohCAAAAAAgUIIAgAAABAohCAAAAAAgUIIAgAAABAoM91uAKzJZKSeHmlgQGpslOJxKRx2u1UAAACA/xCCfCCZlNrbpQMHjm5rbpa2bpUSCffaBXsRdAEAACqD4XAel0xKbW35AUiS+vvN9mTSnXbBXsmk1NIirVwprV1rzltaeH0BAACcQAjysEzG9ABlsxOvy23r6DD7wb8IugAAAJVFCPKwnp6JX4zHymalvj6zH/yJoAsAAFB5joSg559/Xp/85Ce1ZMkS1dXV6cQTT9QNN9ygI0eO5O33y1/+UvF4XLNnz1YsFtOWLVucaI5vDQzYux+8h6ALAABQeY4URnj66ac1MjKir3zlK/rLv/xLPfXUU/rUpz6lw4cP60tf+pIkKZ1O64Mf/KBaW1u1bds2/epXv9InPvEJHXvssbrsssucaJbvNDbaux+8h6ALAABQeY6EoPPOO0/nnXfe6OUTTjhB+/fv19133z0agu677z4dOXJE//7v/66amhqdcsopevLJJ3XLLbcQgt4Sj5sqcP39hYdLhULm+ni88m2DPQi6AAAAlVexOUGpVErz5s0bvbx37169//3vV01Nzei2VatWaf/+/Xr11VeL3s/w8LDS6XTeqVqFw6YMtmQCz1i5y52dlFH2s1zQHf/65oRCUixG0AUAALBTRULQc889p9tvv13/+I//OLptcHBQ9fX1efvlLg8ODha9r82bNysajY6eYrGYM432iERC2rlTWrQof3tzs9nOOkH+RtAFAACovJJC0MaNGxUKhSY9Pf3003m36e/v13nnnacLLrhAn/rUp8pu8HXXXadUKjV66uvrK/s+vS6RkJ5/XtqzR+rqMue9vQSgakHQBQAAqKyS5gRdffXVuvTSSyfd54QTThj994svvqiVK1fqnHPO0fbt2/P2a2ho0NDQUN623OWGhoai919bW6va2tpSml0VwmFpxQq3WwGnJBLS6tWmCtzAgJkDFI/TAwQAAOCEkkLQ8ccfr+OPP97Svv39/Vq5cqXOPPNM3XPPPZoxI7/Tafny5frc5z6nN998U7NmzZIk7dq1S29/+9t13HHHldIsoCoQdAEAACrDkTlB/f39WrFihRYvXqwvfelLOnjwoAYHB/Pm+qxdu1Y1NTX65Cc/qV//+td64IEHtHXrVm3YsMGJJgEAAACAJIdKZO/atUvPPfecnnvuOTU3N+ddl32r1nM0GtUPfvADrV+/XmeeeaYWLFig66+/nvLYAAAAABwVymYLrUDjH+l0WtFoVKlUSpFIxO3mAAAAAHCJ1WxQsXWCAAAAAMALCEEAAAAAAoUQBAAAACBQCEEAAAAAAoUQBAAAACBQCEEAAAAAAoUQBAAAACBQCEEAAAAAAoUQBAAAACBQCEEAAAAAAmWm2w0oVzablSSl02mXWwIAAADATblMkMsIxfg+BB06dEiSFIvFXG4JAAAAAC84dOiQotFo0etD2alikseNjIzoxRdf1Ny5cxUKhRz/e+l0WrFYTH19fYpEIo7/PVQvjiXYieMJduFYgp04nmAnK8dTNpvVoUOH1NTUpBkzis/88X1P0IwZM9Tc3FzxvxuJRPjPDFtwLMFOHE+wC8cS7MTxBDtNdTxN1gOUQ2EEAAAAAIFCCAIAAAAQKISgEtXW1uqGG25QbW2t202Bz3EswU4cT7ALxxLsxPEEO9l5PPm+MAIAAAAAlIKeIAAAAACBQggCAAAAECiEIAAAAACBQggCAAAAECiEIAAAAACBQggqwZ133qmWlhbNnj1by5Yt02OPPeZ2k+BDmzdv1llnnaW5c+dq4cKFOv/887V//363m4Uq8MUvflGhUEgdHR1uNwU+1d/fr4997GOaP3++6urqdNppp+mnP/2p282CD2UyGW3atElLlixRXV2dTjzxRH3+858XRYlhxY9+9CN99KMfVVNTk0KhkP7zP/8z7/psNqvrr79ejY2NqqurU2trq5599tmS/gYhyKIHHnhAGzZs0A033KCf/exneve7361Vq1bppZdecrtp8JmHH35Y69ev109+8hPt2rVLb775pj74wQ/q8OHDbjcNPvb444/rK1/5it71rne53RT41Kuvvqpzzz1Xs2bN0ne/+1395je/0Ze//GUdd9xxbjcNPnTzzTfr7rvv1h133KHf/va3uvnmm7VlyxbdfvvtbjcNPnD48GG9+93v1p133lnw+i1btui2227Ttm3btG/fPs2ZM0erVq3SG2+8YflvsE6QRcuWLdNZZ52lO+64Q5I0MjKiWCymK664Qhs3bnS5dfCzgwcPauHChXr44Yf1/ve/3+3mwIf++Mc/6owzztBdd92lG2+8Ue95z3vU2dnpdrPgMxs3btQjjzyinp4et5uCKvCRj3xE9fX1+trXvja6bc2aNaqrq9PXv/51F1sGvwmFQnrwwQd1/vnnSzK9QE1NTbr66qt1zTXXSJJSqZTq6+t177336qKLLrJ0v/QEWXDkyBE98cQTam1tHd02Y8YMtba2au/evS62DNUglUpJkubNm+dyS+BX69ev14c//OG89yigVN/61re0dOlSXXDBBVq4cKFOP/10ffWrX3W7WfCpc845R7t379YzzzwjSfrFL36hH//4x/rQhz7kcsvgd729vRocHMz7zItGo1q2bFlJ38tnOtG4avPyyy8rk8movr4+b3t9fb2efvppl1qFajAyMqKOjg6de+65OvXUU91uDnzo/vvv189+9jM9/vjjbjcFPvf73/9ed999tzZs2KDPfvazevzxx3XllVeqpqZG69atc7t58JmNGzcqnU7r5JNPVjgcViaT0Re+8AVdcsklbjcNPjc4OChJBb+X566zghAEuGj9+vV66qmn9OMf/9jtpsCH+vr61N7erl27dmn27NluNwc+NzIyoqVLl+qmm26SJJ1++ul66qmntG3bNkIQSvaNb3xD9913n7q6unTKKafoySefVEdHh5qamjie4AkMh7NgwYIFCofDGhoayts+NDSkhoYGl1oFv7v88sv17W9/W3v27FFzc7PbzYEPPfHEE3rppZd0xhlnaObMmZo5c6Yefvhh3XbbbZo5c6YymYzbTYSPNDY26p3vfGfetne84x164YUXXGoR/Ozaa6/Vxo0bddFFF+m0007T3//93+uqq67S5s2b3W4afC733bvc7+WEIAtqamp05plnavfu3aPbRkZGtHv3bi1fvtzFlsGPstmsLr/8cj344IP64Q9/qCVLlrjdJPjUBz7wAf3qV7/Sk08+OXpaunSpLrnkEj355JMKh8NuNxE+cu65504o1//MM8/obW97m0stgp+9/vrrmjEj/2tmOBzWyMiISy1CtViyZIkaGhryvpen02nt27evpO/lDIezaMOGDVq3bp2WLl2q9773vers7NThw4f18Y9/3O2mwWfWr1+vrq4uPfTQQ5o7d+7o+NVoNKq6ujqXWwc/mTt37oS5ZHPmzNH8+fOZY4aSXXXVVTrnnHN000036cILL9Rjjz2m7du3a/v27W43DT700Y9+VF/4whe0ePFinXLKKfr5z3+uW265RZ/4xCfcbhp84I9//KOee+650cu9vb168sknNW/ePC1evFgdHR268cYbddJJJ2nJkiXatGmTmpqaRivIWZKFZbfffnt28eLF2Zqamux73/ve7E9+8hO3mwQfklTwdM8997jdNFSBv/7rv862t7e73Qz41H/9139lTz311GxtbW325JNPzm7fvt3tJsGn0ul0tr29Pbt48eLs7NmzsyeccEL2c5/7XHZ4eNjtpsEH9uzZU/C70rp167LZbDY7MjKS3bRpU7a+vj5bW1ub/cAHPpDdv39/SX+DdYIAAAAABApzggAAAAAECiEIAAAAQKAQggAAAAAECiEIAAAAQKAQggAAAAAECiEIAAAAQKAQggAAAAAECiEIAAAAQKAQggAAAAAECiEIAAAAQKAQggAAAAAEyv8PC1piVuX73l0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_hat = output[:, 1].detach().numpy()\n",
    "plt.figure(figsize=(10, 6))  # Optional: specifies the figure size\n",
    "plt.scatter(x, y, label='y1', color='blue', marker='o')  # Plot x vs. y1\n",
    "plt.scatter(x, y_hat, label='y2', color='red', marker='s') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_hat = output[:, 0].detach().numpy()\n",
    "\n",
    "plt.scatter(x, x_hat, color='red', marker='s')  # Plot x vs. y_hat\n",
    "\n",
    "plt.legend()  # Show legend to differentiate between actual and predicted values\n",
    "plt.title('Actual vs Predicted Values')  # Optional: Adds a title to the plot\n",
    "plt.xlabel('x-axis label')  # Optional: Label for the x-axis\n",
    "plt.ylabel('x_hat-axis label')  # Optional: Label for the y-axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer: 0 shape: torch.Size([10, 2])\n",
      "layer: 1 shape: torch.Size([10])\n",
      "layer: 2 shape: torch.Size([10, 2])\n",
      "layer: 3 shape: torch.Size([10])\n",
      "layer: 4 shape: torch.Size([2, 5, 8])\n",
      "layer: 5 shape: torch.Size([2, 8])\n",
      "layer: 6 shape: torch.Size([2, 8, 1])\n",
      "layer: 7 shape: torch.Size([2, 1])\n",
      "rho:  1.0\n",
      "h_val:  tensor(0.6767, grad_fn=<SubBackward0>)\n",
      "squared loss: 102.1714303016425\n",
      "loss: 102.48883815844975\n",
      "penalty: 0.22898724017121966\n",
      "h_val:  tensor(0.3515, grad_fn=<SubBackward0>)\n",
      "squared loss: 88.9680517732698\n",
      "loss: 89.1183559752392\n",
      "penalty: 0.06177015485116566\n",
      "h_val:  tensor(0.0705, grad_fn=<SubBackward0>)\n",
      "squared loss: 68.45433322241699\n",
      "loss: 68.60629243018496\n",
      "penalty: 0.0024837579586243414\n",
      "h_val:  tensor(0.0757, grad_fn=<SubBackward0>)\n",
      "squared loss: 68.3206113368099\n",
      "loss: 68.46994004618053\n",
      "penalty: 0.0028626941079034057\n",
      "h_val:  tensor(0.1138, grad_fn=<SubBackward0>)\n",
      "squared loss: 68.10081534047131\n",
      "loss: 68.25338810883282\n",
      "penalty: 0.006478544672912043\n",
      "h_val:  tensor(0.4402, grad_fn=<SubBackward0>)\n",
      "squared loss: 67.62409568229872\n",
      "loss: 67.87555486870559\n",
      "penalty: 0.09688215618725773\n",
      "h_val:  tensor(0.6825, grad_fn=<SubBackward0>)\n",
      "squared loss: 67.26130892870401\n",
      "loss: 67.65906201701259\n",
      "penalty: 0.2328710535832704\n",
      "h_val:  tensor(0.7647, grad_fn=<SubBackward0>)\n",
      "squared loss: 66.87204506049669\n",
      "loss: 67.34677732038092\n",
      "penalty: 0.29240219821014213\n",
      "h_val:  tensor(0.6552, grad_fn=<SubBackward0>)\n",
      "squared loss: 66.5113521038782\n",
      "loss: 66.92673489989623\n",
      "penalty: 0.21462124513643488\n",
      "h_val:  tensor(0.3029, grad_fn=<SubBackward0>)\n",
      "squared loss: 65.84991182638254\n",
      "loss: 66.19135383827482\n",
      "penalty: 0.04588621340708687\n",
      "h_val:  tensor(0.6065, grad_fn=<SubBackward0>)\n",
      "squared loss: 64.93949600829495\n",
      "loss: 65.42493599589594\n",
      "penalty: 0.1839467983957454\n",
      "h_val:  tensor(1.3310, grad_fn=<SubBackward0>)\n",
      "squared loss: 70.81578575164762\n",
      "loss: 71.86777369443777\n",
      "penalty: 0.8858057826139836\n",
      "h_val:  tensor(0.0852, grad_fn=<SubBackward0>)\n",
      "squared loss: 64.87504361731844\n",
      "loss: 65.14792319072689\n",
      "penalty: 0.0036316266926762356\n",
      "h_val:  tensor(0.2737, grad_fn=<SubBackward0>)\n",
      "squared loss: 64.07766674705738\n",
      "loss: 64.41178514402755\n",
      "penalty: 0.03746330362581632\n",
      "h_val:  tensor(0.4985, grad_fn=<SubBackward0>)\n",
      "squared loss: 63.69389406634362\n",
      "loss: 64.13405619310387\n",
      "penalty: 0.12427394467083569\n",
      "h_val:  tensor(1.3059, grad_fn=<SubBackward0>)\n",
      "squared loss: 61.943930357198184\n",
      "loss: 63.187374194523414\n",
      "penalty: 0.8527225638724393\n",
      "h_val:  tensor(1.6543, grad_fn=<SubBackward0>)\n",
      "squared loss: 58.67422608219462\n",
      "loss: 60.583274812057105\n",
      "penalty: 1.3683738148052031\n",
      "h_val:  tensor(1.0450, grad_fn=<SubBackward0>)\n",
      "squared loss: 57.24298117860077\n",
      "loss: 58.390820618610206\n",
      "penalty: 0.5460293895291355\n",
      "h_val:  tensor(0.4296, grad_fn=<SubBackward0>)\n",
      "squared loss: 56.49247797493848\n",
      "loss: 57.21382403234168\n",
      "penalty: 0.09225731071779572\n",
      "h_val:  tensor(1.0899, grad_fn=<SubBackward0>)\n",
      "squared loss: 52.57925170456972\n",
      "loss: 54.069262872345185\n",
      "penalty: 0.5939054589666035\n",
      "h_val:  tensor(0.5688, grad_fn=<SubBackward0>)\n",
      "squared loss: 51.165551261356654\n",
      "loss: 52.39246239893546\n",
      "penalty: 0.16176877709261708\n",
      "h_val:  tensor(1.1132, grad_fn=<SubBackward0>)\n",
      "squared loss: 49.59157791091652\n",
      "loss: 51.55440587578382\n",
      "penalty: 0.6196171086375191\n",
      "h_val:  tensor(6.0216, grad_fn=<SubBackward0>)\n",
      "squared loss: 47.38779560848755\n",
      "loss: 67.6808305186314\n",
      "penalty: 18.129993523225554\n",
      "h_val:  tensor(0.8961, grad_fn=<SubBackward0>)\n",
      "squared loss: 48.67549620951033\n",
      "loss: 50.662420250047305\n",
      "penalty: 0.4014753285027955\n",
      "h_val:  tensor(0.2343, grad_fn=<SubBackward0>)\n",
      "squared loss: 47.49138844933051\n",
      "loss: 49.45784147446254\n",
      "penalty: 0.027437344143207282\n",
      "h_val:  tensor(1.0595, grad_fn=<SubBackward0>)\n",
      "squared loss: 44.94803893971948\n",
      "loss: 48.32354193776729\n",
      "penalty: 0.5612207780702417\n",
      "h_val:  tensor(1.5665, grad_fn=<SubBackward0>)\n",
      "squared loss: 42.35217367787061\n",
      "loss: 47.3776400032034\n",
      "penalty: 1.2268946789004904\n",
      "h_val:  tensor(1.4861, grad_fn=<SubBackward0>)\n",
      "squared loss: 41.46166482885866\n",
      "loss: 46.83702377029185\n",
      "penalty: 1.1041802254596325\n",
      "h_val:  tensor(0.1975, grad_fn=<SubBackward0>)\n",
      "squared loss: 41.69721791770644\n",
      "loss: 45.8719311741321\n",
      "penalty: 0.019511355080406974\n",
      "h_val:  tensor(0.1862, grad_fn=<SubBackward0>)\n",
      "squared loss: 39.73150087010095\n",
      "loss: 45.00613106506222\n",
      "penalty: 0.017333256373218402\n",
      "h_val:  tensor(0.0423, grad_fn=<SubBackward0>)\n",
      "squared loss: 37.72096423216211\n",
      "loss: 44.5229798061156\n",
      "penalty: 0.0008944283274885887\n",
      "h_val:  tensor(0.2336, grad_fn=<SubBackward0>)\n",
      "squared loss: 37.5813235405923\n",
      "loss: 44.424778614333626\n",
      "penalty: 0.027295700868926465\n",
      "h_val:  tensor(0.5578, grad_fn=<SubBackward0>)\n",
      "squared loss: 37.4172693183743\n",
      "loss: 44.34446577887741\n",
      "penalty: 0.15559816533897616\n",
      "h_val:  tensor(0.4171, grad_fn=<SubBackward0>)\n",
      "squared loss: 37.60694935851582\n",
      "loss: 44.1628922701357\n",
      "penalty: 0.08700183785728066\n",
      "h_val:  tensor(0.3749, grad_fn=<SubBackward0>)\n",
      "squared loss: 37.73725860014192\n",
      "loss: 43.94244473557237\n",
      "penalty: 0.070275508238256\n",
      "h_val:  tensor(0.4596, grad_fn=<SubBackward0>)\n",
      "squared loss: 37.75829416015153\n",
      "loss: 43.46379467215421\n",
      "penalty: 0.10559819270851234\n",
      "h_val:  tensor(0.2307, grad_fn=<SubBackward0>)\n",
      "squared loss: 37.969198710118796\n",
      "loss: 43.21348992274026\n",
      "penalty: 0.02660545011374899\n",
      "h_val:  tensor(0.2910, grad_fn=<SubBackward0>)\n",
      "squared loss: 37.62353257473884\n",
      "loss: 43.12607591076182\n",
      "penalty: 0.0423284251137165\n",
      "h_val:  tensor(0.1516, grad_fn=<SubBackward0>)\n",
      "squared loss: 37.460261526690196\n",
      "loss: 43.06489586795841\n",
      "penalty: 0.011497571300923088\n",
      "h_val:  tensor(0.0698, grad_fn=<SubBackward0>)\n",
      "squared loss: 37.371264865852446\n",
      "loss: 43.01612843704464\n",
      "penalty: 0.0024347777468528954\n",
      "h_val:  tensor(0.1239, grad_fn=<SubBackward0>)\n",
      "squared loss: 37.193433455162555\n",
      "loss: 42.88808955415795\n",
      "penalty: 0.007678842893063839\n",
      "h_val:  tensor(1.4897, grad_fn=<SubBackward0>)\n",
      "squared loss: 36.66254080886525\n",
      "loss: 43.64106362940463\n",
      "penalty: 1.1096158574040442\n",
      "h_val:  tensor(0.2802, grad_fn=<SubBackward0>)\n",
      "squared loss: 37.03115592798982\n",
      "loss: 42.81033327034954\n",
      "penalty: 0.03925114257818896\n",
      "h_val:  tensor(0.6497, grad_fn=<SubBackward0>)\n",
      "squared loss: 36.835491807806235\n",
      "loss: 42.78309900873014\n",
      "penalty: 0.21107998456603916\n",
      "h_val:  tensor(0.3427, grad_fn=<SubBackward0>)\n",
      "squared loss: 36.948354080272495\n",
      "loss: 42.745585123100426\n",
      "penalty: 0.05870686863165459\n",
      "h_val:  tensor(0.3959, grad_fn=<SubBackward0>)\n",
      "squared loss: 36.974793147159474\n",
      "loss: 42.64250446352649\n",
      "penalty: 0.07838187421862408\n",
      "h_val:  tensor(0.3497, grad_fn=<SubBackward0>)\n",
      "squared loss: 36.93066371150413\n",
      "loss: 42.477750688757055\n",
      "penalty: 0.06114753876332897\n",
      "h_val:  tensor(0.3823, grad_fn=<SubBackward0>)\n",
      "squared loss: 36.49948759303819\n",
      "loss: 41.95349310161378\n",
      "penalty: 0.07307089706020034\n",
      "h_val:  tensor(0.1395, grad_fn=<SubBackward0>)\n",
      "squared loss: 36.08356981955669\n",
      "loss: 41.599462652932566\n",
      "penalty: 0.009735538575598763\n",
      "h_val:  tensor(0.0967, grad_fn=<SubBackward0>)\n",
      "squared loss: 35.85035542133115\n",
      "loss: 41.51604657135752\n",
      "penalty: 0.0046718189180817524\n",
      "h_val:  tensor(0.0718, grad_fn=<SubBackward0>)\n",
      "squared loss: 35.59207727486059\n",
      "loss: 41.46953232859924\n",
      "penalty: 0.0025791445007894436\n",
      "h_val:  tensor(0.0629, grad_fn=<SubBackward0>)\n",
      "squared loss: 35.41618525380758\n",
      "loss: 41.44593782576394\n",
      "penalty: 0.0019804120411366154\n",
      "h_val:  tensor(0.1025, grad_fn=<SubBackward0>)\n",
      "squared loss: 35.08535278005467\n",
      "loss: 41.40506788053708\n",
      "penalty: 0.005254823499603169\n",
      "h_val:  tensor(0.1715, grad_fn=<SubBackward0>)\n",
      "squared loss: 34.81931608555986\n",
      "loss: 41.36145604472937\n",
      "penalty: 0.014700404210835757\n",
      "h_val:  tensor(0.2411, grad_fn=<SubBackward0>)\n",
      "squared loss: 34.52158761139717\n",
      "loss: 41.261178771646655\n",
      "penalty: 0.02905920175219971\n",
      "h_val:  tensor(0.2443, grad_fn=<SubBackward0>)\n",
      "squared loss: 34.27032801730316\n",
      "loss: 41.090897573856104\n",
      "penalty: 0.029834602819474223\n",
      "h_val:  tensor(0.3049, grad_fn=<SubBackward0>)\n",
      "squared loss: 33.95162647343733\n",
      "loss: 40.84262210371222\n",
      "penalty: 0.046474331361607855\n",
      "h_val:  tensor(0.5425, grad_fn=<SubBackward0>)\n",
      "squared loss: 33.38551309708983\n",
      "loss: 40.49713612720632\n",
      "penalty: 0.1471667119200345\n",
      "h_val:  tensor(0.2972, grad_fn=<SubBackward0>)\n",
      "squared loss: 32.87955586144145\n",
      "loss: 40.17890368697435\n",
      "penalty: 0.044170896550291244\n",
      "h_val:  tensor(0.4081, grad_fn=<SubBackward0>)\n",
      "squared loss: 32.44265438581484\n",
      "loss: 39.912375338265456\n",
      "penalty: 0.0832763932246537\n",
      "h_val:  tensor(0.0915, grad_fn=<SubBackward0>)\n",
      "squared loss: 32.24053302458531\n",
      "loss: 39.38882913248183\n",
      "penalty: 0.004183969040576946\n",
      "h_val:  tensor(0.1457, grad_fn=<SubBackward0>)\n",
      "squared loss: 32.67201278830741\n",
      "loss: 39.20799067600292\n",
      "penalty: 0.01062096940223962\n",
      "h_val:  tensor(0.1075, grad_fn=<SubBackward0>)\n",
      "squared loss: 32.923921829743996\n",
      "loss: 39.1569318571324\n",
      "penalty: 0.005780427110791004\n",
      "h_val:  tensor(0.0898, grad_fn=<SubBackward0>)\n",
      "squared loss: 32.94847562915034\n",
      "loss: 39.12232292588002\n",
      "penalty: 0.004030749623118963\n",
      "h_val:  tensor(0.1996, grad_fn=<SubBackward0>)\n",
      "squared loss: 32.89029733076814\n",
      "loss: 38.90171314237647\n",
      "penalty: 0.019926714886914915\n",
      "h_val:  tensor(0.7801, grad_fn=<SubBackward0>)\n",
      "squared loss: 32.693183030393214\n",
      "loss: 38.83404300481276\n",
      "penalty: 0.30425435253492616\n",
      "h_val:  tensor(0.3091, grad_fn=<SubBackward0>)\n",
      "squared loss: 32.8007787475697\n",
      "loss: 38.77678561994058\n",
      "penalty: 0.047766444942919845\n",
      "h_val:  tensor(0.6796, grad_fn=<SubBackward0>)\n",
      "squared loss: 32.493880485067955\n",
      "loss: 38.59126335930196\n",
      "penalty: 0.23091731727717518\n",
      "h_val:  tensor(0.3972, grad_fn=<SubBackward0>)\n",
      "squared loss: 32.28269969784292\n",
      "loss: 38.20839783520442\n",
      "penalty: 0.07888273253610831\n",
      "h_val:  tensor(0.5838, grad_fn=<SubBackward0>)\n",
      "squared loss: 31.561768222414425\n",
      "loss: 37.596074595482705\n",
      "penalty: 0.17041744962978664\n",
      "h_val:  tensor(0.0980, grad_fn=<SubBackward0>)\n",
      "squared loss: 31.179932089395688\n",
      "loss: 37.018141857250306\n",
      "penalty: 0.004797998767817116\n",
      "h_val:  tensor(0.1433, grad_fn=<SubBackward0>)\n",
      "squared loss: 31.1251557671787\n",
      "loss: 36.86677649620876\n",
      "penalty: 0.01027146046455965\n",
      "h_val:  tensor(0.5247, grad_fn=<SubBackward0>)\n",
      "squared loss: 31.14672500159726\n",
      "loss: 36.41754551489757\n",
      "penalty: 0.13767678157003005\n",
      "h_val:  tensor(0.5311, grad_fn=<SubBackward0>)\n",
      "squared loss: 31.278170843347652\n",
      "loss: 35.63699499344196\n",
      "penalty: 0.14103518793381836\n",
      "h_val:  tensor(0.3280, grad_fn=<SubBackward0>)\n",
      "squared loss: 36.67305574136003\n",
      "loss: 38.63439566971725\n",
      "penalty: 0.05377678177552884\n",
      "h_val:  tensor(0.4564, grad_fn=<SubBackward0>)\n",
      "squared loss: 31.55133756142539\n",
      "loss: 34.89232054445977\n",
      "penalty: 0.10414104046545869\n",
      "h_val:  tensor(0.3716, grad_fn=<SubBackward0>)\n",
      "squared loss: 31.56700473846687\n",
      "loss: 34.49355677068508\n",
      "penalty: 0.06905230272101717\n",
      "h_val:  tensor(0.3929, grad_fn=<SubBackward0>)\n",
      "squared loss: 31.19890499782702\n",
      "loss: 34.198719333096335\n",
      "penalty: 0.0772023481690357\n",
      "h_val:  tensor(0.1927, grad_fn=<SubBackward0>)\n",
      "squared loss: 31.32944545392114\n",
      "loss: 34.0919740439247\n",
      "penalty: 0.018575723266220395\n",
      "h_val:  tensor(0.1692, grad_fn=<SubBackward0>)\n",
      "squared loss: 31.159935451558553\n",
      "loss: 34.04778170747884\n",
      "penalty: 0.014308443700645027\n",
      "h_val:  tensor(0.1060, grad_fn=<SubBackward0>)\n",
      "squared loss: 31.101507042436687\n",
      "loss: 34.02874388151225\n",
      "penalty: 0.005620686066982761\n",
      "h_val:  tensor(0.0926, grad_fn=<SubBackward0>)\n",
      "squared loss: 31.061633920575073\n",
      "loss: 34.00859326804781\n",
      "penalty: 0.004290788424111552\n",
      "h_val:  tensor(0.3522, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.984326018048364\n",
      "loss: 33.98340682554482\n",
      "penalty: 0.06200725119620529\n",
      "h_val:  tensor(0.1451, grad_fn=<SubBackward0>)\n",
      "squared loss: 31.026210499393432\n",
      "loss: 33.97689331165516\n",
      "penalty: 0.01052932651229093\n",
      "h_val:  tensor(0.4329, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.977974478999194\n",
      "loss: 33.96892706561002\n",
      "penalty: 0.09370712941906147\n",
      "h_val:  tensor(0.2025, grad_fn=<SubBackward0>)\n",
      "squared loss: 31.00671308849149\n",
      "loss: 33.95079965174942\n",
      "penalty: 0.020496037376394667\n",
      "h_val:  tensor(0.3074, grad_fn=<SubBackward0>)\n",
      "squared loss: 31.03087507332435\n",
      "loss: 33.90960337758299\n",
      "penalty: 0.047249653737799414\n",
      "h_val:  tensor(0.2170, grad_fn=<SubBackward0>)\n",
      "squared loss: 31.10531005252591\n",
      "loss: 33.86639005391862\n",
      "penalty: 0.023549153121988457\n",
      "h_val:  tensor(0.2135, grad_fn=<SubBackward0>)\n",
      "squared loss: 31.150063990817216\n",
      "loss: 33.80417228813258\n",
      "penalty: 0.022788131914127343\n",
      "h_val:  tensor(0.1670, grad_fn=<SubBackward0>)\n",
      "squared loss: 31.194641096456497\n",
      "loss: 33.68709695046132\n",
      "penalty: 0.013938162516607476\n",
      "h_val:  tensor(0.1433, grad_fn=<SubBackward0>)\n",
      "squared loss: 31.147255903058262\n",
      "loss: 33.61017445346563\n",
      "penalty: 0.010265849100720018\n",
      "h_val:  tensor(0.1506, grad_fn=<SubBackward0>)\n",
      "squared loss: 31.007681816840865\n",
      "loss: 33.54530920278085\n",
      "penalty: 0.01133799229193079\n",
      "h_val:  tensor(0.1436, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.882896756727877\n",
      "loss: 33.50824834286391\n",
      "penalty: 0.01031261665398298\n",
      "h_val:  tensor(0.1209, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.80738552408339\n",
      "loss: 33.481319800072605\n",
      "penalty: 0.007314184285814672\n",
      "h_val:  tensor(0.1682, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.74806937924009\n",
      "loss: 33.45896149091334\n",
      "penalty: 0.014137946525319172\n",
      "h_val:  tensor(0.2034, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.725793626554122\n",
      "loss: 33.431912471173646\n",
      "penalty: 0.020690967653816957\n",
      "h_val:  tensor(0.1843, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.724276758580096\n",
      "loss: 33.403745947647494\n",
      "penalty: 0.016987403033156108\n",
      "h_val:  tensor(0.2483, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.672137959273723\n",
      "loss: 33.342506737005195\n",
      "penalty: 0.030836078782475518\n",
      "h_val:  tensor(0.2559, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.61422290034637\n",
      "loss: 33.27809841444814\n",
      "penalty: 0.03274661221173174\n",
      "h_val:  tensor(0.1845, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.551620159123434\n",
      "loss: 33.2168598848429\n",
      "penalty: 0.017019386055093618\n",
      "h_val:  tensor(0.1774, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.496140444413154\n",
      "loss: 33.16697329734551\n",
      "penalty: 0.015738998596152624\n",
      "h_val:  tensor(0.1145, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.481269584296093\n",
      "loss: 33.13719457609867\n",
      "penalty: 0.0065535579280061665\n",
      "h_val:  tensor(0.1407, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.488209347668302\n",
      "loss: 33.11055807552925\n",
      "penalty: 0.009900110433535106\n",
      "h_val:  tensor(0.1086, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.508164328173656\n",
      "loss: 33.08949451531248\n",
      "penalty: 0.005901290326152475\n",
      "h_val:  tensor(0.1112, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.54117217861889\n",
      "loss: 33.066523527501175\n",
      "penalty: 0.006186513599987935\n",
      "h_val:  tensor(0.1043, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.583197165024092\n",
      "loss: 33.02831492275865\n",
      "penalty: 0.005440268952206939\n",
      "h_val:  tensor(0.1946, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.595408281817505\n",
      "loss: 32.98272526860121\n",
      "penalty: 0.01892911307510471\n",
      "h_val:  tensor(0.1822, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.57465877815468\n",
      "loss: 32.94210186813184\n",
      "penalty: 0.016603602050909848\n",
      "h_val:  tensor(0.0976, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.545768637157348\n",
      "loss: 32.90636499329538\n",
      "penalty: 0.004767108749591315\n",
      "h_val:  tensor(0.0619, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.5323953512994\n",
      "loss: 32.890902115709785\n",
      "penalty: 0.0019139521249615294\n",
      "h_val:  tensor(0.0479, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.501736742125516\n",
      "loss: 32.87493772179377\n",
      "penalty: 0.0011449209117842338\n",
      "h_val:  tensor(0.1506, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.426244694774187\n",
      "loss: 32.854942804623754\n",
      "penalty: 0.011342469819052733\n",
      "h_val:  tensor(0.1134, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.412202488277384\n",
      "loss: 32.843319283755335\n",
      "penalty: 0.006433163534803353\n",
      "h_val:  tensor(0.1794, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.33399722765718\n",
      "loss: 32.824354982115196\n",
      "penalty: 0.01608684806889249\n",
      "h_val:  tensor(0.1097, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.32582329897726\n",
      "loss: 32.81277999601456\n",
      "penalty: 0.006021550116300422\n",
      "h_val:  tensor(0.1066, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.302437856053167\n",
      "loss: 32.80750499305186\n",
      "penalty: 0.005678067813477619\n",
      "h_val:  tensor(0.0879, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.285184006145062\n",
      "loss: 32.80402623825206\n",
      "penalty: 0.00386621320905829\n",
      "h_val:  tensor(0.0847, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.274021836072723\n",
      "loss: 32.80202635601423\n",
      "penalty: 0.003583132658235099\n",
      "h_val:  tensor(0.0977, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.259964191514133\n",
      "loss: 32.79926895365747\n",
      "penalty: 0.004770619996811333\n",
      "h_val:  tensor(0.1071, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.2552044126315\n",
      "loss: 32.79730847870716\n",
      "penalty: 0.00573400625170278\n",
      "h_val:  tensor(0.1150, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.252478081201208\n",
      "loss: 32.794053853767764\n",
      "penalty: 0.006611978202847711\n",
      "h_val:  tensor(0.1384, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.24895861196874\n",
      "loss: 32.78651601388342\n",
      "penalty: 0.009582640841812949\n",
      "h_val:  tensor(0.1696, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.24492617837028\n",
      "loss: 32.76964538402505\n",
      "penalty: 0.01437946214730818\n",
      "h_val:  tensor(0.1707, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.241083987976857\n",
      "loss: 32.73697020595121\n",
      "penalty: 0.01457527101513878\n",
      "h_val:  tensor(0.1127, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.245306950494548\n",
      "loss: 32.694474142417704\n",
      "penalty: 0.006345693851180767\n",
      "h_val:  tensor(0.1043, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.252458304252947\n",
      "loss: 32.68594559597047\n",
      "penalty: 0.005436044904569444\n",
      "h_val:  tensor(0.1326, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.265401356919053\n",
      "loss: 32.67995971872162\n",
      "penalty: 0.008791992652088422\n",
      "h_val:  tensor(0.1181, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.267040883484427\n",
      "loss: 32.67878880352731\n",
      "penalty: 0.0069686280548522045\n",
      "h_val:  tensor(0.1119, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.267844235477998\n",
      "loss: 32.67695498024028\n",
      "penalty: 0.006258486785279782\n",
      "h_val:  tensor(0.1034, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.266861121427624\n",
      "loss: 32.67400183003917\n",
      "penalty: 0.005342382443054723\n",
      "h_val:  tensor(0.1019, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.264282901130073\n",
      "loss: 32.66615530428702\n",
      "penalty: 0.005188904355586341\n",
      "h_val:  tensor(0.0880, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.26285026137124\n",
      "loss: 32.65468393869342\n",
      "penalty: 0.0038719269060650636\n",
      "h_val:  tensor(0.0795, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.260816336592576\n",
      "loss: 32.64278725611579\n",
      "penalty: 0.003156999385387398\n",
      "h_val:  tensor(0.0482, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.25868016137177\n",
      "loss: 32.63089183272842\n",
      "penalty: 0.0011626675576687764\n",
      "h_val:  tensor(0.0559, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.245519968384446\n",
      "loss: 32.62071792559137\n",
      "penalty: 0.0015623310329907051\n",
      "h_val:  tensor(0.2236, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.22343183557663\n",
      "loss: 32.62333371932231\n",
      "penalty: 0.025009155873755958\n",
      "h_val:  tensor(0.1094, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.23434881209105\n",
      "loss: 32.61458016095465\n",
      "penalty: 0.005986272606329424\n",
      "h_val:  tensor(0.1241, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.23103141955557\n",
      "loss: 32.612088325276474\n",
      "penalty: 0.007704273180013717\n",
      "h_val:  tensor(0.1137, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.231818609268846\n",
      "loss: 32.607008686533526\n",
      "penalty: 0.006464538031936945\n",
      "h_val:  tensor(0.1202, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.232167497586666\n",
      "loss: 32.59793783383144\n",
      "penalty: 0.007228441744123299\n",
      "h_val:  tensor(0.1031, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.238423002502266\n",
      "loss: 32.58094613233481\n",
      "penalty: 0.005314831816460323\n",
      "h_val:  tensor(0.0884, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.24221033715969\n",
      "loss: 32.566234782987465\n",
      "penalty: 0.0039056021275911254\n",
      "h_val:  tensor(0.0756, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.239116597778906\n",
      "loss: 32.550342941637055\n",
      "penalty: 0.0028612234223410688\n",
      "h_val:  tensor(0.1193, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.222156646579926\n",
      "loss: 32.53802723258983\n",
      "penalty: 0.007115166245125975\n",
      "h_val:  tensor(0.1386, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.207459894474724\n",
      "loss: 32.533854490330114\n",
      "penalty: 0.009606488548453952\n",
      "h_val:  tensor(0.1238, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.194704506993556\n",
      "loss: 32.52976320535723\n",
      "penalty: 0.007662271495712314\n",
      "h_val:  tensor(0.1072, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.18346096412811\n",
      "loss: 32.52648119361886\n",
      "penalty: 0.0057484057552079125\n",
      "h_val:  tensor(0.0954, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.175652582721877\n",
      "loss: 32.52413049005888\n",
      "penalty: 0.004546355917563018\n",
      "h_val:  tensor(0.0937, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.170982378542504\n",
      "loss: 32.5223315669988\n",
      "penalty: 0.00438611628137883\n",
      "h_val:  tensor(0.1049, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.16632546745167\n",
      "loss: 32.520259671462384\n",
      "penalty: 0.005499956711415729\n",
      "h_val:  tensor(0.1147, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.161357320659263\n",
      "loss: 32.51708735747491\n",
      "penalty: 0.006573555344378342\n",
      "h_val:  tensor(0.1275, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.15095773948707\n",
      "loss: 32.5090964804535\n",
      "penalty: 0.008132816207220278\n",
      "h_val:  tensor(0.1141, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.138925299748976\n",
      "loss: 32.494060503651994\n",
      "penalty: 0.006512696826586891\n",
      "h_val:  tensor(0.1121, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.135267729463862\n",
      "loss: 32.486012574969095\n",
      "penalty: 0.006280644908322739\n",
      "h_val:  tensor(0.1136, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.1353740504969\n",
      "loss: 32.4827883571568\n",
      "penalty: 0.006454723303247448\n",
      "h_val:  tensor(0.1122, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.136625945268225\n",
      "loss: 32.48040983368545\n",
      "penalty: 0.006291673094282442\n",
      "h_val:  tensor(0.1109, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.136685685108088\n",
      "loss: 32.477103607419075\n",
      "penalty: 0.0061447892166859724\n",
      "h_val:  tensor(0.1074, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.136623946380144\n",
      "loss: 32.47158098856681\n",
      "penalty: 0.005763826815173244\n",
      "h_val:  tensor(0.1033, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.136330662242745\n",
      "loss: 32.46371788382215\n",
      "penalty: 0.00533856130453169\n",
      "h_val:  tensor(0.0921, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.135806639834257\n",
      "loss: 32.455635676693696\n",
      "penalty: 0.0042439275162451905\n",
      "h_val:  tensor(0.1222, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.130523502579123\n",
      "loss: 32.44966230918101\n",
      "penalty: 0.00746912566092816\n",
      "h_val:  tensor(0.1240, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.1264233175553\n",
      "loss: 32.44635443432834\n",
      "penalty: 0.007683309100924036\n",
      "h_val:  tensor(0.1173, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.12302643831403\n",
      "loss: 32.44313000717619\n",
      "penalty: 0.00687639731377654\n",
      "h_val:  tensor(0.1132, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.110392930722437\n",
      "loss: 32.431481785382964\n",
      "penalty: 0.006402199308387286\n",
      "h_val:  tensor(0.0773, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.106330988300865\n",
      "loss: 32.42225348041606\n",
      "penalty: 0.002984592512624279\n",
      "h_val:  tensor(0.0788, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.10433622261565\n",
      "loss: 32.41621243317662\n",
      "penalty: 0.0031074141190992823\n",
      "h_val:  tensor(0.0926, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.103255126373895\n",
      "loss: 32.4130250552215\n",
      "penalty: 0.004286856063498821\n",
      "h_val:  tensor(0.1070, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.101714957982974\n",
      "loss: 32.41176659447766\n",
      "penalty: 0.0057227022140889155\n",
      "h_val:  tensor(0.1082, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.100397373856595\n",
      "loss: 32.41001748375255\n",
      "penalty: 0.005858614836350232\n",
      "h_val:  tensor(0.1111, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.09565397639206\n",
      "loss: 32.40451763435791\n",
      "penalty: 0.006174671087020041\n",
      "h_val:  tensor(0.1034, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.091602344983507\n",
      "loss: 32.398335140122704\n",
      "penalty: 0.005345129549375549\n",
      "h_val:  tensor(0.0922, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.08902905910584\n",
      "loss: 32.393025067241744\n",
      "penalty: 0.004250356975358213\n",
      "h_val:  tensor(0.0926, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.087142062680886\n",
      "loss: 32.39022963266279\n",
      "penalty: 0.004287594795596276\n",
      "h_val:  tensor(0.1036, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.084331635816877\n",
      "loss: 32.38897326031494\n",
      "penalty: 0.005362907058407944\n",
      "h_val:  tensor(0.1035, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.083444280360272\n",
      "loss: 32.38824352410037\n",
      "penalty: 0.005356181835323461\n",
      "h_val:  tensor(0.1076, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.08171296431818\n",
      "loss: 32.387125868139854\n",
      "penalty: 0.0057892705683106\n",
      "h_val:  tensor(0.1008, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.076777525262187\n",
      "loss: 32.381554941719585\n",
      "penalty: 0.005085131257952852\n",
      "h_val:  tensor(0.1268, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.062476827291437\n",
      "loss: 32.37122670946912\n",
      "penalty: 0.008033086817949479\n",
      "h_val:  tensor(0.0941, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.043541307859968\n",
      "loss: 32.35135934845927\n",
      "penalty: 0.004425023268329144\n",
      "h_val:  tensor(0.0346, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.033155359911753\n",
      "loss: 32.34163677447323\n",
      "penalty: 0.0005996400713275712\n",
      "h_val:  tensor(0.0626, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.021465817097862\n",
      "loss: 32.33174370765698\n",
      "penalty: 0.0019565093441435426\n",
      "h_val:  tensor(0.1880, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.001028371728207\n",
      "loss: 32.32780183962867\n",
      "penalty: 0.017667538298247663\n",
      "h_val:  tensor(0.1114, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.01062060413113\n",
      "loss: 32.3255447295792\n",
      "penalty: 0.006210022455732051\n",
      "h_val:  tensor(0.1320, grad_fn=<SubBackward0>)\n",
      "squared loss: 30.00113971760776\n",
      "loss: 32.32094152950063\n",
      "penalty: 0.008713879590825332\n",
      "h_val:  tensor(0.1090, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.994670715993973\n",
      "loss: 32.3138639286624\n",
      "penalty: 0.005937697725185079\n",
      "h_val:  tensor(0.1047, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.950357819339093\n",
      "loss: 32.28793909906552\n",
      "penalty: 0.005477202222884061\n",
      "h_val:  tensor(0.0948, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.92868027989247\n",
      "loss: 32.279617522185745\n",
      "penalty: 0.004492396453278897\n",
      "h_val:  tensor(0.0893, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.91386197952977\n",
      "loss: 32.27510171505336\n",
      "penalty: 0.003983864349669296\n",
      "h_val:  tensor(0.0813, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.90358520249911\n",
      "loss: 32.27164179138946\n",
      "penalty: 0.0033024730212090057\n",
      "h_val:  tensor(0.1018, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.89404017855209\n",
      "loss: 32.26899141765649\n",
      "penalty: 0.005186084609811387\n",
      "h_val:  tensor(0.0936, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.891266681252514\n",
      "loss: 32.26705853142194\n",
      "penalty: 0.004382744548375259\n",
      "h_val:  tensor(0.1078, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.886566879208512\n",
      "loss: 32.26525134482957\n",
      "penalty: 0.005809162306365438\n",
      "h_val:  tensor(0.1265, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.88124796676101\n",
      "loss: 32.26300812932998\n",
      "penalty: 0.008002232135159649\n",
      "h_val:  tensor(0.1161, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.879770777949687\n",
      "loss: 32.261363828136986\n",
      "penalty: 0.0067424455085612215\n",
      "h_val:  tensor(0.0996, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.87989124530273\n",
      "loss: 32.259553944578705\n",
      "penalty: 0.004955188025569171\n",
      "h_val:  tensor(0.0938, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.880559522081697\n",
      "loss: 32.25902385602128\n",
      "penalty: 0.00440145618726229\n",
      "h_val:  tensor(0.0837, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.882505758835812\n",
      "loss: 32.25749047201501\n",
      "penalty: 0.003505997477139104\n",
      "h_val:  tensor(0.0781, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.883756769925608\n",
      "loss: 32.25593556085811\n",
      "penalty: 0.003047411991413621\n",
      "h_val:  tensor(0.0756, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.884106997871402\n",
      "loss: 32.254026181893195\n",
      "penalty: 0.0028580069112737975\n",
      "h_val:  tensor(0.0817, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.882606145243486\n",
      "loss: 32.25225201850436\n",
      "penalty: 0.0033369135530004704\n",
      "h_val:  tensor(0.0939, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.879451408711702\n",
      "loss: 32.25085693313689\n",
      "penalty: 0.004412340135972803\n",
      "h_val:  tensor(0.0965, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.876206812172082\n",
      "loss: 32.24920697049701\n",
      "penalty: 0.004656764762233533\n",
      "h_val:  tensor(0.1227, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.85724535740838\n",
      "loss: 32.24295411777094\n",
      "penalty: 0.0075329281080824964\n",
      "h_val:  tensor(0.0972, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.850960848677076\n",
      "loss: 32.23874382196329\n",
      "penalty: 0.004726486028890162\n",
      "h_val:  tensor(0.0847, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.827243703105168\n",
      "loss: 32.23068664438207\n",
      "penalty: 0.003586013470176826\n",
      "h_val:  tensor(0.0721, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.81544130629941\n",
      "loss: 32.22674624396636\n",
      "penalty: 0.002601924859486894\n",
      "h_val:  tensor(0.1036, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.79501515879194\n",
      "loss: 32.221020863021764\n",
      "penalty: 0.005362932012424776\n",
      "h_val:  tensor(0.1046, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.782005953739038\n",
      "loss: 32.21549039443366\n",
      "penalty: 0.0054713904441854\n",
      "h_val:  tensor(0.1001, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.77063868493208\n",
      "loss: 32.20784419077221\n",
      "penalty: 0.005011116221094057\n",
      "h_val:  tensor(0.1049, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.758296483808632\n",
      "loss: 32.19685715478724\n",
      "penalty: 0.005499384729739614\n",
      "h_val:  tensor(0.0973, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.751226328040765\n",
      "loss: 32.1894721086752\n",
      "penalty: 0.004730712844484164\n",
      "h_val:  tensor(0.1211, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.745233465577055\n",
      "loss: 32.18242312461388\n",
      "penalty: 0.0073337583569643774\n",
      "h_val:  tensor(0.1125, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.741721565522205\n",
      "loss: 32.17505804675554\n",
      "penalty: 0.006323402603106193\n",
      "h_val:  tensor(0.1033, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.74042944837346\n",
      "loss: 32.16860608630419\n",
      "penalty: 0.005339736427013236\n",
      "h_val:  tensor(0.0962, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.73770891471173\n",
      "loss: 32.16452522589421\n",
      "penalty: 0.004629888492073542\n",
      "h_val:  tensor(0.0970, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.734281239651615\n",
      "loss: 32.16212010071654\n",
      "penalty: 0.004706531742691971\n",
      "h_val:  tensor(0.1003, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.730255216229995\n",
      "loss: 32.15920147244818\n",
      "penalty: 0.005028083798836882\n",
      "h_val:  tensor(0.1004, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.72649912267249\n",
      "loss: 32.1552036961549\n",
      "penalty: 0.005036693016353404\n",
      "h_val:  tensor(0.1017, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.722162190998404\n",
      "loss: 32.149252637768164\n",
      "penalty: 0.00517023429575545\n",
      "h_val:  tensor(0.0887, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.72156157950981\n",
      "loss: 32.14404225225462\n",
      "penalty: 0.003932900537942612\n",
      "h_val:  tensor(0.0920, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.72181546065248\n",
      "loss: 32.14132427902694\n",
      "penalty: 0.0042346534052970725\n",
      "h_val:  tensor(0.0987, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.72214295166404\n",
      "loss: 32.13918634560899\n",
      "penalty: 0.004873340440999166\n",
      "h_val:  tensor(0.0951, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.72296084091757\n",
      "loss: 32.136446778962856\n",
      "penalty: 0.004521693074811014\n",
      "h_val:  tensor(0.0982, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.7227694619071\n",
      "loss: 32.13104272536478\n",
      "penalty: 0.004822539435776047\n",
      "h_val:  tensor(0.0888, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.723983328326298\n",
      "loss: 32.12527461442418\n",
      "penalty: 0.003939033222453851\n",
      "h_val:  tensor(0.0955, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.722680146528774\n",
      "loss: 32.12216662997069\n",
      "penalty: 0.004561275769517912\n",
      "h_val:  tensor(0.1031, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.719888174606456\n",
      "loss: 32.119820664398304\n",
      "penalty: 0.005316444353213711\n",
      "h_val:  tensor(0.0999, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.71721126004764\n",
      "loss: 32.11754932429809\n",
      "penalty: 0.004985897277301473\n",
      "h_val:  tensor(0.1162, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.707721573316817\n",
      "loss: 32.11266571291689\n",
      "penalty: 0.006750989861227261\n",
      "h_val:  tensor(0.0923, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.69536620330506\n",
      "loss: 32.10343937833983\n",
      "penalty: 0.004255177249251915\n",
      "h_val:  tensor(0.0935, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.68480272249567\n",
      "loss: 32.09669872082608\n",
      "penalty: 0.0043717864237813545\n",
      "h_val:  tensor(0.0756, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.667272093120644\n",
      "loss: 32.08438834454617\n",
      "penalty: 0.0028598674411133614\n",
      "h_val:  tensor(0.0923, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.653713363229976\n",
      "loss: 32.07694091084304\n",
      "penalty: 0.004258360285984841\n",
      "h_val:  tensor(0.1042, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.645320322977092\n",
      "loss: 32.07305656165882\n",
      "penalty: 0.005431941320568308\n",
      "h_val:  tensor(0.0909, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.643647883186556\n",
      "loss: 32.070877672756104\n",
      "penalty: 0.004135118992911456\n",
      "h_val:  tensor(0.0885, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.636879534730838\n",
      "loss: 32.06567351929087\n",
      "penalty: 0.003912244053431918\n",
      "h_val:  tensor(0.0820, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.628041396582645\n",
      "loss: 32.0579552015524\n",
      "penalty: 0.003361346709787057\n",
      "h_val:  tensor(0.0705, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.62027975056019\n",
      "loss: 32.05045754178668\n",
      "penalty: 0.0024819055640674516\n",
      "h_val:  tensor(0.0639, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.613692273667024\n",
      "loss: 32.04387151686545\n",
      "penalty: 0.0020394258769141763\n",
      "h_val:  tensor(0.0922, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.604142899895965\n",
      "loss: 32.037560531483415\n",
      "penalty: 0.004254563896297917\n",
      "h_val:  tensor(0.1042, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.59977555622195\n",
      "loss: 32.034412465851695\n",
      "penalty: 0.0054295221863744\n",
      "h_val:  tensor(0.1108, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.59640158945847\n",
      "loss: 32.030459482480076\n",
      "penalty: 0.00613575132109924\n",
      "h_val:  tensor(0.1113, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.593545166273778\n",
      "loss: 32.024598708156766\n",
      "penalty: 0.006194379103777038\n",
      "h_val:  tensor(0.1021, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.59092475434609\n",
      "loss: 32.01711516480439\n",
      "penalty: 0.00521197413319585\n",
      "h_val:  tensor(0.0993, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.58910817316405\n",
      "loss: 32.0139555207578\n",
      "penalty: 0.004934555678323651\n",
      "h_val:  tensor(0.1012, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.586174593023703\n",
      "loss: 32.01157378521301\n",
      "penalty: 0.005118546407469022\n",
      "h_val:  tensor(0.0996, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.582975420717766\n",
      "loss: 32.009539829520634\n",
      "penalty: 0.00496240670013651\n",
      "h_val:  tensor(0.0925, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.578580552465077\n",
      "loss: 32.00684361299335\n",
      "penalty: 0.004280740816266856\n",
      "h_val:  tensor(0.0851, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.573457562519724\n",
      "loss: 32.00391704839255\n",
      "penalty: 0.0036242018236221658\n",
      "h_val:  tensor(0.0852, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.56853707676798\n",
      "loss: 32.00103977517831\n",
      "penalty: 0.003633042465497219\n",
      "h_val:  tensor(0.0899, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.563228405193584\n",
      "loss: 31.997274776822625\n",
      "penalty: 0.004039655157865185\n",
      "h_val:  tensor(0.1095, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.556703424154932\n",
      "loss: 31.993073608956077\n",
      "penalty: 0.00599400076925106\n",
      "h_val:  tensor(0.1172, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.549860865393807\n",
      "loss: 31.987188580605153\n",
      "penalty: 0.006864476374790902\n",
      "h_val:  tensor(0.1186, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.536337609899093\n",
      "loss: 31.97472443654479\n",
      "penalty: 0.007032349442007837\n",
      "h_val:  tensor(0.0976, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.520861019077895\n",
      "loss: 31.959057622790127\n",
      "penalty: 0.004760895703093203\n",
      "h_val:  tensor(0.0488, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.50851018628165\n",
      "loss: 31.94667559871099\n",
      "penalty: 0.0011925389410733934\n",
      "h_val:  tensor(0.0951, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.49623087876779\n",
      "loss: 31.93794124288242\n",
      "penalty: 0.0045233606622941645\n",
      "h_val:  tensor(0.1736, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.474953985470467\n",
      "loss: 31.927725029453534\n",
      "penalty: 0.015063326116538198\n",
      "h_val:  tensor(0.1115, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.473385642732218\n",
      "loss: 31.917586767584037\n",
      "penalty: 0.006219561559092965\n",
      "h_val:  tensor(0.1207, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.462575054272634\n",
      "loss: 31.909718098308474\n",
      "penalty: 0.007289278502641805\n",
      "h_val:  tensor(0.0925, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.443290739754016\n",
      "loss: 31.89316671784171\n",
      "penalty: 0.004275850475745357\n",
      "h_val:  tensor(0.0847, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.411110545065096\n",
      "loss: 31.869515898040888\n",
      "penalty: 0.003590383087509836\n",
      "h_val:  tensor(0.0836, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.31804877405797\n",
      "loss: 31.816006001990555\n",
      "penalty: 0.003495319985943053\n",
      "h_val:  tensor(0.0706, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.29987321803502\n",
      "loss: 31.798296043682072\n",
      "penalty: 0.00249527927244043\n",
      "h_val:  tensor(0.3130, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.217082873919438\n",
      "loss: 31.77148410616007\n",
      "penalty: 0.04897804361636429\n",
      "h_val:  tensor(0.0950, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.216416925806545\n",
      "loss: 31.7277668972017\n",
      "penalty: 0.004513401457672787\n",
      "h_val:  tensor(0.0877, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.177051235979036\n",
      "loss: 31.70242084481977\n",
      "penalty: 0.003849793741606669\n",
      "h_val:  tensor(0.0918, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.171994099990993\n",
      "loss: 31.698697418280158\n",
      "penalty: 0.0042181780798679425\n",
      "h_val:  tensor(0.1808, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.135449951656256\n",
      "loss: 31.683862443500047\n",
      "penalty: 0.016352398567367903\n",
      "h_val:  tensor(0.1120, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.134801987411265\n",
      "loss: 31.674733176353733\n",
      "penalty: 0.006267559775260143\n",
      "h_val:  tensor(0.0795, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.122587052900453\n",
      "loss: 31.66634232223066\n",
      "penalty: 0.0031622511313008023\n",
      "h_val:  tensor(0.0598, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.112516142400352\n",
      "loss: 31.661301274903405\n",
      "penalty: 0.001786480383855653\n",
      "h_val:  tensor(0.0496, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.102790837271073\n",
      "loss: 31.65653421393003\n",
      "penalty: 0.0012299762041451357\n",
      "h_val:  tensor(0.0659, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.082214741633205\n",
      "loss: 31.645588572079348\n",
      "penalty: 0.0021689539848963805\n",
      "h_val:  tensor(0.1046, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.055922228323336\n",
      "loss: 31.632239958980186\n",
      "penalty: 0.0054698844915075815\n",
      "h_val:  tensor(0.0957, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.03496826605462\n",
      "loss: 31.616027456315773\n",
      "penalty: 0.004581149164136359\n",
      "h_val:  tensor(0.0593, grad_fn=<SubBackward0>)\n",
      "squared loss: 29.010540070912022\n",
      "loss: 31.597706793700713\n",
      "penalty: 0.0017584797235837425\n",
      "h_val:  tensor(0.0360, grad_fn=<SubBackward0>)\n",
      "squared loss: 28.995387519081103\n",
      "loss: 31.587896388301722\n",
      "penalty: 0.0006468082570403688\n",
      "h_val:  tensor(0.1179, grad_fn=<SubBackward0>)\n",
      "squared loss: 28.957524719184295\n",
      "loss: 31.570675120440075\n",
      "penalty: 0.006947096413916102\n",
      "h_val:  tensor(0.0983, grad_fn=<SubBackward0>)\n",
      "squared loss: 28.947567236448357\n",
      "loss: 31.563525515181265\n",
      "penalty: 0.004826784601175\n",
      "h_val:  tensor(0.1381, grad_fn=<SubBackward0>)\n",
      "squared loss: 28.9219851744927\n",
      "loss: 31.551346670140067\n",
      "penalty: 0.009542060537798483\n",
      "h_val:  tensor(0.1321, grad_fn=<SubBackward0>)\n",
      "squared loss: 28.89411320636872\n",
      "loss: 31.535332063856753\n",
      "penalty: 0.008727400358983542\n",
      "h_val:  tensor(0.1060, grad_fn=<SubBackward0>)\n",
      "squared loss: 28.86830172651382\n",
      "loss: 31.517948406735684\n",
      "penalty: 0.00561785103845361\n",
      "h_val:  tensor(0.1010, grad_fn=<SubBackward0>)\n",
      "squared loss: 28.809906009223575\n",
      "loss: 31.488613718289646\n",
      "penalty: 0.005098033295529527\n",
      "h_val:  tensor(0.0781, grad_fn=<SubBackward0>)\n",
      "squared loss: 28.797621372532532\n",
      "loss: 31.480208069537575\n",
      "penalty: 0.0030480434911726467\n",
      "h_val:  tensor(0.1015, grad_fn=<SubBackward0>)\n",
      "squared loss: 28.78821012494226\n",
      "loss: 31.476340714264882\n",
      "penalty: 0.005155472681969976\n",
      "h_val:  tensor(0.1059, grad_fn=<SubBackward0>)\n",
      "squared loss: 28.782307335319544\n",
      "loss: 31.47330611301893\n",
      "penalty: 0.005605827471934712\n",
      "h_val:  tensor(0.1028, grad_fn=<SubBackward0>)\n",
      "squared loss: 28.77126868803654\n",
      "loss: 31.468793035412766\n",
      "penalty: 0.005288803503210825\n",
      "h_val:  tensor(0.1043, grad_fn=<SubBackward0>)\n",
      "squared loss: 28.74653709870213\n",
      "loss: 31.45966484338837\n",
      "penalty: 0.005439799991394434\n",
      "h_val:  tensor(0.1047, grad_fn=<SubBackward0>)\n",
      "squared loss: 28.699216308868557\n",
      "loss: 31.443366652010326\n",
      "penalty: 0.0054782942376456764\n",
      "h_val:  tensor(0.1090, grad_fn=<SubBackward0>)\n",
      "squared loss: 28.619922886421392\n",
      "loss: 31.416829456317153\n",
      "penalty: 0.005943900698644708\n",
      "h_val:  tensor(0.0622, grad_fn=<SubBackward0>)\n",
      "squared loss: 28.496106188274883\n",
      "loss: 31.377358264982\n",
      "penalty: 0.0019316573037781598\n",
      "h_val:  tensor(0.0483, grad_fn=<SubBackward0>)\n",
      "squared loss: 28.410606304612397\n",
      "loss: 31.34858939693445\n",
      "penalty: 0.0011666725350300002\n",
      "h_val:  tensor(0.1223, grad_fn=<SubBackward0>)\n",
      "squared loss: 28.332686101641798\n",
      "loss: 31.32203688876606\n",
      "penalty: 0.007480432745633206\n",
      "h_val:  tensor(0.1039, grad_fn=<SubBackward0>)\n",
      "squared loss: 28.30050724615208\n",
      "loss: 31.31119212622575\n",
      "penalty: 0.005395222361387959\n",
      "h_val:  tensor(0.0921, grad_fn=<SubBackward0>)\n",
      "squared loss: 28.27517112802087\n",
      "loss: 31.303162767542617\n",
      "penalty: 0.004243726317674861\n",
      "h_val:  tensor(0.0844, grad_fn=<SubBackward0>)\n",
      "squared loss: 28.248953131044598\n",
      "loss: 31.29445513105702\n",
      "penalty: 0.0035631433595442377\n",
      "h_val:  tensor(0.0842, grad_fn=<SubBackward0>)\n",
      "squared loss: 28.22493587719626\n",
      "loss: 31.284332749361333\n",
      "penalty: 0.0035428586013269508\n",
      "h_val:  tensor(0.0908, grad_fn=<SubBackward0>)\n",
      "squared loss: 28.20516668143376\n",
      "loss: 31.272685361516253\n",
      "penalty: 0.004119291623679834\n",
      "h_val:  tensor(0.0826, grad_fn=<SubBackward0>)\n",
      "squared loss: 28.190130472022407\n",
      "loss: 31.259682468268537\n",
      "penalty: 0.0034081358516297733\n",
      "h_val:  tensor(0.0905, grad_fn=<SubBackward0>)\n",
      "squared loss: 28.17024471192233\n",
      "loss: 31.24396433766042\n",
      "penalty: 0.0040911822896529506\n",
      "h_val:  tensor(0.0915, grad_fn=<SubBackward0>)\n",
      "squared loss: 28.147339850233436\n",
      "loss: 31.225553276044415\n",
      "penalty: 0.004181684911730545\n",
      "h_val:  tensor(0.1365, grad_fn=<SubBackward0>)\n",
      "squared loss: 28.124297029374272\n",
      "loss: 31.20825861565427\n",
      "penalty: 0.009320592253511785\n",
      "h_val:  tensor(0.1197, grad_fn=<SubBackward0>)\n",
      "squared loss: 28.11060148647156\n",
      "loss: 31.18773592757429\n",
      "penalty: 0.007159554250934334\n",
      "h_val:  tensor(0.1084, grad_fn=<SubBackward0>)\n",
      "squared loss: 28.08889414856351\n",
      "loss: 31.16411334682012\n",
      "penalty: 0.005879285493780106\n",
      "h_val:  tensor(0.0698, grad_fn=<SubBackward0>)\n",
      "squared loss: 28.03586906622384\n",
      "loss: 31.123287154803407\n",
      "penalty: 0.0024371144787877054\n",
      "h_val:  tensor(0.0950, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.94856783577335\n",
      "loss: 31.07851135877566\n",
      "penalty: 0.004510129170513255\n",
      "h_val:  tensor(0.0989, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.80171255662628\n",
      "loss: 31.020416185485196\n",
      "penalty: 0.004892800004035813\n",
      "h_val:  tensor(0.0290, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.692865037325664\n",
      "loss: 30.979285576060374\n",
      "penalty: 0.00041908411789844005\n",
      "h_val:  tensor(0.0972, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.613528630277724\n",
      "loss: 30.95394351799662\n",
      "penalty: 0.004723060438201702\n",
      "h_val:  tensor(0.0878, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.565854106342837\n",
      "loss: 30.944802048212896\n",
      "penalty: 0.0038519247893648713\n",
      "h_val:  tensor(0.0746, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.533542215979676\n",
      "loss: 30.938941765716528\n",
      "penalty: 0.002784912531672968\n",
      "h_val:  tensor(0.0743, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.461820047527006\n",
      "loss: 30.9253974081299\n",
      "penalty: 0.0027588860598436425\n",
      "h_val:  tensor(0.0215, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.40425792941251\n",
      "loss: 30.9093840484538\n",
      "penalty: 0.00023073957350609162\n",
      "h_val:  tensor(0.0935, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.35380750373726\n",
      "loss: 30.889395302094083\n",
      "penalty: 0.004375503637536144\n",
      "h_val:  tensor(0.0561, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.342555094885736\n",
      "loss: 30.868908750862165\n",
      "penalty: 0.0015709776420233148\n",
      "h_val:  tensor(0.0944, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.366336273641693\n",
      "loss: 30.863742147577295\n",
      "penalty: 0.00445919885096861\n",
      "h_val:  tensor(0.0774, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.376304928039094\n",
      "loss: 30.862113709990258\n",
      "penalty: 0.002995935553422763\n",
      "h_val:  tensor(0.0795, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.377199546597637\n",
      "loss: 30.860187428910592\n",
      "penalty: 0.0031619387200860203\n",
      "h_val:  tensor(0.0674, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.37260291723107\n",
      "loss: 30.856724601683172\n",
      "penalty: 0.0022734903281648304\n",
      "h_val:  tensor(0.0647, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.362240599805563\n",
      "loss: 30.854000976510083\n",
      "penalty: 0.00209231579275047\n",
      "h_val:  tensor(0.0605, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.34935401969811\n",
      "loss: 30.851324494471175\n",
      "penalty: 0.0018290300205860818\n",
      "h_val:  tensor(0.0751, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.338544303302577\n",
      "loss: 30.84925488985764\n",
      "penalty: 0.0028236093278671023\n",
      "h_val:  tensor(0.0915, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.332941690316563\n",
      "loss: 30.84800355259632\n",
      "penalty: 0.004189376865935659\n",
      "h_val:  tensor(0.0970, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.330461169695198\n",
      "loss: 30.846305110375553\n",
      "penalty: 0.004702794623996394\n",
      "h_val:  tensor(0.1015, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.326674915965484\n",
      "loss: 30.840176292150655\n",
      "penalty: 0.005151252548124572\n",
      "h_val:  tensor(0.0758, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.327627021711017\n",
      "loss: 30.833270130713498\n",
      "penalty: 0.0028762321652624264\n",
      "h_val:  tensor(0.0729, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.33006043291643\n",
      "loss: 30.830993128299454\n",
      "penalty: 0.0026605975976719334\n",
      "h_val:  tensor(0.0856, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.335489760614422\n",
      "loss: 30.82723345708957\n",
      "penalty: 0.0036613347816191123\n",
      "h_val:  tensor(0.0908, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.342175674508823\n",
      "loss: 30.82474947791647\n",
      "penalty: 0.004121638965650381\n",
      "h_val:  tensor(0.0821, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.352086770315395\n",
      "loss: 30.821510220383875\n",
      "penalty: 0.003371107872107816\n",
      "h_val:  tensor(0.0708, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.36368168726515\n",
      "loss: 30.818264204276176\n",
      "penalty: 0.002507205769366911\n",
      "h_val:  tensor(0.0695, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.375178252482318\n",
      "loss: 30.814979626165833\n",
      "penalty: 0.002414791580874028\n",
      "h_val:  tensor(0.0702, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.382859427453788\n",
      "loss: 30.81171397767268\n",
      "penalty: 0.0024641207226994595\n",
      "h_val:  tensor(0.0821, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.383433740343477\n",
      "loss: 30.809387056049257\n",
      "penalty: 0.0033703536950003476\n",
      "h_val:  tensor(0.0782, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.381330833064414\n",
      "loss: 30.807776551005027\n",
      "penalty: 0.003056156465899904\n",
      "h_val:  tensor(0.0878, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.376603442670273\n",
      "loss: 30.805901891904906\n",
      "penalty: 0.0038575832769727355\n",
      "h_val:  tensor(0.0821, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.370811230218795\n",
      "loss: 30.802569241651423\n",
      "penalty: 0.003372453532538363\n",
      "h_val:  tensor(0.0680, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.37111815128972\n",
      "loss: 30.800930335262883\n",
      "penalty: 0.0023140985790950056\n",
      "h_val:  tensor(0.0639, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.374638247336044\n",
      "loss: 30.798304202450872\n",
      "penalty: 0.0020443633041904567\n",
      "h_val:  tensor(0.0710, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.366892219422848\n",
      "loss: 30.794321569175608\n",
      "penalty: 0.0025176246930956937\n",
      "h_val:  tensor(0.0391, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.356014794286793\n",
      "loss: 30.785939091991793\n",
      "penalty: 0.0007653196063471973\n",
      "h_val:  tensor(0.0749, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.351355075818805\n",
      "loss: 30.776636253354074\n",
      "penalty: 0.0028037623917641385\n",
      "h_val:  tensor(0.0434, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.35773745872092\n",
      "loss: 30.772666071522952\n",
      "penalty: 0.0009397229051737578\n",
      "h_val:  tensor(0.0631, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.35854958755563\n",
      "loss: 30.769695369132698\n",
      "penalty: 0.001990289043127949\n",
      "h_val:  tensor(0.1308, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.35972823814442\n",
      "loss: 30.767372272840696\n",
      "penalty: 0.008555355167512175\n",
      "h_val:  tensor(0.0847, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.36499045710091\n",
      "loss: 30.764609973346996\n",
      "penalty: 0.00358529339189664\n",
      "h_val:  tensor(0.0800, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.369403952923463\n",
      "loss: 30.762026958184187\n",
      "penalty: 0.0031973268260071385\n",
      "h_val:  tensor(0.0768, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.389872175188408\n",
      "loss: 30.754799057581003\n",
      "penalty: 0.0029488233794160348\n",
      "h_val:  tensor(0.0759, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.399704400827666\n",
      "loss: 30.749438851302795\n",
      "penalty: 0.0028801161267689008\n",
      "h_val:  tensor(0.0831, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.422108920895187\n",
      "loss: 30.73805892195922\n",
      "penalty: 0.003450195051977366\n",
      "h_val:  tensor(0.0831, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.423660206381854\n",
      "loss: 30.733864252997865\n",
      "penalty: 0.0034519244439937345\n",
      "h_val:  tensor(0.0624, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.425713503888645\n",
      "loss: 30.728982227805584\n",
      "penalty: 0.0019453081606982162\n",
      "h_val:  tensor(0.0520, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.425941792700815\n",
      "loss: 30.726028679084294\n",
      "penalty: 0.0013503795030428737\n",
      "h_val:  tensor(0.0484, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.426710855064886\n",
      "loss: 30.72280553394571\n",
      "penalty: 0.0011736674146812546\n",
      "h_val:  tensor(0.0787, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.427830225429116\n",
      "loss: 30.71888305640843\n",
      "penalty: 0.0031003057600060765\n",
      "h_val:  tensor(0.0859, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.43041808785826\n",
      "loss: 30.716289031184683\n",
      "penalty: 0.0036895266954925163\n",
      "h_val:  tensor(0.0919, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.43430570662808\n",
      "loss: 30.7114524995795\n",
      "penalty: 0.004223231727061189\n",
      "h_val:  tensor(0.0774, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.43727064300373\n",
      "loss: 30.706777941488635\n",
      "penalty: 0.0029922516149664966\n",
      "h_val:  tensor(0.0736, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.43985633847266\n",
      "loss: 30.703196061851145\n",
      "penalty: 0.0027081164830576876\n",
      "h_val:  tensor(0.0674, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.441993968461954\n",
      "loss: 30.701638572414982\n",
      "penalty: 0.0022743437035352026\n",
      "h_val:  tensor(0.0823, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.445320400241556\n",
      "loss: 30.69964701485778\n",
      "penalty: 0.0033881478611113816\n",
      "h_val:  tensor(0.0721, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.449984141384164\n",
      "loss: 30.698192019563564\n",
      "penalty: 0.0026007078147057897\n",
      "h_val:  tensor(0.0669, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.454462829055803\n",
      "loss: 30.69658776620946\n",
      "penalty: 0.002236273281867877\n",
      "h_val:  tensor(0.0633, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.458057503636105\n",
      "loss: 30.695031699630377\n",
      "penalty: 0.0020030279703412155\n",
      "h_val:  tensor(0.0714, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.45796579193168\n",
      "loss: 30.693809969516725\n",
      "penalty: 0.0025467662347845935\n",
      "h_val:  tensor(0.0834, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.455142474887108\n",
      "loss: 30.692540610409452\n",
      "penalty: 0.003477390202988062\n",
      "h_val:  tensor(0.0873, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.45104700207817\n",
      "loss: 30.69087489900352\n",
      "penalty: 0.0038146472693580784\n",
      "h_val:  tensor(0.0795, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.44314458985397\n",
      "loss: 30.687575765104636\n",
      "penalty: 0.0031595941024674255\n",
      "h_val:  tensor(0.0688, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.44127383930055\n",
      "loss: 30.686866952799114\n",
      "penalty: 0.002369738144727353\n",
      "h_val:  tensor(0.0667, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.44180773593598\n",
      "loss: 30.686573921838548\n",
      "penalty: 0.00222129354057133\n",
      "h_val:  tensor(0.0668, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.441448799087496\n",
      "loss: 30.68572443555076\n",
      "penalty: 0.002231509641183144\n",
      "h_val:  tensor(0.0698, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.43903002425928\n",
      "loss: 30.684839013975836\n",
      "penalty: 0.0024393331284288783\n",
      "h_val:  tensor(0.0729, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.43598713826887\n",
      "loss: 30.6838814603964\n",
      "penalty: 0.0026560783922572094\n",
      "h_val:  tensor(0.0751, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.429560652751206\n",
      "loss: 30.681690608850495\n",
      "penalty: 0.0028210855768900527\n",
      "h_val:  tensor(0.0705, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.42019461084564\n",
      "loss: 30.67806130878242\n",
      "penalty: 0.0024845297177383697\n",
      "h_val:  tensor(0.0607, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.41054881948117\n",
      "loss: 30.673843830145028\n",
      "penalty: 0.0018393628220151439\n",
      "h_val:  tensor(0.0592, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.405533440966895\n",
      "loss: 30.67104744105843\n",
      "penalty: 0.0017515685497529903\n",
      "h_val:  tensor(0.0677, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.405152735787\n",
      "loss: 30.668397841364296\n",
      "penalty: 0.0022906250841839525\n",
      "h_val:  tensor(0.0884, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.406254375109256\n",
      "loss: 30.664407701827876\n",
      "penalty: 0.0039035260009960986\n",
      "h_val:  tensor(0.0767, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.406613889423348\n",
      "loss: 30.661577828806713\n",
      "penalty: 0.002939035328714171\n",
      "h_val:  tensor(0.0693, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.40540404751339\n",
      "loss: 30.655299749080886\n",
      "penalty: 0.002399811136922761\n",
      "h_val:  tensor(0.0738, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.40268186061297\n",
      "loss: 30.652803019219338\n",
      "penalty: 0.002723939540328075\n",
      "h_val:  tensor(0.0770, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.39975823773419\n",
      "loss: 30.65043655242039\n",
      "penalty: 0.0029677426114877556\n",
      "h_val:  tensor(0.0683, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.39632465216154\n",
      "loss: 30.646875943051242\n",
      "penalty: 0.0023300317253416815\n",
      "h_val:  tensor(0.0803, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.38809769504227\n",
      "loss: 30.643274945806393\n",
      "penalty: 0.0032261414341600123\n",
      "h_val:  tensor(0.0543, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.385745822926904\n",
      "loss: 30.64117635642752\n",
      "penalty: 0.0014735896015159544\n",
      "h_val:  tensor(0.0712, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.38173514561585\n",
      "loss: 30.639845685045238\n",
      "penalty: 0.002537908460332221\n",
      "h_val:  tensor(0.0847, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.378100998104028\n",
      "loss: 30.638486554758064\n",
      "penalty: 0.0035852454003330006\n",
      "h_val:  tensor(0.0712, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.375301797482223\n",
      "loss: 30.636028408083877\n",
      "penalty: 0.0025356294762647847\n",
      "h_val:  tensor(0.0556, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.37199288890709\n",
      "loss: 30.634420837930225\n",
      "penalty: 0.0015430757846399503\n",
      "h_val:  tensor(0.0485, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.367814058617252\n",
      "loss: 30.6331245597109\n",
      "penalty: 0.0011772869632465214\n",
      "h_val:  tensor(0.0638, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.36463110580766\n",
      "loss: 30.63216029029721\n",
      "penalty: 0.0020332336686739736\n",
      "h_val:  tensor(0.0747, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.361634713531956\n",
      "loss: 30.631023989941816\n",
      "penalty: 0.0027909958485101993\n",
      "h_val:  tensor(0.0945, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.35424380923355\n",
      "loss: 30.628092510815982\n",
      "penalty: 0.004467023897325102\n",
      "h_val:  tensor(0.0757, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.34831561488539\n",
      "loss: 30.624582478466028\n",
      "penalty: 0.0028678742432363064\n",
      "h_val:  tensor(0.0442, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.33906090917435\n",
      "loss: 30.620513250764713\n",
      "penalty: 0.0009770760407944489\n",
      "h_val:  tensor(0.0528, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.33523919194952\n",
      "loss: 30.618013874975954\n",
      "penalty: 0.0013917098730691945\n",
      "h_val:  tensor(0.0630, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.32883098919432\n",
      "loss: 30.613523852271552\n",
      "penalty: 0.001981835843308728\n",
      "h_val:  tensor(0.0490, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.326567589260186\n",
      "loss: 30.611169017372788\n",
      "penalty: 0.0011993861337657235\n",
      "h_val:  tensor(0.0539, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.32380984431185\n",
      "loss: 30.609011854301357\n",
      "penalty: 0.0014508553972728793\n",
      "h_val:  tensor(0.0765, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.31939012794867\n",
      "loss: 30.60741082919808\n",
      "penalty: 0.00292375953421995\n",
      "h_val:  tensor(0.0678, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.316130426258002\n",
      "loss: 30.60599716355053\n",
      "penalty: 0.002295378528584335\n",
      "h_val:  tensor(0.0629, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.307575448641593\n",
      "loss: 30.603021856071493\n",
      "penalty: 0.0019789091669253794\n",
      "h_val:  tensor(0.0469, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.302695057221953\n",
      "loss: 30.600411168589034\n",
      "penalty: 0.0010976121672481008\n",
      "h_val:  tensor(0.0637, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.29859309068337\n",
      "loss: 30.599348950811216\n",
      "penalty: 0.0020264245963960733\n",
      "h_val:  tensor(0.0692, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.29810834144736\n",
      "loss: 30.598782757592527\n",
      "penalty: 0.002391316902333554\n",
      "h_val:  tensor(0.0740, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.298032786838203\n",
      "loss: 30.598139513227004\n",
      "penalty: 0.002740751734965515\n",
      "h_val:  tensor(0.0760, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.297308261979154\n",
      "loss: 30.597543834336346\n",
      "penalty: 0.002888256406247058\n",
      "h_val:  tensor(0.0801, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.293620889605094\n",
      "loss: 30.59575810133753\n",
      "penalty: 0.0032108491902650992\n",
      "h_val:  tensor(0.0770, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.288521278311112\n",
      "loss: 30.593719301822507\n",
      "penalty: 0.002965436934224147\n",
      "h_val:  tensor(0.0585, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.284203451729418\n",
      "loss: 30.591891290299728\n",
      "penalty: 0.001709161900350982\n",
      "h_val:  tensor(0.0511, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.27787050318294\n",
      "loss: 30.589619459543123\n",
      "penalty: 0.0013046292593677164\n",
      "h_val:  tensor(0.0604, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.269056363871034\n",
      "loss: 30.587222467672124\n",
      "penalty: 0.0018266082607124629\n",
      "h_val:  tensor(0.0609, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.266203212953158\n",
      "loss: 30.586127986389545\n",
      "penalty: 0.001855102816377513\n",
      "h_val:  tensor(0.0619, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.26257439905656\n",
      "loss: 30.584525314772115\n",
      "penalty: 0.0019143557211332974\n",
      "h_val:  tensor(0.0625, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.2589097122319\n",
      "loss: 30.58311391064143\n",
      "penalty: 0.0019512973069148202\n",
      "h_val:  tensor(0.0654, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.254712697967502\n",
      "loss: 30.581937408006695\n",
      "penalty: 0.002136506981624301\n",
      "h_val:  tensor(0.0667, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.251267857543013\n",
      "loss: 30.58104887712589\n",
      "penalty: 0.0022273327211476015\n",
      "h_val:  tensor(0.0632, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.24655320310508\n",
      "loss: 30.57985413643736\n",
      "penalty: 0.001995058103499013\n",
      "h_val:  tensor(0.0602, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.24407582677556\n",
      "loss: 30.579187364021305\n",
      "penalty: 0.0018095957409245626\n",
      "h_val:  tensor(0.0629, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.24236071106199\n",
      "loss: 30.578661612624146\n",
      "penalty: 0.0019788468205673014\n",
      "h_val:  tensor(0.0558, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.240638909660063\n",
      "loss: 30.578076680049232\n",
      "penalty: 0.0015586510740266896\n",
      "h_val:  tensor(0.0693, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.238159115897005\n",
      "loss: 30.577462246572324\n",
      "penalty: 0.0023978303078036347\n",
      "h_val:  tensor(0.0670, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.236277553725696\n",
      "loss: 30.57658696249153\n",
      "penalty: 0.0022440432134250418\n",
      "h_val:  tensor(0.0681, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.232448889979477\n",
      "loss: 30.575427189496537\n",
      "penalty: 0.0023173137967531553\n",
      "h_val:  tensor(0.0651, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.231028496362406\n",
      "loss: 30.575028386420833\n",
      "penalty: 0.0021159934513527716\n",
      "h_val:  tensor(0.0642, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.23021670804566\n",
      "loss: 30.57475898828376\n",
      "penalty: 0.0020604253567355728\n",
      "h_val:  tensor(0.0623, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.22935112744699\n",
      "loss: 30.57430146372351\n",
      "penalty: 0.0019424644834519409\n",
      "h_val:  tensor(0.0554, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.2284008914715\n",
      "loss: 30.573556496030434\n",
      "penalty: 0.0015363871203927604\n",
      "h_val:  tensor(0.0551, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.228512556192438\n",
      "loss: 30.572886615671617\n",
      "penalty: 0.0015185146585303157\n",
      "h_val:  tensor(0.0466, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.229687187892303\n",
      "loss: 30.571915862439035\n",
      "penalty: 0.001084245831110206\n",
      "h_val:  tensor(0.0567, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.230381946654333\n",
      "loss: 30.571182516500148\n",
      "penalty: 0.0016084025727277273\n",
      "h_val:  tensor(0.0764, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.23136468203494\n",
      "loss: 30.570501615636136\n",
      "penalty: 0.002918650695802769\n",
      "h_val:  tensor(0.0761, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.232160831262053\n",
      "loss: 30.56992186692353\n",
      "penalty: 0.002893780984664744\n",
      "h_val:  tensor(0.0659, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.241357945362655\n",
      "loss: 30.566853079515738\n",
      "penalty: 0.0021698439895209043\n",
      "h_val:  tensor(0.0574, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.246428524489904\n",
      "loss: 30.56603081122815\n",
      "penalty: 0.001647750059330984\n",
      "h_val:  tensor(0.0504, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.246038781974416\n",
      "loss: 30.565609936414972\n",
      "penalty: 0.001270447607019769\n",
      "h_val:  tensor(0.0535, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.24502220148797\n",
      "loss: 30.56515779146786\n",
      "penalty: 0.001433177608388275\n",
      "h_val:  tensor(0.0564, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.244748976244853\n",
      "loss: 30.56478780555518\n",
      "penalty: 0.0015886902018747145\n",
      "h_val:  tensor(0.0579, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.24548059465106\n",
      "loss: 30.564048169057315\n",
      "penalty: 0.0016754766849829798\n",
      "h_val:  tensor(0.0593, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.245622613487303\n",
      "loss: 30.563690691979115\n",
      "penalty: 0.0017593639964696723\n",
      "h_val:  tensor(0.0659, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.2477433558739\n",
      "loss: 30.56236735543405\n",
      "penalty: 0.002173989885114182\n",
      "h_val:  tensor(0.0594, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.249896944860236\n",
      "loss: 30.560913263993875\n",
      "penalty: 0.0017647372095173448\n",
      "h_val:  tensor(0.0617, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.25919709267531\n",
      "loss: 30.55797972222923\n",
      "penalty: 0.0019059696909762356\n",
      "h_val:  tensor(0.0567, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.262217216860392\n",
      "loss: 30.55704028785495\n",
      "penalty: 0.001607475622768968\n",
      "h_val:  tensor(0.0441, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.26869220408241\n",
      "loss: 30.55491967041577\n",
      "penalty: 0.0009730692726714549\n",
      "h_val:  tensor(0.0415, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.27221764554259\n",
      "loss: 30.55362423208721\n",
      "penalty: 0.000859637171561641\n",
      "h_val:  tensor(0.0511, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.27093498599715\n",
      "loss: 30.552795131188468\n",
      "penalty: 0.0013034468778533356\n",
      "h_val:  tensor(0.0614, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.26975200655503\n",
      "loss: 30.551984292433794\n",
      "penalty: 0.0018878342390154753\n",
      "h_val:  tensor(0.0675, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.269153322243582\n",
      "loss: 30.55144087343278\n",
      "penalty: 0.0022812354023082847\n",
      "h_val:  tensor(0.0626, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.269305017738986\n",
      "loss: 30.5510560527043\n",
      "penalty: 0.001956265516816407\n",
      "h_val:  tensor(0.0567, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.26969678309463\n",
      "loss: 30.55049195734205\n",
      "penalty: 0.0016067288622985895\n",
      "h_val:  tensor(0.0555, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.270187412957114\n",
      "loss: 30.550147623137665\n",
      "penalty: 0.0015426966110221752\n",
      "h_val:  tensor(0.0551, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.271200565277468\n",
      "loss: 30.54977143347302\n",
      "penalty: 0.0015192556407058703\n",
      "h_val:  tensor(0.0577, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.2719933089933\n",
      "loss: 30.549432153135086\n",
      "penalty: 0.001662022174926823\n",
      "h_val:  tensor(0.0614, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.273620173082055\n",
      "loss: 30.548921233209676\n",
      "penalty: 0.0018826044506406524\n",
      "h_val:  tensor(0.0638, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.276001468217693\n",
      "loss: 30.54823385581644\n",
      "penalty: 0.002034508318353838\n",
      "h_val:  tensor(0.0622, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.28231440490788\n",
      "loss: 30.547059752645325\n",
      "penalty: 0.0019371987087730027\n",
      "h_val:  tensor(0.0573, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.287842707698445\n",
      "loss: 30.546281746220224\n",
      "penalty: 0.0016427848751059595\n",
      "h_val:  tensor(0.0561, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.292344122573038\n",
      "loss: 30.545714992023946\n",
      "penalty: 0.0015717716546303389\n",
      "h_val:  tensor(0.0553, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.293856090657144\n",
      "loss: 30.545494669171273\n",
      "penalty: 0.0015310092682367274\n",
      "h_val:  tensor(0.0585, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.29426671822688\n",
      "loss: 30.545352269506882\n",
      "penalty: 0.0017137652444213834\n",
      "h_val:  tensor(0.0552, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.295419754616685\n",
      "loss: 30.545180269500943\n",
      "penalty: 0.001520891476605624\n",
      "h_val:  tensor(0.0578, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.29605828829602\n",
      "loss: 30.545033667775872\n",
      "penalty: 0.0016678406366974659\n",
      "h_val:  tensor(0.0607, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.300467382979516\n",
      "loss: 30.544574247124622\n",
      "penalty: 0.0018396516362597785\n",
      "h_val:  tensor(0.0568, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.303906430251164\n",
      "loss: 30.54428304558595\n",
      "penalty: 0.0016124795467629771\n",
      "h_val:  tensor(0.0547, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.308483161804432\n",
      "loss: 30.543886289896577\n",
      "penalty: 0.0014942668875639223\n",
      "h_val:  tensor(0.0543, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.31413281282144\n",
      "loss: 30.543330681095686\n",
      "penalty: 0.0014717698233498518\n",
      "h_val:  tensor(0.0531, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.319684811421418\n",
      "loss: 30.542715535645602\n",
      "penalty: 0.0014088471104080182\n",
      "h_val:  tensor(0.0490, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.322613128811316\n",
      "loss: 30.54230884340207\n",
      "penalty: 0.0012003446532019555\n",
      "h_val:  tensor(0.0507, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.32584913508097\n",
      "loss: 30.541839755716946\n",
      "penalty: 0.0012830713140142968\n",
      "h_val:  tensor(0.0533, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.326730837230652\n",
      "loss: 30.54163120199052\n",
      "penalty: 0.0014211059925708905\n",
      "h_val:  tensor(0.0566, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.326395856836047\n",
      "loss: 30.54149904570611\n",
      "penalty: 0.0015992024381923545\n",
      "h_val:  tensor(0.0574, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.325999949916504\n",
      "loss: 30.541370728706475\n",
      "penalty: 0.001646096450371353\n",
      "h_val:  tensor(0.0554, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.32630588809575\n",
      "loss: 30.541121520741203\n",
      "penalty: 0.001533387608934162\n",
      "h_val:  tensor(0.0556, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.32764229692216\n",
      "loss: 30.540892635221027\n",
      "penalty: 0.0015430578721219983\n",
      "h_val:  tensor(0.0523, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.328980613443573\n",
      "loss: 30.540724951419005\n",
      "penalty: 0.0013700022767898553\n",
      "h_val:  tensor(0.0513, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.330323469329876\n",
      "loss: 30.540524747980694\n",
      "penalty: 0.0013174448864650935\n",
      "h_val:  tensor(0.0512, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.330908107154592\n",
      "loss: 30.540389995526912\n",
      "penalty: 0.0013107652892357317\n",
      "h_val:  tensor(0.0525, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.33082853911865\n",
      "loss: 30.540293250479266\n",
      "penalty: 0.0013770833294268416\n",
      "h_val:  tensor(0.0542, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.330320016834566\n",
      "loss: 30.540226901292847\n",
      "penalty: 0.0014675599201719326\n",
      "h_val:  tensor(0.0559, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.329480482196587\n",
      "loss: 30.54014110909569\n",
      "penalty: 0.0015606884994910162\n",
      "h_val:  tensor(0.0567, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.328241259151255\n",
      "loss: 30.539988246091855\n",
      "penalty: 0.0016063640716157536\n",
      "h_val:  tensor(0.0559, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.326623163057754\n",
      "loss: 30.539753339992572\n",
      "penalty: 0.0015609933584257748\n",
      "h_val:  tensor(0.0537, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.325735147961844\n",
      "loss: 30.539559437326606\n",
      "penalty: 0.0014432782397424838\n",
      "h_val:  tensor(0.0511, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.325346014779043\n",
      "loss: 30.53939177906449\n",
      "penalty: 0.0013062836435177201\n",
      "h_val:  tensor(0.0502, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.32509951694758\n",
      "loss: 30.539262769147395\n",
      "penalty: 0.0012577049713022028\n",
      "h_val:  tensor(0.0509, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.32460774424787\n",
      "loss: 30.539122382620192\n",
      "penalty: 0.0012971620915542676\n",
      "h_val:  tensor(0.0527, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.32386776003211\n",
      "loss: 30.539007631641113\n",
      "penalty: 0.001388606253406718\n",
      "h_val:  tensor(0.0537, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.32299832657118\n",
      "loss: 30.538914181495752\n",
      "penalty: 0.0014400293581394287\n",
      "h_val:  tensor(0.0534, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.322237279365165\n",
      "loss: 30.538833467167635\n",
      "penalty: 0.0014249048868030332\n",
      "h_val:  tensor(0.0525, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.32147730410124\n",
      "loss: 30.538731900331985\n",
      "penalty: 0.0013791375204849096\n",
      "h_val:  tensor(0.0514, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.32094058377321\n",
      "loss: 30.538631279806346\n",
      "penalty: 0.0013207037060470159\n",
      "h_val:  tensor(0.0515, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.320670198819897\n",
      "loss: 30.538549997257533\n",
      "penalty: 0.0013251611746875047\n",
      "h_val:  tensor(0.0508, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.320421457562407\n",
      "loss: 30.538430278878714\n",
      "penalty: 0.0012911047015307328\n",
      "h_val:  tensor(0.0519, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.319917052468096\n",
      "loss: 30.538261197958835\n",
      "penalty: 0.001346833760202215\n",
      "h_val:  tensor(0.0521, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.319230262066323\n",
      "loss: 30.53806407555277\n",
      "penalty: 0.0013591477776686161\n",
      "h_val:  tensor(0.0509, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.318289487558754\n",
      "loss: 30.53786018825386\n",
      "penalty: 0.001295675838689302\n",
      "h_val:  tensor(0.0499, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.317438022301236\n",
      "loss: 30.53774373886024\n",
      "penalty: 0.001247019095320273\n",
      "h_val:  tensor(0.0504, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.317001552752135\n",
      "loss: 30.537696400613967\n",
      "penalty: 0.001272395167197624\n",
      "h_val:  tensor(0.0507, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.316467300783103\n",
      "loss: 30.537633902470727\n",
      "penalty: 0.0012837583363473\n",
      "h_val:  tensor(0.0512, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.315326521237104\n",
      "loss: 30.53752030688365\n",
      "penalty: 0.001309021191705006\n",
      "h_val:  tensor(0.0509, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.31388588227368\n",
      "loss: 30.53740032996726\n",
      "penalty: 0.0012939984940588507\n",
      "h_val:  tensor(0.0511, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.312683481642708\n",
      "loss: 30.537317360468002\n",
      "penalty: 0.0013059012801095893\n",
      "h_val:  tensor(0.0512, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.311940273269386\n",
      "loss: 30.53726461953093\n",
      "penalty: 0.0013109340340497334\n",
      "h_val:  tensor(0.0510, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.311186755112608\n",
      "loss: 30.53720544459046\n",
      "penalty: 0.0013002117184940576\n",
      "h_val:  tensor(0.0506, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.30981283912095\n",
      "loss: 30.537105021260118\n",
      "penalty: 0.0012786294966165344\n",
      "h_val:  tensor(0.0497, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.307539688390673\n",
      "loss: 30.536944480742726\n",
      "penalty: 0.0012349839609846675\n",
      "h_val:  tensor(0.0502, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.305348298057524\n",
      "loss: 30.536836155902424\n",
      "penalty: 0.0012618641583757619\n",
      "h_val:  tensor(0.0465, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.304272609537538\n",
      "loss: 30.53677597690525\n",
      "penalty: 0.0010810606783439585\n",
      "h_val:  tensor(0.0475, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.30331897558786\n",
      "loss: 30.536715895148497\n",
      "penalty: 0.0011272453932757755\n",
      "h_val:  tensor(0.0480, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.302451388707553\n",
      "loss: 30.53664118352954\n",
      "penalty: 0.0011536172605849896\n",
      "h_val:  tensor(0.0487, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.301708932802285\n",
      "loss: 30.53657406300337\n",
      "penalty: 0.001186097799844635\n",
      "h_val:  tensor(0.0494, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.301093259004986\n",
      "loss: 30.53652156992702\n",
      "penalty: 0.0012191951113367265\n",
      "h_val:  tensor(0.0497, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.300523370546045\n",
      "loss: 30.536476036378463\n",
      "penalty: 0.001237436909598866\n",
      "h_val:  tensor(0.0494, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.29982284976812\n",
      "loss: 30.53642242598292\n",
      "penalty: 0.001220802317423584\n",
      "h_val:  tensor(0.0485, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.298988199468837\n",
      "loss: 30.53636743775602\n",
      "penalty: 0.001177766178754341\n",
      "h_val:  tensor(0.0479, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.29839573271721\n",
      "loss: 30.536331449990207\n",
      "penalty: 0.001146819606495027\n",
      "h_val:  tensor(0.0478, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.29802051732449\n",
      "loss: 30.53630844586241\n",
      "penalty: 0.001142108284309034\n",
      "h_new:  0.047793478306334514\n",
      "layer: 0 shape: torch.Size([10, 2])\n",
      "layer: 1 shape: torch.Size([10])\n",
      "layer: 2 shape: torch.Size([10, 2])\n",
      "layer: 3 shape: torch.Size([10])\n",
      "layer: 4 shape: torch.Size([2, 5, 8])\n",
      "layer: 5 shape: torch.Size([2, 8])\n",
      "layer: 6 shape: torch.Size([2, 8, 1])\n",
      "layer: 7 shape: torch.Size([2, 1])\n",
      "rho:  1.0\n",
      "h_val:  tensor(0.0478, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.29802051732449\n",
      "loss: 30.53859266243103\n",
      "penalty: 0.003426324852927102\n",
      "h_val:  tensor(8.3323, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.347155844975887\n",
      "loss: 65.69879828880377\n",
      "penalty: 35.11194810303847\n",
      "h_val:  tensor(0.1865, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.312927251669578\n",
      "loss: 30.57697554988213\n",
      "penalty: 0.02630374076112549\n",
      "h_val:  tensor(0.0070, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.30119994192744\n",
      "loss: 30.538829395635016\n",
      "penalty: 0.0003573783856114238\n",
      "h_val:  tensor(0.0264, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.299317079983037\n",
      "loss: 30.538122249805536\n",
      "penalty: 0.0016077863853996848\n",
      "h_val:  tensor(0.0265, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.299312560016336\n",
      "loss: 30.53810302567453\n",
      "penalty: 0.001620402733148379\n",
      "h_new:  0.02653701365298744\n",
      "rho:  10.0\n",
      "h_val:  tensor(0.0265, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.299312560016336\n",
      "loss: 30.541271984595813\n",
      "penalty: 0.004789361654433167\n",
      "h_val:  tensor(29153.0575, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.3857038749076\n",
      "loss: 4249505237.1187897\n",
      "penalty: 4249505206.4749436\n",
      "h_val:  tensor(21.0231, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.358228011232537\n",
      "loss: 2241.4648555383556\n",
      "penalty: 2210.8637286140565\n",
      "h_val:  tensor(0.5489, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.320028007569842\n",
      "loss: 32.091104930202654\n",
      "penalty: 1.5324457734706574\n",
      "h_val:  tensor(0.0019, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.304157632673082\n",
      "loss: 30.541773213380903\n",
      "penalty: 0.00011085759260142488\n",
      "h_val:  tensor(0.0071, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.301144323542044\n",
      "loss: 30.53903264777012\n",
      "penalty: 0.0005914307999385553\n",
      "h_val:  tensor(0.0079, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.30103521110638\n",
      "loss: 30.539007702323683\n",
      "penalty: 0.0006932583603423989\n",
      "h_new:  0.007928674743892472\n",
      "layer: 0 shape: torch.Size([10, 2])\n",
      "layer: 1 shape: torch.Size([10])\n",
      "layer: 2 shape: torch.Size([10, 2])\n",
      "layer: 3 shape: torch.Size([10])\n",
      "layer: 4 shape: torch.Size([2, 5, 8])\n",
      "layer: 5 shape: torch.Size([2, 8])\n",
      "layer: 6 shape: torch.Size([2, 8, 1])\n",
      "layer: 7 shape: torch.Size([2, 1])\n",
      "rho:  10.0\n",
      "h_val:  tensor(0.0079, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.30103521110638\n",
      "loss: 30.539636341155624\n",
      "penalty: 0.0013218971922867825\n",
      "h_val:  tensor(3.4391, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.3385552290183\n",
      "loss: 90.15228329332339\n",
      "penalty: 59.57499622952703\n",
      "h_val:  tensor(0.0949, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.31047966745622\n",
      "loss: 30.605147167708203\n",
      "penalty: 0.057079710250021\n",
      "h_val:  tensor(0.0002, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.30289950133485\n",
      "loss: 30.540258237233125\n",
      "penalty: 1.9386990862121377e-05\n",
      "h_val:  tensor(0.0048, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.30150904931436\n",
      "loss: 30.53952580779502\n",
      "penalty: 0.0007222618778431202\n",
      "h_val:  tensor(0.0049, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.301496685780066\n",
      "loss: 30.539515879504556\n",
      "penalty: 0.0007387427046660504\n",
      "h_val:  tensor(0.0053, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.30144728502988\n",
      "loss: 30.53948056437747\n",
      "penalty: 0.0008090096228738456\n",
      "h_val:  tensor(0.0072, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.301363829608704\n",
      "loss: 30.539274425391913\n",
      "penalty: 0.001173622146389798\n",
      "h_val:  tensor(0.0050, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.301926016070155\n",
      "loss: 30.539175417787945\n",
      "penalty: 0.0007691596109664141\n",
      "h_val:  tensor(0.0044, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.301911422455643\n",
      "loss: 30.53906616733731\n",
      "penalty: 0.0006511171987656851\n",
      "h_val:  tensor(0.0039, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.301932542977937\n",
      "loss: 30.539009543544307\n",
      "penalty: 0.0005741876260910192\n",
      "h_val:  tensor(0.0038, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.301958450346525\n",
      "loss: 30.538999083355066\n",
      "penalty: 0.0005602220948224363\n",
      "h_new:  0.0038309694358460433\n",
      "rho:  100.0\n",
      "h_val:  tensor(0.0038, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.301958450346525\n",
      "loss: 30.539659518061892\n",
      "penalty: 0.0012206568016498312\n",
      "h_val:  tensor(154.4624, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.370347384804372\n",
      "loss: 1192981.617297451\n",
      "penalty: 1192951.0013383022\n",
      "h_val:  tensor(2.4548, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.33429724900309\n",
      "loss: 332.18135253360975\n",
      "penalty: 301.60810057732664\n",
      "h_val:  tensor(0.0701, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.309577356195476\n",
      "loss: 30.801172759492786\n",
      "penalty: 0.2545924847897782\n",
      "h_val:  tensor(0.0001, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.303658244043312\n",
      "loss: 30.54027226230489\n",
      "penalty: 1.7397228967315414e-05\n",
      "h_val:  tensor(0.0015, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.302491026392694\n",
      "loss: 30.539317840241466\n",
      "penalty: 0.0003099765449067883\n",
      "h_val:  tensor(0.0017, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.302451517013665\n",
      "loss: 30.53931004664655\n",
      "penalty: 0.00034946928973087594\n",
      "h_new:  0.0016625105748522273\n",
      "layer: 0 shape: torch.Size([10, 2])\n",
      "layer: 1 shape: torch.Size([10])\n",
      "layer: 2 shape: torch.Size([10, 2])\n",
      "layer: 3 shape: torch.Size([10])\n",
      "layer: 4 shape: torch.Size([2, 5, 8])\n",
      "layer: 5 shape: torch.Size([2, 8])\n",
      "layer: 6 shape: torch.Size([2, 8, 1])\n",
      "layer: 7 shape: torch.Size([2, 1])\n",
      "rho:  100.0\n",
      "h_val:  tensor(0.0017, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.302451517013665\n",
      "loss: 30.539586440787698\n",
      "penalty: 0.0006258634308804242\n",
      "h_val:  tensor(3.4773, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.338036074969313\n",
      "loss: 636.1948594162719\n",
      "penalty: 605.618111885428\n",
      "h_val:  tensor(0.1157, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.311361417114707\n",
      "loss: 31.25112140804864\n",
      "penalty: 0.7027807852620892\n",
      "h_val:  tensor(0.0011, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.304185595835026\n",
      "loss: 30.54116907035877\n",
      "penalty: 0.0003838897772022366\n",
      "h_val:  tensor(0.0008, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.30274588371235\n",
      "loss: 30.539537335854376\n",
      "penalty: 0.000267034340440722\n",
      "h_val:  tensor(0.0010, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.30268028286021\n",
      "loss: 30.539525855373345\n",
      "penalty: 0.0003310172978151573\n",
      "h_new:  0.0009685675001076532\n",
      "rho:  1000.0\n",
      "h_val:  tensor(0.0010, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.30268028286021\n",
      "loss: 30.539948010724366\n",
      "penalty: 0.0007531726488343123\n",
      "h_val:  tensor(1202.4682, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.3787961686619\n",
      "loss: 722965313.0390114\n",
      "penalty: 722965282.4097145\n",
      "h_val:  tensor(6.7063, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.346273559691646\n",
      "loss: 22519.45701464549\n",
      "penalty: 22488.870352559155\n",
      "h_val:  tensor(0.2440, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.314847710095275\n",
      "loss: 60.39954651656978\n",
      "penalty: 29.847293249795808\n",
      "h_val:  tensor(0.0046, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.30500453361127\n",
      "loss: 30.553780525921777\n",
      "penalty: 0.012093162227177346\n",
      "h_val:  tensor(9.3714e-06, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.30333715396867\n",
      "loss: 30.539902069642764\n",
      "penalty: 2.792836651583009e-06\n",
      "h_val:  tensor(0.0001, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.303138558022432\n",
      "loss: 30.539730882576766\n",
      "penalty: 4.80922923347644e-05\n",
      "h_val:  tensor(0.0008, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.30275631476294\n",
      "loss: 30.53979541230235\n",
      "penalty: 0.0005395250336297869\n",
      "h_val:  tensor(0.0004, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.30297079905884\n",
      "loss: 30.53966121740168\n",
      "penalty: 0.00016578177852906033\n",
      "h_val:  tensor(0.0004, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.302952899097022\n",
      "loss: 30.539656689676978\n",
      "penalty: 0.00018646326662654023\n",
      "h_new:  0.00038414179739687526\n",
      "layer: 0 shape: torch.Size([10, 2])\n",
      "layer: 1 shape: torch.Size([10])\n",
      "layer: 2 shape: torch.Size([10, 2])\n",
      "layer: 3 shape: torch.Size([10])\n",
      "layer: 4 shape: torch.Size([2, 5, 8])\n",
      "layer: 5 shape: torch.Size([2, 8])\n",
      "layer: 6 shape: torch.Size([2, 8, 1])\n",
      "layer: 7 shape: torch.Size([2, 1])\n",
      "rho:  1000.0\n",
      "h_val:  tensor(0.0004, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.302952899097022\n",
      "loss: 30.539804254597488\n",
      "penalty: 0.0003340281871338422\n",
      "h_val:  tensor(7.3089, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.34666552488713\n",
      "loss: 26745.484308228843\n",
      "penalty: 26714.89781940153\n",
      "h_val:  tensor(0.2740, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.315525308458383\n",
      "loss: 68.26553177413722\n",
      "penalty: 37.71273071137829\n",
      "h_val:  tensor(0.0063, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.305282835328995\n",
      "loss: 30.565982886001212\n",
      "penalty: 0.024044790741155616\n",
      "h_val:  tensor(7.5047e-07, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.303419413109253\n",
      "loss: 30.53996484414745\n",
      "penalty: 5.087081848686533e-07\n",
      "h_val:  tensor(0.0002, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.303086414962266\n",
      "loss: 30.53976196449334\n",
      "penalty: 0.0001503250487528265\n",
      "h_val:  tensor(0.0002, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.30308177696647\n",
      "loss: 30.539759489623393\n",
      "penalty: 0.00015565214775920328\n",
      "h_val:  tensor(0.0002, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.30306322774123\n",
      "loss: 30.539751269943576\n",
      "penalty: 0.00017863751711788006\n",
      "h_new:  0.00022598959663744367\n",
      "rho:  10000.0\n",
      "h_val:  tensor(0.0002, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.30306322774123\n",
      "loss: 30.539981090783623\n",
      "penalty: 0.00040845835716547525\n",
      "h_val:  tensor(4443.5047, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.380531415152515\n",
      "loss: 98723671829.12627\n",
      "penalty: 98723671798.4924\n",
      "h_val:  tensor(11.5526, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.35160211110111\n",
      "loss: 667351.1928279508\n",
      "penalty: 667320.6000644322\n",
      "h_val:  tensor(0.4276, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.318292754183716\n",
      "loss: 945.1403514053238\n",
      "penalty: 914.5844282728643\n",
      "h_val:  tensor(0.0113, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.305896098315674\n",
      "loss: 31.183788702072953\n",
      "penalty: 0.6411793431906316\n",
      "h_val:  tensor(9.4340e-05, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.303640775437625\n",
      "loss: 30.540300147460748\n",
      "penalty: 0.00010841353329022111\n",
      "h_val:  tensor(4.2603e-05, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.30326190209145\n",
      "loss: 30.539823539464795\n",
      "penalty: 3.793702448244846e-05\n",
      "h_val:  tensor(6.0104e-05, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.303233447208893\n",
      "loss: 30.539812790185227\n",
      "penalty: 5.878139544342971e-05\n",
      "h_new:  6.010407768819448e-05\n",
      "layer: 0 shape: torch.Size([10, 2])\n",
      "layer: 1 shape: torch.Size([10])\n",
      "layer: 2 shape: torch.Size([10, 2])\n",
      "layer: 3 shape: torch.Size([10])\n",
      "layer: 4 shape: torch.Size([2, 5, 8])\n",
      "layer: 5 shape: torch.Size([2, 8])\n",
      "layer: 6 shape: torch.Size([2, 8, 1])\n",
      "layer: 7 shape: torch.Size([2, 1])\n",
      "rho:  10000.0\n",
      "h_val:  tensor(6.0104e-05, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.303233447208893\n",
      "loss: 30.539848915186777\n",
      "penalty: 9.490639699091488e-05\n",
      "h_val:  tensor(0.3550, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.316934586289065\n",
      "loss: 661.2407648601351\n",
      "penalty: 630.6872120400695\n",
      "h_val:  tensor(0.0098, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.30576037612914\n",
      "loss: 31.038973586868135\n",
      "penalty: 0.4966772288871176\n",
      "h_val:  tensor(0.0001, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.303700745507644\n",
      "loss: 30.54052545957228\n",
      "penalty: 0.00030137373148134974\n",
      "h_val:  tensor(3.8727e-05, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.30327059293909\n",
      "loss: 30.539848386375308\n",
      "penalty: 5.70121455827737e-05\n",
      "h_new:  3.872714214203654e-05\n",
      "rho:  100000.0\n",
      "h_val:  tensor(3.8727e-05, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.30327059293909\n",
      "loss: 30.53991587699454\n",
      "penalty: 0.00012450276481480133\n",
      "h_val:  tensor(178.9204, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.372561325823973\n",
      "loss: 1600626303.5237134\n",
      "penalty: 1600626272.9048178\n",
      "h_val:  tensor(2.8134, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.336410220087068\n",
      "loss: 395807.28292323014\n",
      "penalty: 395776.7073178034\n",
      "h_val:  tensor(0.1031, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.31091727270216\n",
      "loss: 561.975430891038\n",
      "penalty: 531.4274385329021\n",
      "h_val:  tensor(0.0026, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.30461077954162\n",
      "loss: 30.882404632156963\n",
      "penalty: 0.34117636957554\n",
      "h_val:  tensor(2.6643e-05, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.30353703190319\n",
      "loss: 30.540146595826794\n",
      "penalty: 6.955687755896606e-05\n",
      "h_val:  tensor(8.4679e-06, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.30334812349223\n",
      "loss: 30.539888910975776\n",
      "penalty: 1.4411551049455913e-05\n",
      "h_new:  8.467879784035404e-06\n",
      "layer: 0 shape: torch.Size([10, 2])\n",
      "layer: 1 shape: torch.Size([10])\n",
      "layer: 2 shape: torch.Size([10, 2])\n",
      "layer: 3 shape: torch.Size([10])\n",
      "layer: 4 shape: torch.Size([2, 5, 8])\n",
      "layer: 5 shape: torch.Size([2, 8])\n",
      "layer: 6 shape: torch.Size([2, 8, 1])\n",
      "layer: 7 shape: torch.Size([2, 1])\n",
      "rho:  100000.0\n",
      "h_val:  tensor(8.4679e-06, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.30334812349223\n",
      "loss: 30.53989608147458\n",
      "penalty: 2.158204985314346e-05\n",
      "h_val:  tensor(0.4169, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.29061074042677\n",
      "loss: 8720.72890904453\n",
      "penalty: 8690.201641748103\n",
      "h_val:  tensor(0.0130, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.300957056461726\n",
      "loss: 38.98799939884585\n",
      "penalty: 8.450495415454446\n",
      "h_val:  tensor(0.0004, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.302967606667462\n",
      "loss: 30.548947475267195\n",
      "penalty: 0.009450312026333858\n",
      "h_val:  tensor(2.0981e-05, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.303310816979092\n",
      "loss: 30.53990410310092\n",
      "penalty: 6.659978817903365e-05\n",
      "h_val:  tensor(1.1341e-05, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.303337794922353\n",
      "loss: 30.53989478943629\n",
      "penalty: 3.053267569207788e-05\n",
      "h_new:  1.1340605299725581e-05\n",
      "rho:  1000000.0\n",
      "h_val:  tensor(1.1341e-05, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.303337794922353\n",
      "loss: 30.539952663634143\n",
      "penalty: 8.840687354595166e-05\n",
      "h_val:  tensor(6104.6248, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.381670724327105\n",
      "loss: 18633222239618.652\n",
      "penalty: 18633222239588.016\n",
      "h_val:  tensor(13.1791, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.35335711043219\n",
      "loss: 86844778.54875971\n",
      "penalty: 86844747.95393927\n",
      "h_val:  tensor(0.4917, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.319337649937133\n",
      "loss: 120921.63297199356\n",
      "penalty: 120891.07590633376\n",
      "h_val:  tensor(0.0147, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.306256668767926\n",
      "loss: 138.5166636851922\n",
      "penalty: 107.97366671247912\n",
      "h_val:  tensor(0.0003, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.30384989204326\n",
      "loss: 30.599962190249606\n",
      "penalty: 0.0595483309228412\n",
      "h_val:  tensor(1.7286e-06, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.303447272575887\n",
      "loss: 30.539986921044118\n",
      "penalty: 5.16798903678079e-06\n",
      "h_val:  tensor(3.0378e-06, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.303375794603838\n",
      "loss: 30.53991611007999\n",
      "penalty: 1.1070319848273549e-05\n",
      "h_val:  tensor(3.6166e-06, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.303372119701535\n",
      "loss: 30.53991510666943\n",
      "penalty: 1.4226383858030132e-05\n",
      "h_new:  3.6166208046140014e-06\n",
      "rho:  10000000.0\n",
      "h_val:  tensor(3.6166e-06, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.303372119701535\n",
      "loss: 30.53997396642663\n",
      "penalty: 7.308614105768085e-05\n",
      "h_val:  tensor(102223.4001, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.385638884935663\n",
      "loss: 5.224811762068395e+16\n",
      "penalty: 5.224811762068393e+16\n",
      "h_val:  tensor(36.9455, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.36288547776494\n",
      "loss: 6824840989.789901\n",
      "penalty: 6824840959.183482\n",
      "h_val:  tensor(1.0992, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.32622703860973\n",
      "loss: 6040841.581916113\n",
      "penalty: 6040811.017335416\n",
      "h_val:  tensor(0.0364, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.30787980512653\n",
      "loss: 6638.033494456599\n",
      "penalty: 6607.488743518892\n",
      "h_val:  tensor(0.0010, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.304140599089482\n",
      "loss: 35.1364178107542\n",
      "penalty: 4.595690137723283\n",
      "h_val:  tensor(1.7460e-05, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.303514210463636\n",
      "loss: 30.541615062425393\n",
      "penalty: 0.001561307728442021\n",
      "h_val:  tensor(1.0831e-09, grad_fn=<SubBackward0>)\n",
      "squared loss: 27.303415827883214\n",
      "loss: 30.539947908009626\n",
      "penalty: 2.307760615081174e-09\n",
      "h_new:  1.0830909502601571e-09\n",
      "[[0.00000000e+00 1.14980585e+01]\n",
      " [2.86225270e-06 0.00000000e+00]]\n",
      "[[0. 1.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "x = np.random.normal(0, 1, 10) \n",
    "\n",
    "np.random.seed(24)\n",
    "epsilon = np.random.normal(0,1, 10) \n",
    "\n",
    "\n",
    "y = [(np.sin(x) + epsilon) *10 for x, epsilon in zip(x, epsilon)]\n",
    "y = np.array(y)\n",
    "\n",
    "X = np.column_stack((x, y))\n",
    "n = X.shape[0]\n",
    "d = X.shape[1]\n",
    "\n",
    "model = NotearsMLP(dims=[d, 5, 8, 1], bias=True)\n",
    "W_est, output = notears_nonlinear(model, X, lambda1=0.01, lambda2=0.01)\n",
    "print(W_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1f3301ecb10>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0EAAAH5CAYAAABH+zXoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/SrBM8AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAuh0lEQVR4nO3df3CdVZ0/8M9t2qZUmpRCaFISoKUFRmT5UYduu0TStWu7slqMZRVZBRYLKju2/FCLfgXBxSKwWGQYwFFad2a/wsJEXHVFSwWtUqsiHbFSbLHQNk0qgiQFJdX0+f5xv42EpG3S5uYm97xeM8/Q5zznJp97ehvz9jzPObksy7IAAABIxIhiFwAAADCYhCAAACApQhAAAJAUIQgAAEiKEAQAACRFCAIAAJIiBAEAAEkZWewCDtbu3btj+/btMW7cuMjlcsUuBwAAKJIsy2Lnzp0xadKkGDFi7/M9wz4Ebd++Perq6opdBgAAMERs3bo1amtr93p92IegcePGRUT+jVZUVBS5GgAAoFja29ujrq6uKyPszbAPQXtugauoqBCCAACA/T4mY2EEAAAgKUIQAACQFCEIAABIihAEAAAkRQgCAACSIgQBAABJEYIAAICkCEEAAEBShCAAACApQhAAAJAUIQgAAEiKEAQAACRFCAIAAJIystgFAAAMps7OiNWrI1paImpqIurrI8rKil0VMJiEIAAgGU1NEYsWRWzb9te22tqI226LaGwsXl3A4HI7HACQhKamiAULugegiIjm5nx7U1Nx6gIGnxAEAJS8zs78DFCW9by2p23x4nw/oPQJQQBAyVu9uucM0GtlWcTWrfl+QOkTggCAktfSMrD9gOFNCAIASl5NzcD2A4a3goagH/7wh/GOd7wjJk2aFLlcLh588MFu17Msi2uuuSZqamrikEMOiTlz5sTGjRsLWRIAkKD6+vwqcLlc79dzuYi6unw/oPQVNAS98sorccopp8Qdd9zR6/WbbropvvjFL8Zdd90Va9eujTe84Q0xd+7cePXVVwtZFgCQmLKy/DLYET2D0J7zZcvsFwSpyGVZb+ukFOAb5XLx9a9/Pc4555yIyM8CTZo0Ka688sq46qqrIiKira0tJk6cGCtWrIj3vve9ffq67e3tUVlZGW1tbVFRUVGo8gGAEtDbPkF1dfkAZJ8gGP76mg2Ktlnq5s2bo7W1NebMmdPVVllZGTNmzIg1a9bsNQR1dHRER0dH13l7e3vBawUASkNjY8T8+flV4Fpa8s8A1debAYLUFC0Etba2RkTExIkTu7VPnDix61pvli5dGtddd11BawMASldZWURDQ7GrAIpp2K0Od/XVV0dbW1vXsXXr1mKXBAAADCNFC0HV1dUREbFjx45u7Tt27Oi61pvy8vKoqKjodgAAAPRV0ULQ5MmTo7q6OlatWtXV1t7eHmvXro2ZM2cWqywAAKDEFfSZoJdffjk2bdrUdb558+ZYt25dTJgwIY4++uhYvHhx/Pu//3tMmzYtJk+eHJ/+9Kdj0qRJXSvIATA8dHZ60ByA4aOgIejnP/95zJ49u+v8iiuuiIiICy64IFasWBEf//jH45VXXolLLrkkXnrppTjzzDPjoYceijFjxhSyLAAGUG9LDtfW5vdkseQwAEPRoO0TVCj2CQIonqamiAULIl7/vyR7Np984AFBCIDB09dsMOxWhwNgaOjszM8A9fZ/pe1pW7w43w8AhhIhCIADsnp191vgXi/LIrZuzfcDgKFECALggLS0DGw/ABgsQhAAB6SmZmD7AcBgEYIAOCD19flV4PYsgvB6uVxEXV2+HwAMJUIQAAekrCy/DHZEzyC053zZMvsFATD0CEEAHLDGxvwy2Ecd1b29ttby2AAMXQXdLBWA0tfYGDF/fn4VuJaW/DNA9fVmgAAYuoQgAA5aWVlEQ0OxqwCAvnE7HAAAkBQhCAAASIoQBAAAJEUIAgAAkiIEAQAASRGCAACApAhBAABAUoQgAAAgKUIQAACQFCEIAABIihAEAAAkRQgCAACSIgQBAABJEYIAAICkjCx2AQAMYxs3Ruzcuffr48ZFTJs2ePUAQB8IQQAcmI0bI44/fv/9fvMbQQiAIcXtcAAcmH3NAB1IPwAYJEIQAACQFCEIAABIihAEAAAkRQgCAACSIgQBAABJEYIAAICkCEEAHJhx4wa2HwAMEpulAnBgpk3Lb4S6r32Axo2zUSoAQ44QBMCBE3AAGIbcDgcAACRFCAIAAJIiBAEAAEkRggAAgKQIQQAAQFKEIAAAIClCEAAAkBQhCAAASIoQBAAAJEUIAgAAkiIEAQAASRGCAACApAhBAABAUooegj7zmc9ELpfrdpx44onFLgsAAChRI4tdQETESSedFA8//HDX+ciRQ6IsAACgBA2JtDFy5Miorq4udhkAAEACin47XETExo0bY9KkSTFlypQ4//zzY8uWLXvt29HREe3t7d0OAACAvip6CJoxY0asWLEiHnroobjzzjtj8+bNUV9fHzt37uy1/9KlS6OysrLrqKurG+SKAQCA4SyXZVlW7CJe66WXXopjjjkmbr311rj44ot7XO/o6IiOjo6u8/b29qirq4u2traoqKgYzFIBAIAhpL29PSorK/ebDYbEM0GvNX78+Dj++ONj06ZNvV4vLy+P8vLyQa4KAAAoFUW/He71Xn755XjmmWeipqam2KUAAAAlqOgh6Kqrroof/OAH8eyzz8Zjjz0W73rXu6KsrCzOO++8YpcGAACUoKLfDrdt27Y477zz4oUXXoiqqqo488wz4yc/+UlUVVUVuzQAAKAEFT0E3XvvvcUuAQAASEjRb4cDAAAYTEIQAACQFCEIAABIihAEAAAkRQgCAACSIgQBAABJEYIAAICkCEEAAEBShCAAACApQhAAAJAUIQgAAEiKEAQAACRFCAIAAJIiBAEAAEkRggAAgKQIQQAAQFKEIAAAIClCEAAAkBQhCAAASIoQBAAAJEUIAgAAkiIEAQAASRGCAACApAhBAABAUoQgAAAgKUIQAACQFCEIAABIihAEAAAkRQgCAACSIgQBAABJEYIAAICkCEEAAEBSRha7AAAoCRs3Ruzcuffr48ZFTJs2ePUAsFdCEAAcrI0bI44/fv/9fvMbQQhgCBCCAOBg7WsG6ED6URhm62DgDPN/T0IQAFD6zNbBwCmBf08WRgAASp/ZOhg4JfDvSQgCAACSIgQBAABJEYIAAICkCEEAAEBShCAAOFjjxg1sPwAKyhLZAHCwpk3LLwU7jPfMAEiJEAQAA0HAGdrM1sHAKYF/T0IQAFD6zNbBwCmBf09CEACQhiH8CxkMO8P835MQBAAAHJDOzojVqyNaWiJqaiLq6yPKyopd1f4JQQAAQL81NUUsWhSxbdtf22prI267LaKxsXh19YUlsgEAgH5paopYsKB7AIqIaG7Otzc1FaeuvhoSIeiOO+6IY489NsaMGRMzZsyIn/70p8UuCQAA6EVnZ34GKMt6XtvTtnhxvt9QVfQQdN9998UVV1wR1157bfziF7+IU045JebOnRu/+93vil0aAADwOqtX95wBeq0si9i6Nd9vqCp6CLr11ltj4cKFcdFFF8Ub3/jGuOuuu2Ls2LFxzz339Nq/o6Mj2tvbux0AAMDgaGkZ2H7FUNQQtGvXrnj88cdjzpw5XW0jRoyIOXPmxJo1a3p9zdKlS6OysrLrqKurG6xyAQAgeTU1A9uvGIoagn7/+99HZ2dnTJw4sVv7xIkTo7W1tdfXXH311dHW1tZ1bN26dTBKBQAAIr8Mdm1tRC7X+/VcLqKuLt9vqCr67XD9VV5eHhUVFd0OAABgcJSV5ZfBjugZhPacL1s2tPcLKmoIOuKII6KsrCx27NjRrX3Hjh1RXV1dpKoAAIB9aWyMeOCBiKOO6t5eW5tvt0/QPowePTqmT58eq1at6mrbvXt3rFq1KmbOnFnEygAAgH1pbIx49tmIRx6J+L//N//fzZuHfgCKiBhZ7AKuuOKKuOCCC+LNb35znHHGGbFs2bJ45ZVX4qKLLip2aQAAwD6UlUU0NBS7iv4regh6z3veE88//3xcc8010draGqeeemo89NBDPRZLAAAAGAi5LOttr9fho729PSorK6Otrc0iCQAAkLC+ZoNhtzocAADAwRCCAACApAhBAABAUoQgAAAgKUIQAACQFCEIAABIihAEAAAkRQgCAACSIgQBAABJEYIAAICkCEEAAEBShCAAACApQhAAAJAUIQgAAEiKEAQAACRFCAIAAJIiBAEAAEkRggAAgKQIQQAAQFKEIAAAIClCEAAAkBQhCAAASIoQBAAAJEUIAgAAkiIEAQAASRGCAACApAhBAABAUoQgAAAgKUIQAACQFCEIAABIihAEAAAkRQgCAACSIgQBAABJEYIAAICkCEEAAEBShCAAACApQhAAAJAUIQgAAEiKEAQAACRFCAIAAJIiBAEAAEkRggAAgKQIQQAAQFKEIAAAIClCEAAAkBQhCAAASIoQBAAAJEUIAgAAklLUEHTsscdGLpfrdtx4443FLAkAAChxI4tdwPXXXx8LFy7sOh83blwRqwEAAEpd0UPQuHHjorq6uthlAAAAiSj6M0E33nhjHH744XHaaafFzTffHH/5y1/22b+joyPa29u7HQAAAH1V1Jmgj370o3H66afHhAkT4rHHHourr746Wlpa4tZbb93ra5YuXRrXXXfdIFYJAACUklyWZdlAfsElS5bE5z//+X32eeqpp+LEE0/s0X7PPffEpZdeGi+//HKUl5f3+tqOjo7o6OjoOm9vb4+6urpoa2uLioqKgyseAAAYttrb26OysnK/2WDAQ9Dzzz8fL7zwwj77TJkyJUaPHt2jff369fGmN70pNmzYECeccEKfvl9f3ygAAFDa+poNBvx2uKqqqqiqqjqg165bty5GjBgRRx555ABXBaSmszNi9eqIlpaImpqI+vqIsrJiVwUADAVFeyZozZo1sXbt2pg9e3aMGzcu1qxZE5dffnn8y7/8Sxx22GHFKgsoAU1NEYsWRWzb9te22tqI226LaGwsXl0AwNBQtBBUXl4e9957b3zmM5+Jjo6OmDx5clx++eVxxRVXFKskoAQ0NUUsWBDx+ht9m5vz7Q88IAgBQOoG/JmgweaZIGCPzs6IY4/tPgP0WrlcfkZo82a3xgFAKeprNij6PkEAA2X16r0HoIj87NDWrfl+AEC6hCCgZLS0DGw/AKA0CUFAyaipGdh+AEBpEoKAklFfn3/mJ5fr/XouF1FXl+8HAKRLCAJKRllZfhnsiJ5BaM/5smUWRQCA1AlBQElpbMwvg33UUd3ba2stjw0A5BVtnyCg8Do78yuhtbTkn4Opr09jFqSxMWL+/DTfOwCwf0IQlKimpohFi7ovGV1bm79dLIXZkLKyiIaGYlcBAAxFboeDEtTUFLFgQc89c5qb8+1NTcWpCwBgKBCCoMR0duZngLKs57U9bYsX5/sBAKRICIISs3p1zxmg18qyiK1b8/0AAFIkBEGJaWkZ2H4AAKVGCIISU1MzsP0AAEqNEAQlpr4+vwrc6zcL3SOXi6iry/cDAEiREAQlpqwsvwx2RM8gtOd82TJ75gAA6RKCoAQ1NkY88EDEUUd1b6+tzbensE8QAMDe2CwVSlRjY8T8+flV4Fpa8s8A1debAQIAEIKghJWVRTQ0FLsKAIChxe1wAABAUoQgAAAgKUIQAACQFCEIAABIihAEAAAkRQgCAACSIgQBAABJEYIAAICk2CwVAAZQZ2fE6tURLS0RNTUR9fX5jYsBGDqEIAAYIE1NEYsWRWzb9te22tqI226LaGwsXl0AdOd2OAAYAE1NEQsWdA9AERHNzfn2pqbi1AVAT0IQABykzs78DFCW9by2p23x4nw/AIpPCAKAg7R6dc8ZoNfKsoitW/P9ACg+IQgADlJLy8D2A6CwhCAAOEg1NQPbD4DCEoIA4CDV1+dXgcvler+ey0XU1eX7AVB8QhAAHKSysvwy2BE9g9Ce82XL7BcEMFQIQQAwABobIx54IOKoo7q319bm2+0TBDB02CwVAAZIY2PE/Pn5VeBaWvLPANXXmwECGGqEIAAYQGVlEQ0Nxa4CgH1xOxwAAJAUIQgAAEiKEAQAACRFCAIAAJIiBAEAAEkRggAAgKQIQQAAQFKEIAAAIClCEAAAkBQhCAAASErBQtANN9wQs2bNirFjx8b48eN77bNly5Y4++yzY+zYsXHkkUfGxz72sfjLX/5SqJIAAABiZKG+8K5du+Lcc8+NmTNnxle+8pUe1zs7O+Pss8+O6urqeOyxx6KlpSU+8IEPxKhRo+Jzn/tcocoCAAASl8uyLCvkN1ixYkUsXrw4XnrppW7t3/nOd+Kf/umfYvv27TFx4sSIiLjrrrviE5/4RDz//PMxevToPn399vb2qKysjLa2tqioqBjo8gEAgGGir9mgaM8ErVmzJk4++eSuABQRMXfu3Ghvb4/169fv9XUdHR3R3t7e7QAAAOirooWg1tbWbgEoIrrOW1tb9/q6pUuXRmVlZddRV1dX0DoBAIDS0q8QtGTJksjlcvs8NmzYUKhaIyLi6quvjra2tq5j69atBf1+AABAaenXwghXXnllXHjhhfvsM2XKlD59rerq6vjpT3/arW3Hjh1d1/amvLw8ysvL+/Q9AAAAXq9fIaiqqiqqqqoG5BvPnDkzbrjhhvjd734XRx55ZERErFy5MioqKuKNb3zjgHwPAACA1yvYEtlbtmyJF198MbZs2RKdnZ2xbt26iIiYOnVqHHroofG2t70t3vjGN8b73//+uOmmm6K1tTX+z//5P3HZZZeZ6QEAAAqmYEtkX3jhhfHVr361R/sjjzwSDQ0NERHx3HPPxYc//OF49NFH4w1veENccMEFceONN8bIkX3PZpbIBgAAIvqeDQq+T1ChCUEAAEDEMNgnCAAAoBiEIAAAIClCEAAAkBQhCAAASIoQBAAAJEUIAgAAkiIEAQAASen7rqTsU2dnxOrVES0tETU1EfX1EWVlxa4KAAB4PSFoADQ1RSxaFLFt21/bamsjbrstorGxeHUBAAA9uR3uIDU1RSxY0D0ARUQ0N+fbm5qKUxcAANA7IeggdHbmZ4CyrOe1PW2LF+f7AQAAQ4MQdBBWr+45A/RaWRaxdWu+HwAAMDQIQQehpWVg+wEAAIUnBB2EmpqB7QcAABSeEHQQ6uvzq8Dlcr1fz+Ui6ury/QAAgKFBCDoIZWX5ZbAjegahPefLltkvCAAAhhIh6CA1NkY88EDEUUd1b6+tzbfbJwgAAIYWm6UOgMbGiPnz86vAtbTknwGqrzcDBAAAQ5EQNEDKyiIaGopdBQAAsD9uhwMAAJIiBAEAAEkRggAAgKQIQQAAQFKEIAAAIClCEAAAkBQhCAAASIoQBAAAJEUIAgAAkiIEAQAASRGCAACApAhBAABAUoQgAAAgKUIQAACQFCEIAABIihAEAAAkRQgCAACSIgQBAABJEYIAAICkCEEAAEBShCAAACApQhAAAJCUkcUuAKDoNm6M2Llz79fHjYuYNm3w6gEACkoIAtK2cWPE8cfvv99vfiMIAUCJcDsckLZ9zQAdSD8AYMgTggAAgKS4HQ4oLZ7vAQD2QwgCSofnewCAPnA7HFA6PN8DAPRBwULQDTfcELNmzYqxY8fG+PHje+2Ty+V6HPfee2+hSgIAACjc7XC7du2Kc889N2bOnBlf+cpX9tpv+fLlMW/evK7zvQUmYJjyjA4AMMQULARdd911ERGxYsWKffYbP358VFdXF6qMwkv9F7zU3/9QNVT+XobDMzrjxg1sPwBgyCv6wgiXXXZZfPCDH4wpU6bEhz70objooosil8vttX9HR0d0dHR0nbe3tw9Gmb0bDr/gFVLq73+oGkp/L8PhGZ1p0/JjMRRCIwAwKIoagq6//vr4+7//+xg7dmx873vfi4985CPx8ssvx0c/+tG9vmbp0qVds0xFNxx+wSuk1N//UOXvpf8EHABISr8WRliyZEmvixm89tiwYUOfv96nP/3p+Lu/+7s47bTT4hOf+ER8/OMfj5tvvnmfr7n66qujra2t69i6dWt/3gIAAJC4fs0EXXnllXHhhRfus8+UKVMOuJgZM2bEZz/72ejo6Ijy8vJe+5SXl+/1GpA4z/cAAH3QrxBUVVUVVVVVhaol1q1bF4cddpiQAxwYz/cAAH1QsGeCtmzZEi+++GJs2bIlOjs7Y926dRERMXXq1Dj00EPjm9/8ZuzYsSP+9m//NsaMGRMrV66Mz33uc3HVVVcVqiQgBQIOALAfBQtB11xzTXz1q1/tOj/ttNMiIuKRRx6JhoaGGDVqVNxxxx1x+eWXR5ZlMXXq1Lj11ltj4cKFhSoJAACgcCFoxYoV+9wjaN68ed02SQVKkGd0AIAhqOj7BA1rqf+Cl/r7H6qG0t+LZ3QAgCFICDoYqf+Cl/r7H6qG2t+Lv38AYIgRgg5W6r/gpf7+hyp/LwAAe9WvzVIBAACGOyEIAABIihAEAAAkRQgCAACSIgQBAABJEYIAAICkCEEAAEBShCAAACApQhAAAJAUIQgAAEiKEAQAACRFCAIAAJIiBAEAAEkRggAAgKQIQQAAQFKEIAAAIClCEAAAkBQhCAAASIoQBAAAJEUIAgAAkiIEAQAASRGCAACApAhBAABAUoQgAAAgKUIQAACQFCEIAABIihAEAAAkRQgCAACSIgQBAABJEYIAAICkCEEAAEBShCAAACApQhAAAJAUIQgAAEiKEAQAACRFCAIAAJIiBAEAAEkRggAAgKQIQQAAQFKEIAAAIClCEAAAkBQhCAAASIoQBAAAJEUIAgAAkiIEAQAASRGCAACApBQsBD377LNx8cUXx+TJk+OQQw6J4447Lq699trYtWtXt36//OUvo76+PsaMGRN1dXVx0003FaokAACAGFmoL7xhw4bYvXt33H333TF16tT41a9+FQsXLoxXXnklbrnlloiIaG9vj7e97W0xZ86cuOuuu+LJJ5+Mf/3Xf43x48fHJZdcUqjSAACAhOWyLMsG65vdfPPNceedd8Zvf/vbiIi4884741Of+lS0trbG6NGjIyJiyZIl8eCDD8aGDRt6/RodHR3R0dHRdd7e3h51dXXR1tYWFRUVhX8TAADAkNTe3h6VlZX7zQaD+kxQW1tbTJgwoet8zZo18Za3vKUrAEVEzJ07N55++un4wx/+0OvXWLp0aVRWVnYddXV1Ba8bAAAoHYMWgjZt2hS33357XHrppV1tra2tMXHixG799py3trb2+nWuvvrqaGtr6zq2bt1auKLps87OiEcfjfja1/L/7ewsdkUAANC7foegJUuWRC6X2+fx+lvZmpubY968eXHuuefGwoULD6rg8vLyqKio6HZQXE1NEcceGzF7dsT73pf/77HH5tsBAGCo6ffCCFdeeWVceOGF++wzZcqUrj9v3749Zs+eHbNmzYovfelL3fpVV1fHjh07urXtOa+uru5vaRRBU1PEggURr3+yrLk53/7AAxGNjcWpDQAAetPvEFRVVRVVVVV96tvc3ByzZ8+O6dOnx/Lly2PEiO4TTzNnzoxPfepT8ec//zlGjRoVERErV66ME044IQ477LD+lsYg6+yMWLSoZwCKyLflchGLF0fMnx9RVjbo5QEAQK8K9kxQc3NzNDQ0xNFHHx233HJLPP/889Ha2trtWZ/3ve99MXr06Lj44otj/fr1cd9998Vtt90WV1xxRaHKYgCtXh2xbdver2dZxNat+X4AADBUFGyfoJUrV8amTZti06ZNUVtb2+3anlW5Kysr43vf+15cdtllMX369DjiiCPimmuusUfQMNHSMrD9AABgMAzqPkGF0Ne1wBl4jz6aXwRhfx55JKKhodDVAACQuiG5TxClpb4+orY2/+xPb3K5iLq6fD8AABgqhCAOWFlZxG235f/8+iC053zZMosiAAAwtAhBHJTGxvwy2Ecd1b29ttby2AAADE0FWxiBdDQ25pfBXr06vwhCTU3+FjgzQAAADEVCEAOirMziBwAADA9uhwMAAJIiBAEAAEkRggAAgKQIQQAAQFIsjADw/3V2WuUQAFIgBAFERFNTxKJFEdu2/bWttja/IbD9rgCgtLgdDkheU1PEggXdA1BERHNzvr2pqTh1AQCFIQQBSevszM8AZVnPa3vaFi/O9wMASoMQBCRt9eqeM0CvlWURW7fm+wEApUEIApLW0jKw/QCAoU8IApJWUzOw/QCAoU8IApJWX59fBS6X6/16LhdRV5fvBwCUBiEISFpZWX4Z7IieQWjP+bJl9gsCgFIiBAHJa2yMeOCBiKOO6t5eW5tvt08QAJQWm6UCRD7ozJ+fXwWupSX/DFB9vRkgAChFQhDA/1dWFtHQUOwqAIBCczscAACQFCEIAABIihAEAAAkRQgCAACSIgQBAABJEYIAAICkCEEAAEBShCAAACApQhAAAJAUIQgAAEiKEAQAACRFCAIAAJIiBAEAAEkRggAAgKQIQQAAQFKEIAAAIClCEAAAkBQhCAAASIoQBAAAJEUIAgAAkiIEAQAASRGCAACApAhBAABAUoQgAAAgKUIQAACQlJHFLgBIQ2dnxOrVES0tETU1EfX1EWVlxa4KAEiREAQUXFNTxKJFEdu2/bWttjbittsiGhuLVxcAkKaC3Q737LPPxsUXXxyTJ0+OQw45JI477ri49tprY9euXd365HK5HsdPfvKTQpUFDLKmpogFC7oHoIiI5uZ8e1NTceoCANJVsJmgDRs2xO7du+Puu++OqVOnxq9+9atYuHBhvPLKK3HLLbd06/vwww/HSSed1HV++OGHF6osYBB1duZngLKs57Usi8jlIhYvjpg/361xAMDgKVgImjdvXsybN6/rfMqUKfH000/HnXfe2SMEHX744VFdXV2oUoAiWb265wzQa2VZxNat+X4NDYNWFgCQuEFdHa6trS0mTJjQo/2d73xnHHnkkXHmmWfG//zP/+zza3R0dER7e3u3AxiaWloGth8AwEAYtBC0adOmuP322+PSSy/tajv00EPjP/7jP+L++++Pb3/723HmmWfGOeecs88gtHTp0qisrOw66urqBqN84ADU1AxsPwCAgZDLst7u1t+7JUuWxOc///l99nnqqafixBNP7Dpvbm6Os846KxoaGuLLX/7yPl/7gQ98IDZv3hyrV6/u9XpHR0d0dHR0nbe3t0ddXV20tbVFRUVFP94JUGidnRHHHptfBKG3nzS5XH6VuM2bPRMEABy89vb2qKys3G826PczQVdeeWVceOGF++wzZcqUrj9v3749Zs+eHbNmzYovfelL+/36M2bMiJUrV+71enl5eZSXl/e5XqB4ysryy2AvWJAPPK8NQrlc/r/LlglAAMDg6ncIqqqqiqqqqj71bW5ujtmzZ8f06dNj+fLlMWLE/u++W7duXdS4NwZKRmNjxAMP9L5P0LJl9gkCAAZfwVaHa25ujoaGhjjmmGPilltuieeff77r2p6V4L761a/G6NGj47TTTouIiKamprjnnnv2e8scMLw0NuaXwV69Or8IQk1NRH29GSAAoDgKFoJWrlwZmzZtik2bNkVtbW23a699DOmzn/1sPPfcczFy5Mg48cQT47777osFCxYUqiygSMrKLIMNAAwN/V4YYajp68NPAABAaetrNhjUfYIAAACKTQgCAACSIgQBAABJEYIAAICkCEEAAEBShCAAACApQhAAAJAUIQgAAEiKEAQAACRFCAIAAJIiBAEAAEkRggAAgKSMLHYBByvLsoiIaG9vL3IlAABAMe3JBHsywt4M+xC0c+fOiIioq6srciUAAMBQsHPnzqisrNzr9Vy2v5g0xO3evTu2b98e48aNi1wuV+xyBkV7e3vU1dXF1q1bo6KiotjlJMGYDz5jPviMeXEY98FnzAefMS+OFMc9y7LYuXNnTJo0KUaM2PuTP8N+JmjEiBFRW1tb7DKKoqKiIpkP9FBhzAefMR98xrw4jPvgM+aDz5gXR2rjvq8ZoD0sjAAAACRFCAIAAJIiBA1D5eXlce2110Z5eXmxS0mGMR98xnzwGfPiMO6Dz5gPPmNeHMZ974b9wggAAAD9YSYIAABIihAEAAAkRQgCAACSIgQBAABJEYIAAICkCEFD3LPPPhsXX3xxTJ48OQ455JA47rjj4tprr41du3bt83WvvvpqXHbZZXH44YfHoYceGu9+97tjx44dg1R1abjhhhti1qxZMXbs2Bg/fnyfXnPhhRdGLpfrdsybN6+whZaQAxnzLMvimmuuiZqamjjkkENizpw5sXHjxsIWWkJefPHFOP/886OioiLGjx8fF198cbz88sv7fE1DQ0OPz/mHPvShQap4eLrjjjvi2GOPjTFjxsSMGTPipz/96T7733///XHiiSfGmDFj4uSTT47//d//HaRKS0d/xnzFihU9PtNjxowZxGqHvx/+8Ifxjne8IyZNmhS5XC4efPDB/b7m0UcfjdNPPz3Ky8tj6tSpsWLFioLXWUr6O+aPPvpoj895LpeL1tbWwSl4iBGChrgNGzbE7t274+67747169fHF77whbjrrrvik5/85D5fd/nll8c3v/nNuP/+++MHP/hBbN++PRobGwep6tKwa9euOPfcc+PDH/5wv143b968aGlp6Tq+9rWvFajC0nMgY37TTTfFF7/4xbjrrrti7dq18YY3vCHmzp0br776agErLR3nn39+rF+/PlauXBnf+ta34oc//GFccskl+33dwoULu33Ob7rppkGodni677774oorrohrr702fvGLX8Qpp5wSc+fOjd/97ne99n/sscfivPPOi4svvjieeOKJOOecc+Kcc86JX/3qV4Nc+fDV3zGPiKioqOj2mX7uuecGseLh75VXXolTTjkl7rjjjj7137x5c5x99tkxe/bsWLduXSxevDg++MEPxne/+90CV1o6+jvmezz99NPdPutHHnlkgSoc4jKGnZtuuimbPHnyXq+/9NJL2ahRo7L777+/q+2pp57KIiJbs2bNYJRYUpYvX55VVlb2qe8FF1yQzZ8/v6D1pKCvY7579+6suro6u/nmm7vaXnrppay8vDz72te+VsAKS8Ovf/3rLCKyn/3sZ11t3/nOd7JcLpc1Nzfv9XVnnXVWtmjRokGosDScccYZ2WWXXdZ13tnZmU2aNClbunRpr/3/+Z//OTv77LO7tc2YMSO79NJLC1pnKenvmPfn5zz7FxHZ17/+9X32+fjHP56ddNJJ3dre8573ZHPnzi1gZaWrL2P+yCOPZBGR/eEPfxiUmoY6M0HDUFtbW0yYMGGv1x9//PH485//HHPmzOlqO/HEE+Poo4+ONWvWDEaJSXv00UfjyCOPjBNOOCE+/OEPxwsvvFDskkrW5s2bo7W1tdtnvbKyMmbMmOGz3gdr1qyJ8ePHx5vf/Oautjlz5sSIESNi7dq1+3ztf/3Xf8URRxwRb3rTm+Lqq6+OP/7xj4Uud1jatWtXPP74490+oyNGjIg5c+bs9TO6Zs2abv0jIubOnesz3UcHMuYRES+//HIcc8wxUVdXF/Pnz4/169cPRrnJ8jkvnlNPPTVqamriH/7hH+LHP/5xscspmpHFLoD+2bRpU9x+++1xyy237LVPa2trjB49usczFRMnTkz2vs/BMm/evGhsbIzJkyfHM888E5/85CfjH//xH2PNmjVRVlZW7PJKzp7P88SJE7u1+6z3TWtra4/bIEaOHBkTJkzY5/i9733vi2OOOSYmTZoUv/zlL+MTn/hEPP3009HU1FTokoed3//+99HZ2dnrZ3TDhg29vqa1tdVn+iAcyJifcMIJcc8998Tf/M3fRFtbW9xyyy0xa9asWL9+fdTW1g5G2cnZ2+e8vb09/vSnP8UhhxxSpMpKV01NTdx1113x5je/OTo6OuLLX/5yNDQ0xNq1a+P0008vdnmDzkxQkSxZsqTXh9Nee7z+h3Vzc3PMmzcvzj333Fi4cGGRKh/eDmTc++O9731vvPOd74yTTz45zjnnnPjWt74VP/vZz+LRRx8duDcxzBR6zOmp0GN+ySWXxNy5c+Pkk0+O888/P/7zP/8zvv71r8czzzwzgO8CBs/MmTPjAx/4QJx66qlx1llnRVNTU1RVVcXdd99d7NJgwJxwwglx6aWXxvTp02PWrFlxzz33xKxZs+ILX/hCsUsrCjNBRXLllVfGhRdeuM8+U6ZM6frz9u3bY/bs2TFr1qz40pe+tM/XVVdXx65du+Kll17qNhu0Y8eOqK6uPpiyh73+jvvBmjJlShxxxBGxadOmeOtb3zpgX3c4KeSY7/k879ixI2pqarrad+zYEaeeeuoBfc1S0Ncxr66u7vGg+F/+8pd48cUX+/WzYsaMGRGRn6k+7rjj+l1vKTviiCOirKysx+qc+/p5XF1d3a/+dHcgY/56o0aNitNOOy02bdpUiBKJvX/OKyoqzAINojPOOCN+9KMfFbuMohCCiqSqqiqqqqr61Le5uTlmz54d06dPj+XLl8eIEfuewJs+fXqMGjUqVq1aFe9+97sjIr8SyJYtW2LmzJkHXftw1p9xHwjbtm2LF154odsv6Kkp5JhPnjw5qqurY9WqVV2hp729PdauXdvvVf1KSV/HfObMmfHSSy/F448/HtOnT4+IiO9///uxe/furmDTF+vWrYuISPpzvjejR4+O6dOnx6pVq+Kcc86JiIjdu3fHqlWr4t/+7d96fc3MmTNj1apVsXjx4q62lStXJv/zu68OZMxfr7OzM5588sl4+9vfXsBK0zZz5sweS7/7nA++devWpfuzu9grM7Bv27Zty6ZOnZq99a1vzbZt25a1tLR0Ha/tc8IJJ2Rr167tavvQhz6UHX300dn3v//97Oc//3k2c+bMbObMmcV4C8PWc889lz3xxBPZddddlx166KHZE088kT3xxBPZzp07u/qccMIJWVNTU5ZlWbZz587sqquuytasWZNt3rw5e/jhh7PTTz89mzZtWvbqq68W620MK/0d8yzLshtvvDEbP3589o1vfCP75S9/mc2fPz+bPHly9qc//akYb2HYmTdvXnbaaadla9euzX70ox9l06ZNy84777yu66//+bJp06bs+uuvz37+859nmzdvzr7xjW9kU6ZMyd7ylrcU6y0Meffee29WXl6erVixIvv1r3+dXXLJJdn48eOz1tbWLMuy7P3vf3+2ZMmSrv4//vGPs5EjR2a33HJL9tRTT2XXXnttNmrUqOzJJ58s1lsYdvo75tddd1323e9+N3vmmWeyxx9/PHvve9+bjRkzJlu/fn2x3sKws3Pnzq6f2RGR3XrrrdkTTzyRPffcc1mWZdmSJUuy97///V39f/vb32Zjx47NPvaxj2VPPfVUdscdd2RlZWXZQw89VKy3MOz0d8y/8IUvZA8++GC2cePG7Mknn8wWLVqUjRgxInv44YeL9RaKSgga4pYvX55FRK/HHps3b84iInvkkUe62v70pz9lH/nIR7LDDjssGzt2bPaud72rW3Bi/y644IJex/214xwR2fLly7Msy7I//vGP2dve9rasqqoqGzVqVHbMMcdkCxcu7PofXfavv2OeZfllsj/96U9nEydOzMrLy7O3vvWt2dNPPz34xQ9TL7zwQnbeeedlhx56aFZRUZFddNFF3ULn63++bNmyJXvLW96STZgwISsvL8+mTp2afexjH8va2tqK9A6Gh9tvvz07+uijs9GjR2dnnHFG9pOf/KTr2llnnZVdcMEF3fr/93//d3b88cdno0ePzk466aTs29/+9iBXPPz1Z8wXL17c1XfixInZ29/+9uwXv/hFEaoevvYsv/z6Y884X3DBBdlZZ53V4zWnnnpqNnr06GzKlCndfrazf/0d889//vPZcccdl40ZMyabMGFC1tDQkH3/+98vTvFDQC7Lsqzw800AAABDg9XhAACApAhBAABAUoQgAAAgKUIQAACQFCEIAABIihAEAAAkRQgCAACSIgQBAABJEYIAAICkCEEAAEBShCAAACAp/w/QAOG1vqJqIAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_hat = output[:, 1].detach().numpy()\n",
    "plt.figure(figsize=(10, 6))  # Optional: specifies the figure size\n",
    "plt.scatter(x, y, label='y1', color='blue', marker='o')  # Plot x vs. y1\n",
    "plt.scatter(x, y_hat, label='y2', color='red', marker='s') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.32921217e+00, -7.70033452e-01, -3.16280360e-01, -9.90810387e-01,\n",
       "       -1.07081626e+00, -1.43871328e+00,  5.64416852e-01,  2.95721888e-01,\n",
       "       -1.62640423e+00,  2.19565199e-01,  6.78804799e-01,  1.88927273e+00,\n",
       "        9.61538399e-01,  1.04011196e-01, -4.81165317e-01,  8.50228531e-01,\n",
       "        1.45342467e+00,  1.05773744e+00,  1.65561607e-01,  5.15018378e-01,\n",
       "       -1.33693569e+00,  5.62861137e-01,  1.39285483e+00, -6.33279835e-02,\n",
       "        1.21668362e-01,  1.20760254e+00, -2.04021491e-03,  1.62779574e+00,\n",
       "        3.54492786e-01,  1.03752763e+00, -3.85683513e-01,  5.19818001e-01,\n",
       "        1.68658289e+00, -1.32596315e+00,  1.42898370e+00, -2.08935428e+00,\n",
       "       -1.29819937e-01,  6.31522949e-01, -5.86538064e-01,  2.90720081e-01,\n",
       "        1.26410337e+00,  2.90034782e-01, -1.97028850e+00,  8.03905889e-01,\n",
       "        1.03055033e+00,  1.18097936e-01, -2.18533273e-02,  4.68407139e-02,\n",
       "       -1.62875286e+00, -3.92360590e-01,  1.70097271e+00,  1.06132976e+00,\n",
       "        6.95803574e-01, -4.35988570e-01, -3.32941620e-01,  6.02134563e-01,\n",
       "        1.08788959e-01,  3.67669286e-02, -5.38963407e-01,  4.99177887e-01,\n",
       "       -7.11951757e-01, -2.37000969e-01,  8.57119235e-01, -1.88235190e+00,\n",
       "        4.20705086e-01, -1.06917534e+00, -2.57685880e+00, -1.22492344e+00,\n",
       "       -1.15675186e+00,  9.31304271e-01,  9.84257148e-01,  2.22615937e+00,\n",
       "       -4.18503915e-01, -3.33777673e-01, -1.47426869e-01,  1.56130767e+00,\n",
       "        6.84709996e-01,  2.94623651e-02, -8.52474920e-01,  1.98141717e+00,\n",
       "       -1.63019863e+00, -2.35034324e-01, -1.64071261e-01,  6.58479613e-01,\n",
       "       -1.30789212e+00,  1.35378504e+00, -1.28522651e-01, -9.73279189e-01,\n",
       "       -6.94448784e-01, -4.88414674e-02, -5.79608261e-01,  8.71106114e-01,\n",
       "       -9.70320260e-01,  4.23359442e-01,  1.97424450e+00,  3.06420378e-01,\n",
       "        9.99261932e-01,  8.72336098e-01, -1.86798030e+00,  1.23074409e+00])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NOTEARS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
