{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lbfgsb_scipy import LBFGSBScipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define kernel function\n",
    "def gaussian_kernel(x, y, gamma= 1): # [d, 1] * [d, 1] -> [1, 1]\n",
    "      distance_squared = torch.norm(x-y, dim=-1)**2\n",
    "      return torch.exp(-distance_squared / (gamma**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotearsRKHS(nn.Module):\n",
    "    \"\"\"n: number of samples, d: num variables\"\"\"\n",
    "    def __init__(self, n, d):\n",
    "        super(NotearsRKHS, self).__init__() # inherit  nn.Module\n",
    "        self.d = d\n",
    "        self.n = n\n",
    "\n",
    "        # initialize coefficients alpha\n",
    "        self.fc1_pos = nn.Linear(n, d, bias=False)  # fc1_pos.weight = [d ,n], fc1_pos(x) = x @ fc1_pos.weight^T\n",
    "        self.fc1_neg = nn.Linear(n, d, bias=False)\n",
    "\n",
    "        # Set positiv boundary for coefficients alpha to apply L-BFGS-B  \n",
    "        self.fc1_pos.weight.bounds = self._bounds() \n",
    "        self.fc1_neg.weight.bounds = self._bounds()\n",
    "        nn.init.zeros_(self.fc1_pos.weight)\n",
    "        nn.init.zeros_(self.fc1_neg.weight)\n",
    "        self.I = torch.eye(self.d)\n",
    "\n",
    "    def _bounds(self):\n",
    "        \"\"\"Set boundary for coefficients alpha\"\"\"\n",
    "        bounds = []\n",
    "        for _ in range(self.n):\n",
    "            for _ in range(self.d):\n",
    "                bound = (0, None)\n",
    "                bounds.append(bound)\n",
    "        return bounds\n",
    "    '''\n",
    "    def kernel_matrix(self, x: np.array): # [n, d] -> [n, n]\n",
    "       #get the kernel matrix s.t. K_{il} = k(x^i, x^l)\n",
    "       x = torch.tensor(x, dtype=torch.float32, requires_grad=True)\n",
    "       x1 = x.unsqueeze(-1)\n",
    "       x1 = x1.repeat(1, 1, 2).transpose(1, 2)\n",
    "       x2 = x.unsqueeze(0)\n",
    "       x2 = x.repeat(2, 1, 1)\n",
    "       K = gaussian_kernel(x1, x2)\n",
    "       return K\n",
    "    '''\n",
    "    def gaussian_kernel_matrix_and_grad(self, x, gamma=1):\n",
    "      x = torch.tensor(x, dtype=torch.float32, requires_grad=True)\n",
    "    # Compute pairwise squared Euclidean distances using broadcasting\n",
    "      diff = x.unsqueeze(1) - x.unsqueeze(0)\n",
    "      sq_dist = torch.sum(diff ** 2, dim=2)\n",
    "\n",
    "      # Compute the Gaussian kernel matrix\n",
    "      K = torch.exp(-sq_dist / (gamma ** 2))\n",
    "\n",
    "      # Compute the gradient of K with respect to X\n",
    "      grad_K = -2 * diff / (gamma ** 2) * K.unsqueeze(2)\n",
    "\n",
    "      return K, grad_K\n",
    "\n",
    "    \n",
    "    def forward(self, x: np.array): #[n,d] -> [n, d], forward(x)_{l,j} = estimation of x_j at lth observation \n",
    "      \"\"\"\n",
    "      x: data matrix of shape [n, d] (np.array)\n",
    "      forward(x)_{l,j} = estimation of x_j at lth observation\n",
    "      \"\"\"\n",
    "      K = self.gaussian_kernel_matrix_and_grad(x)[0]\n",
    "      output = self.fc1_pos(K) - self.fc1_neg(K)\n",
    "      return output\n",
    "    \n",
    "    def L_risk(self, x: np.array, penalty): # [1, 1]\n",
    "      \"\"\"compute the regularized iempirical L_risk of squared loss function, penalty: penalty for H_norm\"\"\"\n",
    "      x_est = self.forward(x)\n",
    "      K = self.gaussian_kernel_matrix_and_grad(x)[0] # [n, n]\n",
    "      x = torch.tensor(x, dtype=torch.float32, requires_grad=True)\n",
    "      squared_loss = 0.5 / self.n * torch.sum((x_est - x) ** 2)\n",
    "      fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight\n",
    "      temp = torch.matmul(torch.matmul(fc1_weight, K), fc1_weight.t())\n",
    "      diagonal = torch.sum(torch.diag(temp))\n",
    "      regularized = penalty*diagonal \n",
    "      loss = squared_loss + regularized\n",
    "      return loss\n",
    "    \"\"\"\n",
    "    def grad_kernel(self, x) -> torch.Tensor: # [n, n, d] grad_storage[i, l, k]: gradient of k(x^i, x^l) wrt x^l_{k}\n",
    "      #Get W from fc1 weights, take L2 norm of the gradient\n",
    "      # get [n, n, d] gradient matrix of the kernel\n",
    "      # Initialize a tensor to store gradients\n",
    "      x = torch.tensor(x, dtype=torch.float32, requires_grad=True)\n",
    "      grad_K = torch.zeros(self.n, self.n, self.d)\n",
    "      # Compute gradients\n",
    "      for i in range(self.n):\n",
    "         for l in range(self.n):\n",
    "            x_i = x[i,:]\n",
    "            x_l = x[l,:]\n",
    "            x_l.retain_grad()\n",
    "            K_il = gaussian_kernel(x_i, x_l, gamma=1)\n",
    "            # Zero out gradients in x, otherwise it accumulate the gradients\n",
    "            if x_l.grad is not None:\n",
    "                  x_l.grad.zero_()\n",
    "            # Select individual scalar element from y\n",
    "            # Perform backward pass on the scalar element\n",
    "            K_il.backward()\n",
    "            # Store the computed gradients\n",
    "            grad_K[i, l] = x_l.grad.clone() \n",
    "      return grad_K\n",
    "      \"\"\"\n",
    "\n",
    "    def fc1_to_adj(self, x: np.array) -> torch.Tensor: # [d, d]\n",
    "       grad_K = self.gaussian_kernel_matrix_and_grad(x)[1]\n",
    "       grad_K = grad_K.transpose(0, 2).unsqueeze(-1) # [d ,n, 1]\n",
    "       fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight # [d, n]\n",
    "       weight = torch.matmul(fc1_weight, grad_K).squeeze(-1) # [d, n, d]\n",
    "       weight = torch.sum(weight ** 2, dim = 1)/self.n # [d, d]\n",
    "       return weight\n",
    "    \n",
    "    def h_func(self, x: np.array, t: float = 1.0) -> torch.Tensor: #[1, 1]\n",
    "      \"\"\"\n",
    "      Parameters\n",
    "      ----------\n",
    "      t : float, optional\n",
    "          Controls the domain of M-matrices, by default 1.0\n",
    "\n",
    "      Returns\n",
    "      -------\n",
    "      torch.Tensor\n",
    "          A scalar value of the log-det acyclicity function :math:`h(\\Theta)`.\n",
    "      \"\"\"\n",
    "      weight = self.fc1_to_adj(x)\n",
    "      sign, logabsdet = torch.linalg.slogdet(t*self.I - weight)\n",
    "      h = -sign * logabsdet + self.d * np.log(t)\n",
    "      return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dual_ascent_step(model, x, lambda1, mu, rho, h, rho_max):\n",
    "    \"\"\"Perform one step of dual ascent in augmented Lagrangian.\"\"\"\n",
    "    h_new = None\n",
    "    optimizer = LBFGSBScipy(model.parameters()) # since model inherit from nn.Module, model.parameters returns an iterator over all the parameters (weights, biases) of the model\n",
    "    #X_torch = torch.from_numpy(X)\n",
    "    print(rho)\n",
    "    print(rho_max)\n",
    "    while rho < rho_max:\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            #x_hat = model(x)\n",
    "            loss = model.L_risk(x, penalty = lambda1)\n",
    "            h_val = model.h_func(x)\n",
    "            penalty = 0.5 * rho * h_val * h_val + mu * h_val\n",
    "            #l2_reg = 0.5 * lambda2 * model.l2_reg()\n",
    "            #l1_reg = lambda1 * model.fc1_l1_reg()\n",
    "            primal_obj = loss + penalty \n",
    "            primal_obj.backward()\n",
    "            return primal_obj\n",
    "        optimizer.step(closure)  # NOTE: one step update model in-place\n",
    "        #with torch.no_grad():\n",
    "        h_new = model.h_func(x).item()\n",
    "        if h_new > 0.25 * h: # if h_new shrink enough, then stop updating\n",
    "            rho *= 10\n",
    "        else:\n",
    "            break\n",
    "    mu += rho * h_new\n",
    "    return rho, mu, h_new\n",
    "\n",
    "\n",
    "def RKHS_nonlinear(model: nn.Module,\n",
    "                    x: torch.Tensor,\n",
    "                    lambda1: float = 0.,\n",
    "                    mu: float = 0.,\n",
    "                    max_iter: int = 100,\n",
    "                    h_tol: float = 1e-8,\n",
    "                    rho_max: float = 1e+16,\n",
    "                    w_threshold: float = 0.1):\n",
    "    rho, mu, h = 1.0, 0.0, np.inf\n",
    "    for _ in range(max_iter):\n",
    "        rho, mu, h = dual_ascent_step(model, x, lambda1, mu,\n",
    "                                        rho, h, rho_max)\n",
    "        if h <= h_tol or rho >= rho_max:\n",
    "            break\n",
    "    W_est = model.fc1_to_adj(x)\n",
    "    W_est = W_est.detach().numpy()\n",
    "    W_est[np.abs(W_est) < w_threshold] = 0\n",
    "    W_est[np.abs(W_est) >= w_threshold] = 1\n",
    "    return W_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], requires_grad=True)\n",
      "[(0, None), (0, None), (0, None), (0, None), (0, None), (0, None)]\n"
     ]
    }
   ],
   "source": [
    "instance = NotearsRKHS(2, 3)\n",
    "# Intialization\n",
    "print(instance.fc1_pos.weight)\n",
    "print(instance.fc1_pos.weight.bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K1: tensor([[1.0000e+00, 1.7139e-15],\n",
      "        [1.7139e-15, 1.0000e+00]], grad_fn=<ExpBackward0>)\n",
      "K2: tensor([[1.0000e+00, 1.7139e-15],\n",
      "        [1.7139e-15, 1.0000e+00]], grad_fn=<CopySlices>)\n",
      "False\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# forward\n",
    "x = np.array([[1, 2, 3], [4, 5, 7]])\n",
    "K1 = instance.gaussian_kernel_matrix_and_grad(x)[0]\n",
    "print(f\"K1: {K1}\")\n",
    "\n",
    "x = torch.tensor(x, dtype=torch.float32, requires_grad=True)\n",
    "K2 = torch.zeros(2, 2)\n",
    "for i in range(2):\n",
    "      for l in range(2):\n",
    "            K2[i,l] = gaussian_kernel(x[i,:], x[l,:])\n",
    "print(f\"K2: {K2}\")\n",
    "print(torch.equal(K1, K2))\n",
    "\n",
    "x = np.array([[1, 2, 3], [4, 5, 7]])\n",
    "print(instance.forward(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance.fc1_to_adj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
       "         [ 1.0283e-14,  1.0283e-14,  1.3711e-14]],\n",
       "\n",
       "        [[-1.0283e-14, -1.0283e-14, -1.3711e-14],\n",
       "         [-0.0000e+00, -0.0000e+00, -0.0000e+00]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3], [4, 5, 7]])\n",
    "instance.gaussian_kernel_matrix_and_grad(x)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0000e+00, -0.0000e+00, -0.0000e+00],\n",
       "         [ 1.0283e-14,  1.0283e-14,  1.3711e-14]],\n",
       "\n",
       "        [[-1.0283e-14, -1.0283e-14, -1.3711e-14],\n",
       "         [-0.0000e+00, -0.0000e+00, -0.0000e+00]]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 2\n",
    "d = 3\n",
    "def grad_kernel(x) -> torch.Tensor: # [n, n, d] grad_storage[i, l, k]: gradient of k(x^i, x^l) wrt x^l_{k}\n",
    "      #Get W from fc1 weights, take L2 norm of the gradient\n",
    "      # get [n, n, d] gradient matrix of the kernel\n",
    "      # Initialize a tensor to store gradients\n",
    "      x = torch.tensor(x, dtype=torch.float32, requires_grad=True)\n",
    "      grad_K = torch.zeros(n, n, d)\n",
    "      # Compute gradients\n",
    "      for i in range(n):\n",
    "         for l in range(n):\n",
    "            x_i = x[i,:]\n",
    "            x_l = x[l,:]\n",
    "            x_i.retain_grad()\n",
    "            K_il = gaussian_kernel(x_i, x_l, gamma=1)\n",
    "            # Zero out gradients in x, otherwise it accumulate the gradients\n",
    "            if x_i.grad is not None:\n",
    "                  x_i.grad.zero_()\n",
    "            # Select individual scalar element from y\n",
    "            # Perform backward pass on the scalar element\n",
    "            K_il.backward()\n",
    "            # Store the computed gradients\n",
    "            grad_K[i, l] = x_i.grad.clone() \n",
    "      return grad_K\n",
    "\n",
    "grad_kernel(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13.6000, grad_fn=<MulBackward0>)\n",
      "tensor(13.6000, grad_fn=<MulBackward0>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# L_risk \n",
    "x = np.array([[1, 2, 3], [4, 5, 7]])\n",
    "fc1_weight = torch.tensor([[3,7,4], [6, 1, 5]], dtype=torch.float32, requires_grad=True).t()\n",
    "penalty = 0.1\n",
    "K = instance.gaussian_kernel_matrix_and_grad(x)[0] # [n, n]\n",
    "x = torch.tensor(x, dtype=torch.float32, requires_grad=True)\n",
    "temp = torch.matmul(torch.matmul(fc1_weight, K), fc1_weight.t())\n",
    "diagonal = torch.sum(torch.diag(temp))\n",
    "regularized1 = penalty*diagonal \n",
    "print(regularized1)\n",
    "\n",
    "regularized2 = 0\n",
    "for j in range(fc1_weight.shape[0]):\n",
    "    for m in range(fc1_weight.shape[1]):\n",
    "        for l in range(fc1_weight.shape[1]):\n",
    "            regularized2 += fc1_weight[j, m]*fc1_weight[j, l]*K[m,l]\n",
    "regularized2 = penalty*regularized2\n",
    "print(regularized2)\n",
    "print(torch.equal(regularized1, regularized2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[15., 22., 17.],\n",
       "         [42., 46., 44.]],\n",
       "\n",
       "        [[33., 51., 38.],\n",
       "         [33., 12., 29.]],\n",
       "\n",
       "        [[30., 31., 31.],\n",
       "         [57., 42., 55.]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight\n",
    "x = np.array([[1, 2, 3], [4, 5, 7]])\n",
    "x = torch.tensor(x, dtype=torch.float32, requires_grad=True)\n",
    "grad_K = torch.tensor([[[3,7,4], [6, 1, 5]], \n",
    "                       [[1, 2, 3], [4, 5, 7]]], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "def fc1_to_adj(x, grad_K) -> torch.Tensor: # [d, d]\n",
    "       grad_K = grad_K.transpose(0, 2).unsqueeze(-1) # [d ,n, 1]\n",
    "       weight = torch.matmul(fc1_weight, grad_K).squeeze(-1) # [d, n, d]\n",
    "       #weight = torch.sum(weight ** 2, dim = 1)/n # [d, d]\n",
    "       return weight\n",
    "\n",
    "fc1_to_adj(x, grad_K)\n",
    "#print(fc1_to_adj(x, grad_K).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[15., 22., 17.],\n",
      "         [42., 46., 44.]],\n",
      "\n",
      "        [[33., 51., 38.],\n",
      "         [33., 12., 29.]],\n",
      "\n",
      "        [[30., 31., 31.],\n",
      "         [57., 42., 55.]]], grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "d = 3\n",
    "n = 2\n",
    "storage = torch.zeros(d, n, d)\n",
    "for k in range(0, d):\n",
    "    for l in range(0,n):\n",
    "        for j in range(0, d):\n",
    "            for i in range(0, n):\n",
    "                storage[k,l,j] += grad_K[i,l,k]*fc1_weight[j, i]\n",
    "print(storage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1e+16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RKHS_nonlinear(instance, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning causality by self generated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### linear case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "x = np.random.normal(0,1, 400)\n",
    "\n",
    "np.random.seed(24)\n",
    "l = np.random.normal(0,1, 400) \n",
    "y = [0.5*a + b for a, b in zip(x, l)]\n",
    "\n",
    "matrix = np.column_stack((x, y))\n",
    "n = matrix.shape[0]\n",
    "d = matrix.shape[1]\n",
    "\n",
    "print(n)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = NotearsRKHS(n, d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1e+16\n",
      "1.0\n",
      "1e+16\n",
      "100.0\n",
      "1e+16\n",
      "1000.0\n",
      "1e+16\n",
      "10000.0\n",
      "1e+16\n",
      "1000000.0\n",
      "1e+16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RKHS_nonlinear(example, matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### non-linear case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "x = np.random.normal(0,1, 400)\n",
    "\n",
    "np.random.seed(24)\n",
    "l = np.random.normal(0,1, 400) \n",
    " \n",
    "y = [a**2 + b for a, b in zip(x, l)]\n",
    "\n",
    "matrix = np.column_stack((x, y))\n",
    "n = matrix.shape[0]\n",
    "d = matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1e+16\n",
      "1.0\n",
      "1e+16\n",
      "100.0\n",
      "1e+16\n",
      "1000.0\n",
      "1e+16\n",
      "100000.0\n",
      "1e+16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_linear_example = NotearsRKHS(n, d)\n",
    "RKHS_nonlinear(non_linear_example, matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "x = np.random.normal(0,1, 400)\n",
    "\n",
    "np.random.seed(24)\n",
    "l = np.random.normal(0,1, 400) \n",
    " \n",
    "y = [np.sin(a) + b for a, b in zip(x, l)]\n",
    "\n",
    "matrix = np.column_stack((x, y))\n",
    "n = matrix.shape[0]\n",
    "d = matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1e+16\n",
      "1.0\n",
      "1e+16\n",
      "100.0\n",
      "1e+16\n",
      "1000.0\n",
      "1e+16\n",
      "100000.0\n",
      "1e+16\n",
      "10000000.0\n",
      "1e+16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_linear_example = NotearsRKHS(n, d)\n",
    "RKHS_nonlinear(non_linear_example, matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NOTEARS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
