{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lbfgsb_scipy import LBFGSBScipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from trace_expm import trace_expm\n",
    "import numpy.linalg as la\n",
    "import time\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotearsRKHS(nn.Module):\n",
    "    \"\"\"n: number of samples, d: num variables\"\"\"\n",
    "    def __init__(self, n, d, kernel):\n",
    "        super(NotearsRKHS, self).__init__() # inherit  nn.Module\n",
    "        self.d = d\n",
    "        self.n = n\n",
    "        self.kernel = kernel\n",
    "\n",
    "        #torch.manual_seed(42)\n",
    "\n",
    "        # initialize coefficients alpha\n",
    "        self.fc1_pos = nn.Linear(n, d, bias=False)  # fc1_pos.weight = [d ,n], fc1_pos(x) = x @ fc1_pos.weight^T\n",
    "        #self.fc1_neg = nn.Linear(n, d, bias=False)\n",
    "\n",
    "        # Set positiv boundary for coefficients alpha to apply L-BFGS-B  \n",
    "        # self.fc1_pos.weight.bounds = self._bounds() \n",
    "        # self.fc1_neg.weight.bounds = self._bounds()\n",
    "        nn.init.zeros_(self.fc1_pos.weight)\n",
    "        #nn.init.zeros_(self.fc1_neg.weight)\n",
    "        self.I = torch.eye(self.d)\n",
    "\n",
    "    # def _bounds(self):\n",
    "    #     \"\"\"Set boundary for coefficients alpha\"\"\"\n",
    "    #     bounds = []\n",
    "    #     for _ in range(self.n):\n",
    "    #         for _ in range(self.d):\n",
    "    #             bound = (0, None)\n",
    "    #             bounds.append(bound)\n",
    "    #     return bounds\n",
    "    \n",
    "    \n",
    "    def get_fc1_weight(self): # [d, n]\n",
    "       fc1_weight = self.fc1_pos.weight\n",
    "       #print(\"fc1_weight shape: \", fc1_weight.shape)\n",
    "       return fc1_weight\n",
    "    \n",
    "    #@timing_decorator\n",
    "    @torch.no_grad()\n",
    "    def gaussian_kernel_matrix_and_grad(self, x, gamma=1): # x: [n, d]; K: [n, n]; grad_K: [n, n, d]: gradient of k(x^i, x^l) wrt x^i_{k}\n",
    "      #x = torch.tensor(x, requires_grad=True) \n",
    "      # Compute pairwise squared Euclidean distances using broadcasting\n",
    "      diff = x.unsqueeze(1) - x.unsqueeze(0) # [n, n, d]\n",
    "      sq_dist = torch.sum(diff ** 2, dim=2)\n",
    "\n",
    "      # Compute the Gaussian kernel matrix\n",
    "      K = torch.exp(-sq_dist / (gamma ** 2)) # [n, n]\n",
    "      # Compute the gradient of K with respect to X\n",
    "      grad_K = -2 * diff / (gamma ** 2) * K.unsqueeze(2) # K.unsqueeze(2): [n, n, 1]\n",
    "\n",
    "      return K, grad_K\n",
    "    \n",
    "    #@timing_decorator\n",
    "    @torch.no_grad()\n",
    "    def matern3_2_kernel_matrix_and_grad(self, x, gamma=1): # K: [n, n]; grad_K: [n, n, d]: gradient of k(x^i, x^l) wrt x^i_{k}\n",
    "       #x = torch.tensor(x, requires_grad=True)\n",
    "       diff = x.unsqueeze(1) - x.unsqueeze(0) #[n, n, d]\n",
    "       dist = torch.sqrt(torch.sum(diff ** 2, dim=2)) #[n, n]\n",
    "       K = (1 + np.sqrt(3)*dist / gamma)*torch.exp(-np.sqrt(3)*dist / gamma) #[n, n]\n",
    "       temp = -3/(gamma**2)*torch.exp(-np.sqrt(3)*dist/gamma) #[n, n]\n",
    "       grad_K = diff*temp.unsqueeze(2)\n",
    "\n",
    "       return K, grad_K\n",
    "    \n",
    "    #@timing_decorator\n",
    "    @torch.no_grad()\n",
    "    def matern5_2_kernel_matrix_and_grad(self, x, gamma=1): # [n, n, d]: gradient of k(x^i, x^l) wrt x^i_{k}\n",
    "       #x = torch.tensor(x, requires_grad=True)\n",
    "       diff = x.unsqueeze(1) - x.unsqueeze(0) #[n, n, d]\n",
    "       dist = torch.sqrt(torch.sum(diff ** 2, dim=2)) #[n, n]\n",
    "       K = (1 + np.sqrt(5)*dist / gamma + 5*(dist**2)/(3*(gamma**2)))*torch.exp(-np.sqrt(5)*dist / gamma)\n",
    "       temp = (-5/(3*gamma**2) - 5*np.sqrt(5)*dist/(3*(gamma**3)))*torch.exp(-np.sqrt(5)*dist/gamma) #[n, n]\n",
    "       grad_K = diff*temp.unsqueeze(2)\n",
    "\n",
    "       return K, grad_K\n",
    "    \n",
    "    # @torch.no_grad()\n",
    "    # def polynomial_kernel_matrix_and_grad(self, x, c=1, degree=3): # x: [n, d]; K: [n, n]; grad_K: [n, n, d]: gradient of k(x^i, x^l) wrt x^i_{k}\n",
    "    #   #x = torch.tensor(x, requires_grad=True) \n",
    "    #   # Compute pairwise squared Euclidean distances using broadcasting\n",
    "    #   mul = x.unsqueeze(1)*x.unsqueeze(0) # [n, n, d]\n",
    "\n",
    "    #   # Compute the Gaussian kernel matrix\n",
    "    #   K = (mul + c)**degree # [n, n]\n",
    "    #   # Compute the gradient of K with respect to X\n",
    "    #   grad_K = -2 * diff / (gamma ** 2) * K.unsqueeze(2) # K.unsqueeze(2): [n, n, 1]\n",
    "\n",
    "    #   return K, grad_K\n",
    "    \n",
    "    #@timing_decorator\n",
    "    def forward(self, x: torch.tensor): #[n,d] -> [n, d], forward(x)_{l,j} = estimation of x_j at lth observation \n",
    "      \"\"\"\n",
    "      x: data matrix of shape [n, d] (np.array)\n",
    "      forward(x)_{l,j} = estimation of x_j at lth observation\n",
    "      \"\"\"\n",
    "      if self.kernel == \"gaussian\":\n",
    "        K = self.gaussian_kernel_matrix_and_grad(x)[0]\n",
    "      elif self.kernel == \"matern3_2\":\n",
    "        K = self.matern3_2_kernel_matrix_and_grad(x)[0]\n",
    "      elif self.kernel == \"matern5_2\":\n",
    "        K = self.matern5_2_kernel_matrix_and_grad(x)[0]\n",
    "      else:\n",
    "        print(\"Given kernel is invalid.\")\n",
    "        \n",
    "      output = self.fc1_pos(K) #- self.fc1_neg(K)\n",
    "      return output\n",
    "    \n",
    "\n",
    "\n",
    "    #@timing_decorator\n",
    "    def fc1_to_adj(self, x: torch.tensor) -> torch.Tensor: # [d, d]\n",
    "      if self.kernel == \"gaussian\":\n",
    "        grad_K = self.gaussian_kernel_matrix_and_grad(x)[1] # [n, n, d]\n",
    "      elif self.kernel == \"matern3_2\":\n",
    "        grad_K = self.matern3_2_kernel_matrix_and_grad(x)[1]\n",
    "      elif self.kernel == \"matern5_2\":\n",
    "        grad_K = self.matern5_2_kernel_matrix_and_grad(x)[1]\n",
    "      else:\n",
    "        print(\"Given kernel is invalid.\")\n",
    "\n",
    "      fc1_weight = self.fc1_pos.weight #- self.fc1_neg.weight # [d, n]\n",
    "      weight = torch.einsum('ji, lik -> jlk', fc1_weight, grad_K) # [d, n, d]\n",
    "      weight = torch.sum(weight ** 2, dim = 1)/self.n # [d, d]\n",
    "      return weight.t()\n",
    "    \n",
    "    #@timing_decorator\n",
    "    # expoential h\n",
    "    def h_func(self, x: torch.tensor):\n",
    "      weight = self.fc1_to_adj(x)\n",
    "      h = trace_expm(weight)-self.d\n",
    "      return h\n",
    "    \n",
    "    # log determinant h\n",
    "    # def h_func(self, x: torch.tensor, t = 200):\n",
    "    #   weight = self.fc1_to_adj(x)\n",
    "    #   #print(\"weight:\", weight)\n",
    "    #   A = t*self.I - weight\n",
    "    #   sign, logabsdet = torch.linalg.slogdet(A)\n",
    "    #   h = -logabsdet + self.d * np.log(t)\n",
    "    #   #print(\"h: \", h)\n",
    "    #   return h\n",
    "\n",
    "    #spetrum h\n",
    "    # def h_func(self, x: torch.tensor):\n",
    "    #   weight = self.fc1_to_adj(x)\n",
    "    #   w = torch.ones(self.d)\n",
    "    #   for _ in range(10):\n",
    "    #       w = weight @ w\n",
    "    #       w = w / (torch.norm(w) + 1e-8)\n",
    "    #   return w @ weight @ w\n",
    "    \n",
    "\n",
    "    #@timing_decorator\n",
    "    def L_risk(self, x: torch.tensor, lambda1=0.0001): # [1, 1]\n",
    "      \"\"\"compute the regularized iempirical L_risk of squared loss function, penalty: penalty for H_norm\"\"\"\n",
    "      if self.kernel == \"gaussian\":\n",
    "          K = self.gaussian_kernel_matrix_and_grad(x)[0] # [n, n]\n",
    "      elif self.kernel == \"matern3_2\":\n",
    "          K = self.matern3_2_kernel_matrix_and_grad(x)[0] # [n, n]\n",
    "      elif self.kernel == \"matern5_2\":\n",
    "          K = self.matern5_2_kernel_matrix_and_grad(x)[0] # [n, n]\n",
    "      else:\n",
    "          print(\"Given kernel is invalid.\") \n",
    "\n",
    "      x_est = self.forward(x) # [n, d]\n",
    "      squared_loss = 0.5 / self.n * torch.sum((x_est - x) ** 2)\n",
    "      fc1_weight = self.fc1_pos.weight #- self.fc1_neg.weight\n",
    "      temp = torch.matmul(torch.matmul(fc1_weight, K), fc1_weight.t())\n",
    "      diagonal = torch.sum(torch.diag(temp))\n",
    "      regularized = lambda1*diagonal \n",
    "      loss = squared_loss + regularized\n",
    "      return loss\n",
    "  \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning(model, X):\n",
    "    rho, mu, h = 1000000000.0, 0.0, np.inf\n",
    "    x_torch = torch.from_numpy(X)\n",
    "    h_new = None\n",
    "    optimizer = LBFGSBScipy(model.parameters())\n",
    "\n",
    "    def closure():\n",
    "            optimizer.zero_grad()\n",
    "            h_val = model.h_func(x_torch)\n",
    "            print(\"h_val: \", h_val)\n",
    "            #penalty = 0.5 * rho * h_val * h_val + mu * h_val\n",
    "            L_risk = model.L_risk(x_torch)\n",
    "            loss = L_risk #+ penalty\n",
    "            print('L_risk:', L_risk.item())\n",
    "            print('loss:', loss.item())\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        \n",
    "        #start_time = time.time()\n",
    "    optimizer.step(closure)\n",
    "    h_new = model.h_func(x_torch).item()\n",
    "    print(\"h_new: \", h_new)\n",
    "\n",
    "    output = model.forward(x_torch)\n",
    "    W_est = model.fc1_to_adj(x_torch)\n",
    "    W_est = torch.sqrt(W_est)\n",
    "    W_est = W_est.detach().numpy()\n",
    "    print(W_est)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dual_ascent_step(model, X, lambda1, rho, mu, h, rho_max):\n",
    "    \"\"\"Perform one step of dual ascent in augmented Lagrangian.\"\"\"\n",
    "    h_new = None\n",
    "    optimizer = LBFGSBScipy(model.parameters())\n",
    "    X_torch = torch.from_numpy(X)\n",
    "    while rho < rho_max:\n",
    "        print(\"rho: \", rho)\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            L_risk = model.L_risk(X_torch, lambda1=lambda1)\n",
    "            h_val = model.h_func(X_torch)\n",
    "            print(\"h_val: \", h_val)\n",
    "            penalty = 0.5 * rho * h_val * h_val + mu * h_val\n",
    "            primal_obj = L_risk + penalty\n",
    "            primal_obj.backward()\n",
    "            print('L_risk:', L_risk.item())\n",
    "            print('loss:', primal_obj.item())\n",
    "            #print('penalty:', penalty.item())\n",
    "            return primal_obj\n",
    "        optimizer.step(closure)  # NOTE: updates model in-place\n",
    "        with torch.no_grad():\n",
    "            h_new = model.h_func(X_torch).item()\n",
    "            print(\"h_new: \", h_new)\n",
    "        if h_new > 0.25 * h:\n",
    "            rho *= 10\n",
    "        else:\n",
    "            break\n",
    "    mu += rho * h_new\n",
    "    return rho, mu, h_new\n",
    "\n",
    "\n",
    "def notears_nonlinear(model: nn.Module,\n",
    "                      X: np.ndarray,\n",
    "                      lambda1: float = 0.0001,\n",
    "                      max_iter: int = 100,\n",
    "                      h_tol: float = 1e-8,\n",
    "                      rho_max: float = 1e+16,\n",
    "                      w_threshold: float = 0.3):\n",
    "    rho, mu, h = 1.0, 1.0, np.inf\n",
    "    for k in range(max_iter):\n",
    "        rho, mu, h = dual_ascent_step(model, X, lambda1,\n",
    "                                         rho, mu, h, rho_max)\n",
    "        print(\"iteration: \", k+1)\n",
    "        if h <= h_tol or rho >= rho_max:\n",
    "            break\n",
    "    X_torch = torch.tensor(X, dtype=torch.float64, requires_grad=True)\n",
    "    W_est = model.fc1_to_adj(X_torch)\n",
    "    W_est = torch.sqrt(W_est)\n",
    "    W_est = W_est.detach().numpy()\n",
    "    print(W_est)\n",
    "    W_est[np.abs(W_est) < w_threshold] = 0\n",
    "    W_est[np.abs(W_est) >= w_threshold] = 1\n",
    "    output = model.forward(X_torch)\n",
    "    return W_est, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "2\n",
      "h_val:  tensor(0., grad_fn=<SubBackward0>)\n",
      "L_risk: 5889.293525997783\n",
      "loss: 5889.293525997783\n",
      "h_val:  tensor(0.0155, grad_fn=<SubBackward0>)\n",
      "L_risk: 5865.199892148727\n",
      "loss: 5865.199892148727\n",
      "h_val:  tensor(0.4675, grad_fn=<SubBackward0>)\n",
      "L_risk: 5769.74267545799\n",
      "loss: 5769.74267545799\n",
      "h_val:  tensor(866.6801, grad_fn=<SubBackward0>)\n",
      "L_risk: 5402.59090798289\n",
      "loss: 5402.59090798289\n",
      "h_val:  tensor(1.3797e+48, grad_fn=<SubBackward0>)\n",
      "L_risk: 4168.817426687971\n",
      "loss: 4168.817426687971\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 1674.263350495443\n",
      "loss: 1674.263350495443\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 965.5109618155076\n",
      "loss: 965.5109618155076\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 280.7121250440017\n",
      "loss: 280.7121250440017\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 404.9409878944685\n",
      "loss: 404.9409878944685\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 173.3946017907823\n",
      "loss: 173.3946017907823\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 105.6602153159917\n",
      "loss: 105.6602153159917\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 88.03101396439874\n",
      "loss: 88.03101396439874\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 80.79135833824438\n",
      "loss: 80.79135833824438\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 78.87701474267124\n",
      "loss: 78.87701474267124\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 77.75277825331212\n",
      "loss: 77.75277825331212\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 77.42447944394189\n",
      "loss: 77.42447944394189\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 77.04282168007872\n",
      "loss: 77.04282168007872\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 76.69901941864343\n",
      "loss: 76.69901941864343\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 76.42683967758725\n",
      "loss: 76.42683967758725\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 76.30255738385276\n",
      "loss: 76.30255738385276\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 76.1371331977241\n",
      "loss: 76.1371331977241\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 75.98453174356673\n",
      "loss: 75.98453174356673\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 75.78420787432282\n",
      "loss: 75.78420787432282\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 75.5267339986338\n",
      "loss: 75.5267339986338\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 75.32159183188621\n",
      "loss: 75.32159183188621\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 74.96361699605163\n",
      "loss: 74.96361699605163\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 74.82249892549162\n",
      "loss: 74.82249892549162\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 74.64852892764907\n",
      "loss: 74.64852892764907\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 74.59654958158589\n",
      "loss: 74.59654958158589\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 74.55358866425512\n",
      "loss: 74.55358866425512\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 74.47198276354484\n",
      "loss: 74.47198276354484\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 74.39383396904033\n",
      "loss: 74.39383396904033\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 74.33296718217032\n",
      "loss: 74.33296718217032\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 74.25755028860547\n",
      "loss: 74.25755028860547\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 74.21674481801342\n",
      "loss: 74.21674481801342\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 74.15966540802273\n",
      "loss: 74.15966540802273\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 74.0442102928363\n",
      "loss: 74.0442102928363\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.99126690190589\n",
      "loss: 73.99126690190589\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.9250445246258\n",
      "loss: 73.9250445246258\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.92666759875569\n",
      "loss: 73.92666759875569\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.89019901842863\n",
      "loss: 73.89019901842863\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.88084263068633\n",
      "loss: 73.88084263068633\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.87534688043027\n",
      "loss: 73.87534688043027\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.86584303398435\n",
      "loss: 73.86584303398435\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.85851986278027\n",
      "loss: 73.85851986278027\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.8512289668579\n",
      "loss: 73.8512289668579\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.84488309435778\n",
      "loss: 73.84488309435778\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.83745303997047\n",
      "loss: 73.83745303997047\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.82973289372227\n",
      "loss: 73.82973289372227\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.83432670056058\n",
      "loss: 73.83432670056058\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.82429788113146\n",
      "loss: 73.82429788113146\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.81558962530589\n",
      "loss: 73.81558962530589\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.81009829886163\n",
      "loss: 73.81009829886163\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.80448366670113\n",
      "loss: 73.80448366670113\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.79832437068463\n",
      "loss: 73.79832437068463\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.79134589692859\n",
      "loss: 73.79134589692859\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.7855547382427\n",
      "loss: 73.7855547382427\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.77857946008842\n",
      "loss: 73.77857946008842\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.7724885583961\n",
      "loss: 73.7724885583961\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.76269703428397\n",
      "loss: 73.76269703428397\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.7534815596432\n",
      "loss: 73.7534815596432\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.74965422924518\n",
      "loss: 73.74965422924518\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.74634178891645\n",
      "loss: 73.74634178891645\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.74260373213609\n",
      "loss: 73.74260373213609\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.73730001491974\n",
      "loss: 73.73730001491974\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.7333482135476\n",
      "loss: 73.7333482135476\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.73000888527226\n",
      "loss: 73.73000888527226\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.72554147192467\n",
      "loss: 73.72554147192467\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.72074150288636\n",
      "loss: 73.72074150288636\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.71531944559644\n",
      "loss: 73.71531944559644\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.708714670113\n",
      "loss: 73.708714670113\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.70485851026099\n",
      "loss: 73.70485851026099\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.70165007471634\n",
      "loss: 73.70165007471634\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.69815735917133\n",
      "loss: 73.69815735917133\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.69576550868895\n",
      "loss: 73.69576550868895\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.69428596934333\n",
      "loss: 73.69428596934333\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.69156440662924\n",
      "loss: 73.69156440662924\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.68934475285252\n",
      "loss: 73.68934475285252\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.6869383690673\n",
      "loss: 73.6869383690673\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.68425898426129\n",
      "loss: 73.68425898426129\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.68232626130123\n",
      "loss: 73.68232626130123\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.67997289676293\n",
      "loss: 73.67997289676293\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.67707556105823\n",
      "loss: 73.67707556105823\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.67294406751641\n",
      "loss: 73.67294406751641\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.67159171590372\n",
      "loss: 73.67159171590372\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.66887543883807\n",
      "loss: 73.66887543883807\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.66796173808635\n",
      "loss: 73.66796173808635\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.66602808957047\n",
      "loss: 73.66602808957047\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.66471183511817\n",
      "loss: 73.66471183511817\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.66282530263078\n",
      "loss: 73.66282530263078\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.65945423699257\n",
      "loss: 73.65945423699257\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.65911223088517\n",
      "loss: 73.65911223088517\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.6569984333776\n",
      "loss: 73.6569984333776\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.65252748959156\n",
      "loss: 73.65252748959156\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.64829163099331\n",
      "loss: 73.64829163099331\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.64421456070713\n",
      "loss: 73.64421456070713\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.64089516557996\n",
      "loss: 73.64089516557996\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.63870120085384\n",
      "loss: 73.63870120085384\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.63707087889165\n",
      "loss: 73.63707087889165\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.63591776878874\n",
      "loss: 73.63591776878874\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.63498763860257\n",
      "loss: 73.63498763860257\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.63440009536964\n",
      "loss: 73.63440009536964\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.63391839711564\n",
      "loss: 73.63391839711564\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.63323910978504\n",
      "loss: 73.63323910978504\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.63258008124562\n",
      "loss: 73.63258008124562\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.63203811970519\n",
      "loss: 73.63203811970519\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.6315127147962\n",
      "loss: 73.6315127147962\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.63084983152483\n",
      "loss: 73.63084983152483\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.62977709354098\n",
      "loss: 73.62977709354098\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.62856664706452\n",
      "loss: 73.62856664706452\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.62726635952464\n",
      "loss: 73.62726635952464\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.62582058603054\n",
      "loss: 73.62582058603054\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.62453981262748\n",
      "loss: 73.62453981262748\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.62305282044416\n",
      "loss: 73.62305282044416\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.62171391595902\n",
      "loss: 73.62171391595902\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.62032624272116\n",
      "loss: 73.62032624272116\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.61946761133927\n",
      "loss: 73.61946761133927\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.61857140738323\n",
      "loss: 73.61857140738323\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.61774358433912\n",
      "loss: 73.61774358433912\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.6172120670215\n",
      "loss: 73.6172120670215\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.61690879472782\n",
      "loss: 73.61690879472782\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.61555649546902\n",
      "loss: 73.61555649546902\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.61519989948745\n",
      "loss: 73.61519989948745\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.61457258281217\n",
      "loss: 73.61457258281217\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.61364108330686\n",
      "loss: 73.61364108330686\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.61272068808537\n",
      "loss: 73.61272068808537\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.61211183477451\n",
      "loss: 73.61211183477451\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.61171666406534\n",
      "loss: 73.61171666406534\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.61145871649856\n",
      "loss: 73.61145871649856\n",
      "h_val:  tensor(inf, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.61108783063084\n",
      "loss: 73.61108783063084\n",
      "h_val:  tensor(1.2127e+308, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.61049897350047\n",
      "loss: 73.61049897350047\n",
      "h_val:  tensor(9.1781e+307, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.61018320527708\n",
      "loss: 73.61018320527708\n",
      "h_val:  tensor(8.8627e+307, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.60979961924436\n",
      "loss: 73.60979961924436\n",
      "h_val:  tensor(8.8181e+307, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.60934899827959\n",
      "loss: 73.60934899827959\n",
      "h_val:  tensor(7.8322e+307, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.6090237210405\n",
      "loss: 73.6090237210405\n",
      "h_val:  tensor(7.0542e+307, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.60900089579981\n",
      "loss: 73.60900089579981\n",
      "h_val:  tensor(7.4172e+307, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.60872844234817\n",
      "loss: 73.60872844234817\n",
      "h_val:  tensor(5.7751e+307, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.60828830630584\n",
      "loss: 73.60828830630584\n",
      "h_val:  tensor(4.6886e+307, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.60792603169665\n",
      "loss: 73.60792603169665\n",
      "h_val:  tensor(3.7595e+307, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.60754844503755\n",
      "loss: 73.60754844503755\n",
      "h_val:  tensor(3.5139e+307, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.60712395288807\n",
      "loss: 73.60712395288807\n",
      "h_val:  tensor(3.6440e+307, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.60658460703263\n",
      "loss: 73.60658460703263\n",
      "h_val:  tensor(3.3677e+307, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.60555616328058\n",
      "loss: 73.60555616328058\n",
      "h_val:  tensor(2.2059e+307, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.60477350097827\n",
      "loss: 73.60477350097827\n",
      "h_val:  tensor(1.7506e+307, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.60368051645652\n",
      "loss: 73.60368051645652\n",
      "h_val:  tensor(8.8535e+306, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.6023183503535\n",
      "loss: 73.6023183503535\n",
      "h_val:  tensor(3.9887e+306, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.60112060922401\n",
      "loss: 73.60112060922401\n",
      "h_val:  tensor(1.1074e+306, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.59910453486269\n",
      "loss: 73.59910453486269\n",
      "h_val:  tensor(1.7779e+305, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5998739225964\n",
      "loss: 73.5998739225964\n",
      "h_val:  tensor(5.1280e+305, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.59831179006858\n",
      "loss: 73.59831179006858\n",
      "h_val:  tensor(4.9533e+305, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.59784973936526\n",
      "loss: 73.59784973936526\n",
      "h_val:  tensor(5.0475e+305, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.59752853926514\n",
      "loss: 73.59752853926514\n",
      "h_val:  tensor(4.7269e+305, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.59719513779075\n",
      "loss: 73.59719513779075\n",
      "h_val:  tensor(3.8042e+305, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5967886806509\n",
      "loss: 73.5967886806509\n",
      "h_val:  tensor(3.2759e+305, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.59636874950935\n",
      "loss: 73.59636874950935\n",
      "h_val:  tensor(2.8931e+305, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.59608456061363\n",
      "loss: 73.59608456061363\n",
      "h_val:  tensor(2.2592e+305, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.59562114742427\n",
      "loss: 73.59562114742427\n",
      "h_val:  tensor(1.5355e+305, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.59496721707289\n",
      "loss: 73.59496721707289\n",
      "h_val:  tensor(7.7396e+304, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.59417683689402\n",
      "loss: 73.59417683689402\n",
      "h_val:  tensor(5.5186e+304, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5934527980114\n",
      "loss: 73.5934527980114\n",
      "h_val:  tensor(3.8218e+304, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.59269856425885\n",
      "loss: 73.59269856425885\n",
      "h_val:  tensor(2.1877e+304, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.59199123079252\n",
      "loss: 73.59199123079252\n",
      "h_val:  tensor(1.0583e+304, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.59105969746643\n",
      "loss: 73.59105969746643\n",
      "h_val:  tensor(4.5987e+303, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5903074470026\n",
      "loss: 73.5903074470026\n",
      "h_val:  tensor(2.7424e+303, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.58989374127056\n",
      "loss: 73.58989374127056\n",
      "h_val:  tensor(2.7611e+303, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.58956924449208\n",
      "loss: 73.58956924449208\n",
      "h_val:  tensor(2.4496e+303, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.58912811228046\n",
      "loss: 73.58912811228046\n",
      "h_val:  tensor(1.7368e+303, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.58855401797477\n",
      "loss: 73.58855401797477\n",
      "h_val:  tensor(8.4308e+302, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.58768764918203\n",
      "loss: 73.58768764918203\n",
      "h_val:  tensor(3.2963e+302, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.58666416841616\n",
      "loss: 73.58666416841616\n",
      "h_val:  tensor(2.0350e+302, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5854230255884\n",
      "loss: 73.5854230255884\n",
      "h_val:  tensor(6.9664e+301, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.58270059495892\n",
      "loss: 73.58270059495892\n",
      "h_val:  tensor(2.8584e+301, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.58039084463077\n",
      "loss: 73.58039084463077\n",
      "h_val:  tensor(2.0499e+300, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.57798791267417\n",
      "loss: 73.57798791267417\n",
      "h_val:  tensor(8.0912e+299, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.57251801806892\n",
      "loss: 73.57251801806892\n",
      "h_val:  tensor(4.7253e+299, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.56976191212145\n",
      "loss: 73.56976191212145\n",
      "h_val:  tensor(1.4110e+299, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.56694266319417\n",
      "loss: 73.56694266319417\n",
      "h_val:  tensor(3.5512e+298, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.56414596177562\n",
      "loss: 73.56414596177562\n",
      "h_val:  tensor(1.4772e+297, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.56347436693177\n",
      "loss: 73.56347436693177\n",
      "h_val:  tensor(3.1402e+297, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.56068188936783\n",
      "loss: 73.56068188936783\n",
      "h_val:  tensor(3.6493e+297, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.55995472183865\n",
      "loss: 73.55995472183865\n",
      "h_val:  tensor(2.5161e+297, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.55863802484109\n",
      "loss: 73.55863802484109\n",
      "h_val:  tensor(8.6718e+296, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.55748501139014\n",
      "loss: 73.55748501139014\n",
      "h_val:  tensor(3.8316e+296, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.55606567868669\n",
      "loss: 73.55606567868669\n",
      "h_val:  tensor(2.1088e+296, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.55540227925\n",
      "loss: 73.55540227925\n",
      "h_val:  tensor(8.9351e+295, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.55452202304681\n",
      "loss: 73.55452202304681\n",
      "h_val:  tensor(3.1943e+295, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.55346315284449\n",
      "loss: 73.55346315284449\n",
      "h_val:  tensor(1.2887e+295, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.55243325046484\n",
      "loss: 73.55243325046484\n",
      "h_val:  tensor(1.4480e+295, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5517884607176\n",
      "loss: 73.5517884607176\n",
      "h_val:  tensor(1.8203e+295, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.55133096344755\n",
      "loss: 73.55133096344755\n",
      "h_val:  tensor(1.8962e+295, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.55088479691578\n",
      "loss: 73.55088479691578\n",
      "h_val:  tensor(1.0920e+295, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.55008166879539\n",
      "loss: 73.55008166879539\n",
      "h_val:  tensor(6.4604e+294, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.54902212607956\n",
      "loss: 73.54902212607956\n",
      "h_val:  tensor(3.4507e+294, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.54848913601558\n",
      "loss: 73.54848913601558\n",
      "h_val:  tensor(1.3663e+294, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.54793405844185\n",
      "loss: 73.54793405844185\n",
      "h_val:  tensor(4.7988e+293, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.54740711481364\n",
      "loss: 73.54740711481364\n",
      "h_val:  tensor(3.5958e+293, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5466917651696\n",
      "loss: 73.5466917651696\n",
      "h_val:  tensor(1.8940e+293, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.54532093230898\n",
      "loss: 73.54532093230898\n",
      "h_val:  tensor(1.1558e+293, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.54439321771014\n",
      "loss: 73.54439321771014\n",
      "h_val:  tensor(1.6744e+292, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.54341275481731\n",
      "loss: 73.54341275481731\n",
      "h_val:  tensor(1.3779e+292, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.54171959192749\n",
      "loss: 73.54171959192749\n",
      "h_val:  tensor(1.3412e+292, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.54127279714851\n",
      "loss: 73.54127279714851\n",
      "h_val:  tensor(8.7399e+291, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.54050234010556\n",
      "loss: 73.54050234010556\n",
      "h_val:  tensor(3.0206e+291, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5401894705159\n",
      "loss: 73.5401894705159\n",
      "h_val:  tensor(2.7722e+291, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53934665589459\n",
      "loss: 73.53934665589459\n",
      "h_val:  tensor(2.2829e+291, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53897276835839\n",
      "loss: 73.53897276835839\n",
      "h_val:  tensor(1.6999e+291, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53856146851787\n",
      "loss: 73.53856146851787\n",
      "h_val:  tensor(1.2173e+291, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53790984012889\n",
      "loss: 73.53790984012889\n",
      "h_val:  tensor(5.1313e+290, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53872074051958\n",
      "loss: 73.53872074051958\n",
      "h_val:  tensor(9.0228e+290, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53760659198171\n",
      "loss: 73.53760659198171\n",
      "h_val:  tensor(7.5930e+290, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53720837940054\n",
      "loss: 73.53720837940054\n",
      "h_val:  tensor(7.2433e+290, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53701095181812\n",
      "loss: 73.53701095181812\n",
      "h_val:  tensor(6.1020e+290, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5367889362982\n",
      "loss: 73.5367889362982\n",
      "h_val:  tensor(5.0209e+290, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5365625195523\n",
      "loss: 73.5365625195523\n",
      "h_val:  tensor(4.1745e+290, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53634253244975\n",
      "loss: 73.53634253244975\n",
      "h_val:  tensor(2.9508e+290, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53610242305477\n",
      "loss: 73.53610242305477\n",
      "h_val:  tensor(2.7221e+290, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53593287512187\n",
      "loss: 73.53593287512187\n",
      "h_val:  tensor(2.6295e+290, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5357751886177\n",
      "loss: 73.5357751886177\n",
      "h_val:  tensor(1.8596e+290, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53527484909613\n",
      "loss: 73.53527484909613\n",
      "h_val:  tensor(1.2766e+290, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53498321012471\n",
      "loss: 73.53498321012471\n",
      "h_val:  tensor(1.2005e+290, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5347990037399\n",
      "loss: 73.5347990037399\n",
      "h_val:  tensor(9.2361e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53454442750342\n",
      "loss: 73.53454442750342\n",
      "h_val:  tensor(7.5173e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53429224616303\n",
      "loss: 73.53429224616303\n",
      "h_val:  tensor(4.6413e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53388597972689\n",
      "loss: 73.53388597972689\n",
      "h_val:  tensor(3.6674e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53369402787114\n",
      "loss: 73.53369402787114\n",
      "h_val:  tensor(4.0858e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53352061592447\n",
      "loss: 73.53352061592447\n",
      "h_val:  tensor(4.2971e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53339264104596\n",
      "loss: 73.53339264104596\n",
      "h_val:  tensor(3.9827e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53324126101823\n",
      "loss: 73.53324126101823\n",
      "h_val:  tensor(3.2322e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53320161279184\n",
      "loss: 73.53320161279184\n",
      "h_val:  tensor(2.9552e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53303905768225\n",
      "loss: 73.53303905768225\n",
      "h_val:  tensor(2.7997e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53300437100118\n",
      "loss: 73.53300437100118\n",
      "h_val:  tensor(2.5322e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53294991372982\n",
      "loss: 73.53294991372982\n",
      "h_val:  tensor(2.2408e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53288207466329\n",
      "loss: 73.53288207466329\n",
      "h_val:  tensor(2.0713e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53279687860457\n",
      "loss: 73.53279687860457\n",
      "h_val:  tensor(1.9774e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53271970028507\n",
      "loss: 73.53271970028507\n",
      "h_val:  tensor(2.0667e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53265544111976\n",
      "loss: 73.53265544111976\n",
      "h_val:  tensor(2.1016e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53261102192378\n",
      "loss: 73.53261102192378\n",
      "h_val:  tensor(2.0389e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5325503863345\n",
      "loss: 73.5325503863345\n",
      "h_val:  tensor(1.9203e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53252587051179\n",
      "loss: 73.53252587051179\n",
      "h_val:  tensor(1.8607e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53250093268586\n",
      "loss: 73.53250093268586\n",
      "h_val:  tensor(1.7579e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53248287285449\n",
      "loss: 73.53248287285449\n",
      "h_val:  tensor(1.6991e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53246909214758\n",
      "loss: 73.53246909214758\n",
      "h_val:  tensor(1.5560e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53244974547552\n",
      "loss: 73.53244974547552\n",
      "h_val:  tensor(1.5421e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5324248371313\n",
      "loss: 73.5324248371313\n",
      "h_val:  tensor(1.5627e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53241073723582\n",
      "loss: 73.53241073723582\n",
      "h_val:  tensor(1.5773e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53239213213918\n",
      "loss: 73.53239213213918\n",
      "h_val:  tensor(1.5633e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53236603651082\n",
      "loss: 73.53236603651082\n",
      "h_val:  tensor(1.5128e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53232854226576\n",
      "loss: 73.53232854226576\n",
      "h_val:  tensor(1.3155e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53226639682464\n",
      "loss: 73.53226639682464\n",
      "h_val:  tensor(1.1076e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53225594339816\n",
      "loss: 73.53225594339816\n",
      "h_val:  tensor(1.0497e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53219273893252\n",
      "loss: 73.53219273893252\n",
      "h_val:  tensor(1.0018e+289, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5321734824297\n",
      "loss: 73.5321734824297\n",
      "h_val:  tensor(9.1623e+288, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53214912158965\n",
      "loss: 73.53214912158965\n",
      "h_val:  tensor(8.4252e+288, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5321203996681\n",
      "loss: 73.5321203996681\n",
      "h_val:  tensor(6.8077e+288, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53211606999243\n",
      "loss: 73.53211606999243\n",
      "h_val:  tensor(7.5361e+288, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53209405508079\n",
      "loss: 73.53209405508079\n",
      "h_val:  tensor(6.9833e+288, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53205385148293\n",
      "loss: 73.53205385148293\n",
      "h_val:  tensor(6.7732e+288, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53202096066654\n",
      "loss: 73.53202096066654\n",
      "h_val:  tensor(6.2181e+288, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53196532255642\n",
      "loss: 73.53196532255642\n",
      "h_val:  tensor(5.5269e+288, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5319019416325\n",
      "loss: 73.5319019416325\n",
      "h_val:  tensor(4.9431e+288, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53182839573175\n",
      "loss: 73.53182839573175\n",
      "h_val:  tensor(4.0255e+288, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53172456072691\n",
      "loss: 73.53172456072691\n",
      "h_val:  tensor(3.4377e+288, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53168061523354\n",
      "loss: 73.53168061523354\n",
      "h_val:  tensor(3.3111e+288, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53163172455291\n",
      "loss: 73.53163172455291\n",
      "h_val:  tensor(2.9560e+288, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53150249074321\n",
      "loss: 73.53150249074321\n",
      "h_val:  tensor(2.9251e+288, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53143211753333\n",
      "loss: 73.53143211753333\n",
      "h_val:  tensor(2.8888e+288, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53135360831973\n",
      "loss: 73.53135360831973\n",
      "h_val:  tensor(2.7696e+288, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53120456040455\n",
      "loss: 73.53120456040455\n",
      "h_val:  tensor(2.6176e+288, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5311204092884\n",
      "loss: 73.5311204092884\n",
      "h_val:  tensor(2.0491e+288, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53097214790182\n",
      "loss: 73.53097214790182\n",
      "h_val:  tensor(1.7895e+288, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53083746633207\n",
      "loss: 73.53083746633207\n",
      "h_val:  tensor(1.6840e+288, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53075058747035\n",
      "loss: 73.53075058747035\n",
      "h_val:  tensor(1.4823e+288, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5306268724761\n",
      "loss: 73.5306268724761\n",
      "h_val:  tensor(1.3185e+288, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53051959845332\n",
      "loss: 73.53051959845332\n",
      "h_val:  tensor(1.2311e+288, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53039186322238\n",
      "loss: 73.53039186322238\n",
      "h_val:  tensor(1.1044e+288, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53022194336108\n",
      "loss: 73.53022194336108\n",
      "h_val:  tensor(1.0107e+288, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.53011395885957\n",
      "loss: 73.53011395885957\n",
      "h_val:  tensor(8.9266e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52995651095215\n",
      "loss: 73.52995651095215\n",
      "h_val:  tensor(7.9494e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52981412102712\n",
      "loss: 73.52981412102712\n",
      "h_val:  tensor(7.6565e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52959904766057\n",
      "loss: 73.52959904766057\n",
      "h_val:  tensor(8.0019e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5294281132194\n",
      "loss: 73.5294281132194\n",
      "h_val:  tensor(8.4488e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5292989078196\n",
      "loss: 73.5292989078196\n",
      "h_val:  tensor(9.3411e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52918642831871\n",
      "loss: 73.52918642831871\n",
      "h_val:  tensor(9.6244e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52897128033538\n",
      "loss: 73.52897128033538\n",
      "h_val:  tensor(9.3625e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52886092382501\n",
      "loss: 73.52886092382501\n",
      "h_val:  tensor(8.8582e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52874721733521\n",
      "loss: 73.52874721733521\n",
      "h_val:  tensor(7.9572e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5285944393674\n",
      "loss: 73.5285944393674\n",
      "h_val:  tensor(5.5372e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52862446361208\n",
      "loss: 73.52862446361208\n",
      "h_val:  tensor(6.7010e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52847721212603\n",
      "loss: 73.52847721212603\n",
      "h_val:  tensor(6.0456e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52834034973489\n",
      "loss: 73.52834034973489\n",
      "h_val:  tensor(5.7074e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52824921699465\n",
      "loss: 73.52824921699465\n",
      "h_val:  tensor(5.4016e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5281579002792\n",
      "loss: 73.5281579002792\n",
      "h_val:  tensor(5.3730e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52805988437825\n",
      "loss: 73.52805988437825\n",
      "h_val:  tensor(5.1537e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52794258090618\n",
      "loss: 73.52794258090618\n",
      "h_val:  tensor(5.2306e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52788111863029\n",
      "loss: 73.52788111863029\n",
      "h_val:  tensor(5.4098e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52781471114136\n",
      "loss: 73.52781471114136\n",
      "h_val:  tensor(5.6344e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52770975783383\n",
      "loss: 73.52770975783383\n",
      "h_val:  tensor(6.1886e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5275647376055\n",
      "loss: 73.5275647376055\n",
      "h_val:  tensor(6.3502e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52742782289126\n",
      "loss: 73.52742782289126\n",
      "h_val:  tensor(6.2554e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52734863530901\n",
      "loss: 73.52734863530901\n",
      "h_val:  tensor(6.2148e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52730770663099\n",
      "loss: 73.52730770663099\n",
      "h_val:  tensor(6.2441e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.527270550297\n",
      "loss: 73.527270550297\n",
      "h_val:  tensor(6.2477e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52722980667382\n",
      "loss: 73.52722980667382\n",
      "h_val:  tensor(6.3513e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52718663045457\n",
      "loss: 73.52718663045457\n",
      "h_val:  tensor(6.3182e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52716511073282\n",
      "loss: 73.52716511073282\n",
      "h_val:  tensor(6.1255e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52710289691055\n",
      "loss: 73.52710289691055\n",
      "h_val:  tensor(5.6112e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52707267732606\n",
      "loss: 73.52707267732606\n",
      "h_val:  tensor(5.5773e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.527001670403\n",
      "loss: 73.527001670403\n",
      "h_val:  tensor(5.4937e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52696498753227\n",
      "loss: 73.52696498753227\n",
      "h_val:  tensor(5.4076e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52693643003336\n",
      "loss: 73.52693643003336\n",
      "h_val:  tensor(5.3880e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5268877648432\n",
      "loss: 73.5268877648432\n",
      "h_val:  tensor(5.3295e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52695440672638\n",
      "loss: 73.52695440672638\n",
      "h_val:  tensor(5.3656e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52685844302734\n",
      "loss: 73.52685844302734\n",
      "h_val:  tensor(5.4659e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52681551654463\n",
      "loss: 73.52681551654463\n",
      "h_val:  tensor(5.5407e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5267945067946\n",
      "loss: 73.5267945067946\n",
      "h_val:  tensor(5.6176e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52677640481213\n",
      "loss: 73.52677640481213\n",
      "h_val:  tensor(5.5069e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52676996960844\n",
      "loss: 73.52676996960844\n",
      "h_val:  tensor(5.5332e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52675308341483\n",
      "loss: 73.52675308341483\n",
      "h_val:  tensor(5.4955e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5267453671977\n",
      "loss: 73.5267453671977\n",
      "h_val:  tensor(5.4448e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52673787426788\n",
      "loss: 73.52673787426788\n",
      "h_val:  tensor(5.4179e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52672198152884\n",
      "loss: 73.52672198152884\n",
      "h_val:  tensor(5.3592e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5267063024485\n",
      "loss: 73.5267063024485\n",
      "h_val:  tensor(5.3490e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52668916748515\n",
      "loss: 73.52668916748515\n",
      "h_val:  tensor(5.3193e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52667286206119\n",
      "loss: 73.52667286206119\n",
      "h_val:  tensor(5.1648e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52665052685555\n",
      "loss: 73.52665052685555\n",
      "h_val:  tensor(4.8027e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52664914853689\n",
      "loss: 73.52664914853689\n",
      "h_val:  tensor(4.9763e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52663255344412\n",
      "loss: 73.52663255344412\n",
      "h_val:  tensor(4.7598e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5266101781898\n",
      "loss: 73.5266101781898\n",
      "h_val:  tensor(4.5921e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52659368588888\n",
      "loss: 73.52659368588888\n",
      "h_val:  tensor(4.4859e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52657804999905\n",
      "loss: 73.52657804999905\n",
      "h_val:  tensor(4.3569e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52656084647052\n",
      "loss: 73.52656084647052\n",
      "h_val:  tensor(4.3568e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.526540823604\n",
      "loss: 73.526540823604\n",
      "h_val:  tensor(4.4132e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52652721543876\n",
      "loss: 73.52652721543876\n",
      "h_val:  tensor(4.4184e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5265124300506\n",
      "loss: 73.5265124300506\n",
      "h_val:  tensor(4.3017e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5264887471959\n",
      "loss: 73.5264887471959\n",
      "h_val:  tensor(4.0461e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52645550381297\n",
      "loss: 73.52645550381297\n",
      "h_val:  tensor(3.6116e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5264174017622\n",
      "loss: 73.5264174017622\n",
      "h_val:  tensor(3.0230e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52638404074465\n",
      "loss: 73.52638404074465\n",
      "h_val:  tensor(2.9205e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52635161177525\n",
      "loss: 73.52635161177525\n",
      "h_val:  tensor(2.7969e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52631935266301\n",
      "loss: 73.52631935266301\n",
      "h_val:  tensor(2.5691e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52627497589388\n",
      "loss: 73.52627497589388\n",
      "h_val:  tensor(2.0866e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52627152432456\n",
      "loss: 73.52627152432456\n",
      "h_val:  tensor(2.3069e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52624707498546\n",
      "loss: 73.52624707498546\n",
      "h_val:  tensor(2.0986e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52620372180203\n",
      "loss: 73.52620372180203\n",
      "h_val:  tensor(1.9487e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52616816070798\n",
      "loss: 73.52616816070798\n",
      "h_val:  tensor(1.7580e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52613452314574\n",
      "loss: 73.52613452314574\n",
      "h_val:  tensor(1.6483e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52610096542045\n",
      "loss: 73.52610096542045\n",
      "h_val:  tensor(1.5479e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52606371730691\n",
      "loss: 73.52606371730691\n",
      "h_val:  tensor(1.4128e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52601141279347\n",
      "loss: 73.52601141279347\n",
      "h_val:  tensor(1.3434e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52598987083871\n",
      "loss: 73.52598987083871\n",
      "h_val:  tensor(1.3434e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52596535178384\n",
      "loss: 73.52596535178384\n",
      "h_val:  tensor(1.3316e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52594496486569\n",
      "loss: 73.52594496486569\n",
      "h_val:  tensor(1.3046e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52593239516756\n",
      "loss: 73.52593239516756\n",
      "h_val:  tensor(1.3033e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52591272432129\n",
      "loss: 73.52591272432129\n",
      "h_val:  tensor(1.2524e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52589826628356\n",
      "loss: 73.52589826628356\n",
      "h_val:  tensor(1.2564e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52588978574673\n",
      "loss: 73.52588978574673\n",
      "h_val:  tensor(1.2447e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52588072493273\n",
      "loss: 73.52588072493273\n",
      "h_val:  tensor(1.2196e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52586566983547\n",
      "loss: 73.52586566983547\n",
      "h_val:  tensor(1.1474e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52583600441511\n",
      "loss: 73.52583600441511\n",
      "h_val:  tensor(1.0733e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52581730955265\n",
      "loss: 73.52581730955265\n",
      "h_val:  tensor(1.0558e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52578303683728\n",
      "loss: 73.52578303683728\n",
      "h_val:  tensor(1.0471e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52575618892467\n",
      "loss: 73.52575618892467\n",
      "h_val:  tensor(1.0504e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52573409937777\n",
      "loss: 73.52573409937777\n",
      "h_val:  tensor(1.0724e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52568552323913\n",
      "loss: 73.52568552323913\n",
      "h_val:  tensor(1.1639e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52565345203362\n",
      "loss: 73.52565345203362\n",
      "h_val:  tensor(1.1845e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52560367978039\n",
      "loss: 73.52560367978039\n",
      "h_val:  tensor(1.1820e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52558855801011\n",
      "loss: 73.52558855801011\n",
      "h_val:  tensor(1.1962e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5255766536091\n",
      "loss: 73.5255766536091\n",
      "h_val:  tensor(1.2199e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52556270781338\n",
      "loss: 73.52556270781338\n",
      "h_val:  tensor(1.2542e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52554332564607\n",
      "loss: 73.52554332564607\n",
      "h_val:  tensor(1.3071e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52552770469877\n",
      "loss: 73.52552770469877\n",
      "h_val:  tensor(1.2950e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52552155110406\n",
      "loss: 73.52552155110406\n",
      "h_val:  tensor(1.2908e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52551720455062\n",
      "loss: 73.52551720455062\n",
      "h_val:  tensor(1.2935e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52551282824027\n",
      "loss: 73.52551282824027\n",
      "h_val:  tensor(1.3096e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52550253300225\n",
      "loss: 73.52550253300225\n",
      "h_val:  tensor(1.3639e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52549202650611\n",
      "loss: 73.52549202650611\n",
      "h_val:  tensor(1.3832e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52547939009676\n",
      "loss: 73.52547939009676\n",
      "h_val:  tensor(1.4608e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52546138948365\n",
      "loss: 73.52546138948365\n",
      "h_val:  tensor(1.5085e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52545164794441\n",
      "loss: 73.52545164794441\n",
      "h_val:  tensor(1.6327e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52543335295067\n",
      "loss: 73.52543335295067\n",
      "h_val:  tensor(1.7608e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52542380682988\n",
      "loss: 73.52542380682988\n",
      "h_val:  tensor(1.7551e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52540807173818\n",
      "loss: 73.52540807173818\n",
      "h_val:  tensor(1.7290e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52540122347179\n",
      "loss: 73.52540122347179\n",
      "h_val:  tensor(1.7196e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52539482797222\n",
      "loss: 73.52539482797222\n",
      "h_val:  tensor(1.6996e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52538563559362\n",
      "loss: 73.52538563559362\n",
      "h_val:  tensor(1.7253e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52537349561133\n",
      "loss: 73.52537349561133\n",
      "h_val:  tensor(1.7684e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52536047963618\n",
      "loss: 73.52536047963618\n",
      "h_val:  tensor(1.8103e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5253503057929\n",
      "loss: 73.5253503057929\n",
      "h_val:  tensor(1.8396e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5253417432521\n",
      "loss: 73.5253417432521\n",
      "h_val:  tensor(1.8480e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52533217821068\n",
      "loss: 73.52533217821068\n",
      "h_val:  tensor(1.8435e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5253179770045\n",
      "loss: 73.5253179770045\n",
      "h_val:  tensor(1.8323e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52530654935421\n",
      "loss: 73.52530654935421\n",
      "h_val:  tensor(1.7428e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52530546385398\n",
      "loss: 73.52530546385398\n",
      "h_val:  tensor(1.7855e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52529698543628\n",
      "loss: 73.52529698543628\n",
      "h_val:  tensor(1.7660e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52528558843987\n",
      "loss: 73.52528558843987\n",
      "h_val:  tensor(1.7491e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52527898560022\n",
      "loss: 73.52527898560022\n",
      "h_val:  tensor(1.7294e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52527083567445\n",
      "loss: 73.52527083567445\n",
      "h_val:  tensor(1.7138e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52526287289712\n",
      "loss: 73.52526287289712\n",
      "h_val:  tensor(1.7275e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52525560942972\n",
      "loss: 73.52525560942972\n",
      "h_val:  tensor(1.7551e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52524475055277\n",
      "loss: 73.52524475055277\n",
      "h_val:  tensor(1.7694e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52523823552305\n",
      "loss: 73.52523823552305\n",
      "h_val:  tensor(1.7690e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52522985416013\n",
      "loss: 73.52522985416013\n",
      "h_val:  tensor(1.7076e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52520803511808\n",
      "loss: 73.52520803511808\n",
      "h_val:  tensor(1.6852e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52519709643632\n",
      "loss: 73.52519709643632\n",
      "h_val:  tensor(1.6234e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52518319565111\n",
      "loss: 73.52518319565111\n",
      "h_val:  tensor(1.5987e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52517663259577\n",
      "loss: 73.52517663259577\n",
      "h_val:  tensor(1.5901e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52516649005338\n",
      "loss: 73.52516649005338\n",
      "h_val:  tensor(1.6000e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52516066105098\n",
      "loss: 73.52516066105098\n",
      "h_val:  tensor(1.6159e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52514842564366\n",
      "loss: 73.52514842564366\n",
      "h_val:  tensor(1.6087e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52513269469642\n",
      "loss: 73.52513269469642\n",
      "h_val:  tensor(1.5419e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.525117256183\n",
      "loss: 73.525117256183\n",
      "h_val:  tensor(1.5087e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5250926452757\n",
      "loss: 73.5250926452757\n",
      "h_val:  tensor(1.4548e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52507694650727\n",
      "loss: 73.52507694650727\n",
      "h_val:  tensor(1.3865e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52506068304449\n",
      "loss: 73.52506068304449\n",
      "h_val:  tensor(1.2915e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52505485443915\n",
      "loss: 73.52505485443915\n",
      "h_val:  tensor(1.2968e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52503349457955\n",
      "loss: 73.52503349457955\n",
      "h_val:  tensor(1.3043e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52502303470322\n",
      "loss: 73.52502303470322\n",
      "h_val:  tensor(1.3085e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52501089149096\n",
      "loss: 73.52501089149096\n",
      "h_val:  tensor(1.2904e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52499420684704\n",
      "loss: 73.52499420684704\n",
      "h_val:  tensor(1.2556e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5249945280878\n",
      "loss: 73.5249945280878\n",
      "h_val:  tensor(1.2729e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52498479297121\n",
      "loss: 73.52498479297121\n",
      "h_val:  tensor(1.2639e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52497678557289\n",
      "loss: 73.52497678557289\n",
      "h_val:  tensor(1.2593e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52496889986239\n",
      "loss: 73.52496889986239\n",
      "h_val:  tensor(1.2648e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.524955022671\n",
      "loss: 73.524955022671\n",
      "h_val:  tensor(1.2799e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52493787494011\n",
      "loss: 73.52493787494011\n",
      "h_val:  tensor(1.2954e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52491730887881\n",
      "loss: 73.52491730887881\n",
      "h_val:  tensor(1.3278e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52489541889115\n",
      "loss: 73.52489541889115\n",
      "h_val:  tensor(1.3382e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52487896170138\n",
      "loss: 73.52487896170138\n",
      "h_val:  tensor(1.3465e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52486313062467\n",
      "loss: 73.52486313062467\n",
      "h_val:  tensor(1.3222e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52485020279265\n",
      "loss: 73.52485020279265\n",
      "h_val:  tensor(1.2880e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52484130657407\n",
      "loss: 73.52484130657407\n",
      "h_val:  tensor(1.2568e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52483359115529\n",
      "loss: 73.52483359115529\n",
      "h_val:  tensor(1.2133e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52482512662633\n",
      "loss: 73.52482512662633\n",
      "h_val:  tensor(1.2067e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52481504383127\n",
      "loss: 73.52481504383127\n",
      "h_val:  tensor(1.2142e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5248103779806\n",
      "loss: 73.5248103779806\n",
      "h_val:  tensor(1.2262e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52480416010926\n",
      "loss: 73.52480416010926\n",
      "h_val:  tensor(1.2089e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52479478928886\n",
      "loss: 73.52479478928886\n",
      "h_val:  tensor(1.2173e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52478190319347\n",
      "loss: 73.52478190319347\n",
      "h_val:  tensor(1.2094e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52476796133932\n",
      "loss: 73.52476796133932\n",
      "h_val:  tensor(1.1908e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52475973199645\n",
      "loss: 73.52475973199645\n",
      "h_val:  tensor(1.1855e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52475291769844\n",
      "loss: 73.52475291769844\n",
      "h_val:  tensor(1.1720e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52474351462826\n",
      "loss: 73.52474351462826\n",
      "h_val:  tensor(1.1761e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52473838366575\n",
      "loss: 73.52473838366575\n",
      "h_val:  tensor(1.1895e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52473376607811\n",
      "loss: 73.52473376607811\n",
      "h_val:  tensor(1.2116e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5247266877638\n",
      "loss: 73.5247266877638\n",
      "h_val:  tensor(1.2467e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52472114378808\n",
      "loss: 73.52472114378808\n",
      "h_val:  tensor(1.2412e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52471617951836\n",
      "loss: 73.52471617951836\n",
      "h_val:  tensor(1.2330e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52470995070261\n",
      "loss: 73.52470995070261\n",
      "h_val:  tensor(1.2198e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52470226408172\n",
      "loss: 73.52470226408172\n",
      "h_val:  tensor(1.2090e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5246942287487\n",
      "loss: 73.5246942287487\n",
      "h_val:  tensor(1.1930e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52468247080624\n",
      "loss: 73.52468247080624\n",
      "h_val:  tensor(1.1931e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52467829625992\n",
      "loss: 73.52467829625992\n",
      "h_val:  tensor(1.1937e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52467395836678\n",
      "loss: 73.52467395836678\n",
      "h_val:  tensor(1.1966e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5246677421414\n",
      "loss: 73.5246677421414\n",
      "h_val:  tensor(1.1847e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52466406484604\n",
      "loss: 73.52466406484604\n",
      "h_val:  tensor(1.1939e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52465487543375\n",
      "loss: 73.52465487543375\n",
      "h_val:  tensor(1.1971e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52464941560592\n",
      "loss: 73.52464941560592\n",
      "h_val:  tensor(1.1984e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52464305976241\n",
      "loss: 73.52464305976241\n",
      "h_val:  tensor(1.2040e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52463004422617\n",
      "loss: 73.52463004422617\n",
      "h_val:  tensor(1.2051e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5246215260987\n",
      "loss: 73.5246215260987\n",
      "h_val:  tensor(1.2208e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52459688170887\n",
      "loss: 73.52459688170887\n",
      "h_val:  tensor(1.2290e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52458567955641\n",
      "loss: 73.52458567955641\n",
      "h_val:  tensor(1.2381e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52457361213521\n",
      "loss: 73.52457361213521\n",
      "h_val:  tensor(1.2447e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5245567402853\n",
      "loss: 73.5245567402853\n",
      "h_val:  tensor(1.2321e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52455932975981\n",
      "loss: 73.52455932975981\n",
      "h_val:  tensor(1.2386e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52454502354053\n",
      "loss: 73.52454502354053\n",
      "h_val:  tensor(1.2412e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52452630602318\n",
      "loss: 73.52452630602318\n",
      "h_val:  tensor(1.2335e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52451625657991\n",
      "loss: 73.52451625657991\n",
      "h_val:  tensor(1.2099e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52450044061489\n",
      "loss: 73.52450044061489\n",
      "h_val:  tensor(1.1891e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52448556508385\n",
      "loss: 73.52448556508385\n",
      "h_val:  tensor(1.1860e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52447395087688\n",
      "loss: 73.52447395087688\n",
      "h_val:  tensor(1.1728e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52444934195361\n",
      "loss: 73.52444934195361\n",
      "h_val:  tensor(1.1634e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52443507989122\n",
      "loss: 73.52443507989122\n",
      "h_val:  tensor(1.1657e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5244199086904\n",
      "loss: 73.5244199086904\n",
      "h_val:  tensor(1.1568e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5243970588361\n",
      "loss: 73.5243970588361\n",
      "h_val:  tensor(1.1663e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5243867732858\n",
      "loss: 73.5243867732858\n",
      "h_val:  tensor(1.1694e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52436529867197\n",
      "loss: 73.52436529867197\n",
      "h_val:  tensor(1.1944e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5243372573395\n",
      "loss: 73.5243372573395\n",
      "h_val:  tensor(1.1893e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52431220323898\n",
      "loss: 73.52431220323898\n",
      "h_val:  tensor(1.1803e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52430197265134\n",
      "loss: 73.52430197265134\n",
      "h_val:  tensor(1.1539e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5242859829078\n",
      "loss: 73.5242859829078\n",
      "h_val:  tensor(1.0600e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52428136262324\n",
      "loss: 73.52428136262324\n",
      "h_val:  tensor(1.0667e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52425678938528\n",
      "loss: 73.52425678938528\n",
      "h_val:  tensor(1.0507e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5242432420379\n",
      "loss: 73.5242432420379\n",
      "h_val:  tensor(1.0117e+287, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52422463640387\n",
      "loss: 73.52422463640387\n",
      "h_val:  tensor(9.5291e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52418887109744\n",
      "loss: 73.52418887109744\n",
      "h_val:  tensor(7.8159e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52422741401689\n",
      "loss: 73.52422741401689\n",
      "h_val:  tensor(8.8086e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52416067349922\n",
      "loss: 73.52416067349922\n",
      "h_val:  tensor(8.0666e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52410558693961\n",
      "loss: 73.52410558693961\n",
      "h_val:  tensor(7.6021e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5240656111817\n",
      "loss: 73.5240656111817\n",
      "h_val:  tensor(7.0233e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52403485481435\n",
      "loss: 73.52403485481435\n",
      "h_val:  tensor(6.9508e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52400973037126\n",
      "loss: 73.52400973037126\n",
      "h_val:  tensor(6.7819e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52398408022106\n",
      "loss: 73.52398408022106\n",
      "h_val:  tensor(6.4940e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52396854513324\n",
      "loss: 73.52396854513324\n",
      "h_val:  tensor(6.2576e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52395250632915\n",
      "loss: 73.52395250632915\n",
      "h_val:  tensor(6.0672e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52393769378433\n",
      "loss: 73.52393769378433\n",
      "h_val:  tensor(5.9651e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52391719622446\n",
      "loss: 73.52391719622446\n",
      "h_val:  tensor(5.6451e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52391998188327\n",
      "loss: 73.52391998188327\n",
      "h_val:  tensor(5.8114e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52390596732285\n",
      "loss: 73.52390596732285\n",
      "h_val:  tensor(5.8641e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52389092665572\n",
      "loss: 73.52389092665572\n",
      "h_val:  tensor(5.9035e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.523870087096\n",
      "loss: 73.523870087096\n",
      "h_val:  tensor(5.7504e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52385045799357\n",
      "loss: 73.52385045799357\n",
      "h_val:  tensor(5.5883e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52383293855404\n",
      "loss: 73.52383293855404\n",
      "h_val:  tensor(5.4247e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52382239178641\n",
      "loss: 73.52382239178641\n",
      "h_val:  tensor(5.1218e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5238110007378\n",
      "loss: 73.5238110007378\n",
      "h_val:  tensor(4.9541e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52380219028198\n",
      "loss: 73.52380219028198\n",
      "h_val:  tensor(4.8609e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52379295454999\n",
      "loss: 73.52379295454999\n",
      "h_val:  tensor(4.6933e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52378153933087\n",
      "loss: 73.52378153933087\n",
      "h_val:  tensor(4.6695e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52377354923509\n",
      "loss: 73.52377354923509\n",
      "h_val:  tensor(4.6568e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52376615257201\n",
      "loss: 73.52376615257201\n",
      "h_val:  tensor(4.3446e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52373803942491\n",
      "loss: 73.52373803942491\n",
      "h_val:  tensor(4.2160e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5237271209104\n",
      "loss: 73.5237271209104\n",
      "h_val:  tensor(4.2395e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52371361644059\n",
      "loss: 73.52371361644059\n",
      "h_val:  tensor(4.2189e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52370635422629\n",
      "loss: 73.52370635422629\n",
      "h_val:  tensor(4.1451e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5236889615\n",
      "loss: 73.5236889615\n",
      "h_val:  tensor(4.1172e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52367619628544\n",
      "loss: 73.52367619628544\n",
      "h_val:  tensor(4.0068e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52367780113335\n",
      "loss: 73.52367780113335\n",
      "h_val:  tensor(4.0636e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52366743211803\n",
      "loss: 73.52366743211803\n",
      "h_val:  tensor(4.0756e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52365668956695\n",
      "loss: 73.52365668956695\n",
      "h_val:  tensor(4.1024e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52364489928512\n",
      "loss: 73.52364489928512\n",
      "h_val:  tensor(4.1252e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52362939911511\n",
      "loss: 73.52362939911511\n",
      "h_val:  tensor(4.0965e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52361724257281\n",
      "loss: 73.52361724257281\n",
      "h_val:  tensor(4.0295e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5235922058569\n",
      "loss: 73.5235922058569\n",
      "h_val:  tensor(3.9101e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52357422503559\n",
      "loss: 73.52357422503559\n",
      "h_val:  tensor(3.7676e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52355888056144\n",
      "loss: 73.52355888056144\n",
      "h_val:  tensor(3.6277e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52353920610504\n",
      "loss: 73.52353920610504\n",
      "h_val:  tensor(3.2039e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52354195983435\n",
      "loss: 73.52354195983435\n",
      "h_val:  tensor(3.4178e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52352518144059\n",
      "loss: 73.52352518144059\n",
      "h_val:  tensor(3.3212e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52350609248369\n",
      "loss: 73.52350609248369\n",
      "h_val:  tensor(3.2774e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5234942267003\n",
      "loss: 73.5234942267003\n",
      "h_val:  tensor(3.2340e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52348417108531\n",
      "loss: 73.52348417108531\n",
      "h_val:  tensor(3.1172e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5234699386043\n",
      "loss: 73.5234699386043\n",
      "h_val:  tensor(2.9987e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52345381288741\n",
      "loss: 73.52345381288741\n",
      "h_val:  tensor(2.8889e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52343810525622\n",
      "loss: 73.52343810525622\n",
      "h_val:  tensor(2.7198e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52341304589754\n",
      "loss: 73.52341304589754\n",
      "h_val:  tensor(2.5515e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52339641007458\n",
      "loss: 73.52339641007458\n",
      "h_val:  tensor(2.4103e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52335064877285\n",
      "loss: 73.52335064877285\n",
      "h_val:  tensor(2.2902e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52331560855805\n",
      "loss: 73.52331560855805\n",
      "h_val:  tensor(2.1654e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52328835297389\n",
      "loss: 73.52328835297389\n",
      "h_val:  tensor(1.9647e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52324958867888\n",
      "loss: 73.52324958867888\n",
      "h_val:  tensor(1.7084e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52326071108077\n",
      "loss: 73.52326071108077\n",
      "h_val:  tensor(1.8448e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52322819228613\n",
      "loss: 73.52322819228613\n",
      "h_val:  tensor(1.8078e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52321603795934\n",
      "loss: 73.52321603795934\n",
      "h_val:  tensor(1.8114e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52321017360737\n",
      "loss: 73.52321017360737\n",
      "h_val:  tensor(1.8171e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52320289190318\n",
      "loss: 73.52320289190318\n",
      "h_val:  tensor(1.8019e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52319637631308\n",
      "loss: 73.52319637631308\n",
      "h_val:  tensor(1.7761e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52319234634119\n",
      "loss: 73.52319234634119\n",
      "h_val:  tensor(1.7325e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5231871688605\n",
      "loss: 73.5231871688605\n",
      "h_val:  tensor(1.6658e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52318287056059\n",
      "loss: 73.52318287056059\n",
      "h_val:  tensor(1.6442e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52317894676077\n",
      "loss: 73.52317894676077\n",
      "h_val:  tensor(1.6015e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52317481169962\n",
      "loss: 73.52317481169962\n",
      "h_val:  tensor(1.5991e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52317164199259\n",
      "loss: 73.52317164199259\n",
      "h_val:  tensor(1.5986e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5231684930526\n",
      "loss: 73.5231684930526\n",
      "h_val:  tensor(1.5663e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52316375230103\n",
      "loss: 73.52316375230103\n",
      "h_val:  tensor(1.5666e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52316097625878\n",
      "loss: 73.52316097625878\n",
      "h_val:  tensor(1.5583e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52315941116622\n",
      "loss: 73.52315941116622\n",
      "h_val:  tensor(1.5052e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52315436471247\n",
      "loss: 73.52315436471247\n",
      "h_val:  tensor(1.4773e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52315229181796\n",
      "loss: 73.52315229181796\n",
      "h_val:  tensor(1.4394e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5231502058067\n",
      "loss: 73.5231502058067\n",
      "h_val:  tensor(1.4301e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5231488813345\n",
      "loss: 73.5231488813345\n",
      "h_val:  tensor(1.4222e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52314701346609\n",
      "loss: 73.52314701346609\n",
      "h_val:  tensor(1.4093e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52314510355784\n",
      "loss: 73.52314510355784\n",
      "h_val:  tensor(1.3928e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52314242317806\n",
      "loss: 73.52314242317806\n",
      "h_val:  tensor(1.3372e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52313808211471\n",
      "loss: 73.52313808211471\n",
      "h_val:  tensor(1.2927e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52313412672332\n",
      "loss: 73.52313412672332\n",
      "h_val:  tensor(1.2624e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52313066730446\n",
      "loss: 73.52313066730446\n",
      "h_val:  tensor(1.1531e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52312541137165\n",
      "loss: 73.52312541137165\n",
      "h_val:  tensor(1.1357e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52312234415932\n",
      "loss: 73.52312234415932\n",
      "h_val:  tensor(1.1432e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52312101420877\n",
      "loss: 73.52312101420877\n",
      "h_val:  tensor(1.1428e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5231201679299\n",
      "loss: 73.5231201679299\n",
      "h_val:  tensor(1.1397e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52311925532076\n",
      "loss: 73.52311925532076\n",
      "h_val:  tensor(1.1314e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52311798627827\n",
      "loss: 73.52311798627827\n",
      "h_val:  tensor(1.1282e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52311651647568\n",
      "loss: 73.52311651647568\n",
      "h_val:  tensor(1.1274e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52311542928548\n",
      "loss: 73.52311542928548\n",
      "h_val:  tensor(1.1293e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52311391472173\n",
      "loss: 73.52311391472173\n",
      "h_val:  tensor(1.1270e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52311276269897\n",
      "loss: 73.52311276269897\n",
      "h_val:  tensor(1.1294e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52311110644794\n",
      "loss: 73.52311110644794\n",
      "h_val:  tensor(1.1303e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5231093307943\n",
      "loss: 73.5231093307943\n",
      "h_val:  tensor(1.1281e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52310811809411\n",
      "loss: 73.52310811809411\n",
      "h_val:  tensor(1.1276e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52310651507386\n",
      "loss: 73.52310651507386\n",
      "h_val:  tensor(1.1219e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52310498886753\n",
      "loss: 73.52310498886753\n",
      "h_val:  tensor(1.1216e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52310405262475\n",
      "loss: 73.52310405262475\n",
      "h_val:  tensor(1.1179e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52310200422157\n",
      "loss: 73.52310200422157\n",
      "h_val:  tensor(1.1081e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52310095890826\n",
      "loss: 73.52310095890826\n",
      "h_val:  tensor(1.1068e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52309953674005\n",
      "loss: 73.52309953674005\n",
      "h_val:  tensor(1.0992e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52309778958545\n",
      "loss: 73.52309778958545\n",
      "h_val:  tensor(1.0924e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52309653742002\n",
      "loss: 73.52309653742002\n",
      "h_val:  tensor(1.0729e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52309249958377\n",
      "loss: 73.52309249958377\n",
      "h_val:  tensor(1.0481e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5230901574474\n",
      "loss: 73.5230901574474\n",
      "h_val:  tensor(1.0554e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52308661135557\n",
      "loss: 73.52308661135557\n",
      "h_val:  tensor(1.0610e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52308459392596\n",
      "loss: 73.52308459392596\n",
      "h_val:  tensor(1.0647e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52308186050874\n",
      "loss: 73.52308186050874\n",
      "h_val:  tensor(1.0616e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52307697311244\n",
      "loss: 73.52307697311244\n",
      "h_val:  tensor(1.0554e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5230728038711\n",
      "loss: 73.5230728038711\n",
      "h_val:  tensor(1.0488e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52306986895032\n",
      "loss: 73.52306986895032\n",
      "h_val:  tensor(1.0319e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52306633550145\n",
      "loss: 73.52306633550145\n",
      "h_val:  tensor(1.0197e+286, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52306245377386\n",
      "loss: 73.52306245377386\n",
      "h_val:  tensor(9.8752e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52305353443633\n",
      "loss: 73.52305353443633\n",
      "h_val:  tensor(9.4785e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5230558882496\n",
      "loss: 73.5230558882496\n",
      "h_val:  tensor(9.6957e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52304912014516\n",
      "loss: 73.52304912014516\n",
      "h_val:  tensor(9.6725e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52304306053605\n",
      "loss: 73.52304306053605\n",
      "h_val:  tensor(9.6474e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52303655623433\n",
      "loss: 73.52303655623433\n",
      "h_val:  tensor(9.6572e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52303244788521\n",
      "loss: 73.52303244788521\n",
      "h_val:  tensor(9.5688e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52302792580497\n",
      "loss: 73.52302792580497\n",
      "h_val:  tensor(9.4072e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52302314559671\n",
      "loss: 73.52302314559671\n",
      "h_val:  tensor(9.1584e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52301801142144\n",
      "loss: 73.52301801142144\n",
      "h_val:  tensor(8.8908e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52301059371614\n",
      "loss: 73.52301059371614\n",
      "h_val:  tensor(8.4850e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52299707281949\n",
      "loss: 73.52299707281949\n",
      "h_val:  tensor(7.8840e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52299094197306\n",
      "loss: 73.52299094197306\n",
      "h_val:  tensor(7.9914e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52297446209428\n",
      "loss: 73.52297446209428\n",
      "h_val:  tensor(8.1243e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52296681359847\n",
      "loss: 73.52296681359847\n",
      "h_val:  tensor(8.2082e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52295707001032\n",
      "loss: 73.52295707001032\n",
      "h_val:  tensor(8.1381e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52294295611932\n",
      "loss: 73.52294295611932\n",
      "h_val:  tensor(7.6074e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52294374406473\n",
      "loss: 73.52294374406473\n",
      "h_val:  tensor(7.8738e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52293491451611\n",
      "loss: 73.52293491451611\n",
      "h_val:  tensor(7.6112e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52292299338957\n",
      "loss: 73.52292299338957\n",
      "h_val:  tensor(7.4925e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52291959344137\n",
      "loss: 73.52291959344137\n",
      "h_val:  tensor(7.3103e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52291590458442\n",
      "loss: 73.52291590458442\n",
      "h_val:  tensor(7.2476e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52291276356073\n",
      "loss: 73.52291276356073\n",
      "h_val:  tensor(7.2180e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52290928333993\n",
      "loss: 73.52290928333993\n",
      "h_val:  tensor(7.2533e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5229035580382\n",
      "loss: 73.5229035580382\n",
      "h_val:  tensor(7.2257e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52290023029242\n",
      "loss: 73.52290023029242\n",
      "h_val:  tensor(7.2026e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52289490532812\n",
      "loss: 73.52289490532812\n",
      "h_val:  tensor(7.1529e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52289911671359\n",
      "loss: 73.52289911671359\n",
      "h_val:  tensor(7.1802e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52288848777731\n",
      "loss: 73.52288848777731\n",
      "h_val:  tensor(7.1051e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52287910707436\n",
      "loss: 73.52287910707436\n",
      "h_val:  tensor(7.0699e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5228757328587\n",
      "loss: 73.5228757328587\n",
      "h_val:  tensor(7.0591e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52287041325742\n",
      "loss: 73.52287041325742\n",
      "h_val:  tensor(7.0844e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52286710219344\n",
      "loss: 73.52286710219344\n",
      "h_val:  tensor(7.1266e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5228645720452\n",
      "loss: 73.5228645720452\n",
      "h_val:  tensor(7.1965e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52286250345547\n",
      "loss: 73.52286250345547\n",
      "h_val:  tensor(7.2177e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52286120727258\n",
      "loss: 73.52286120727258\n",
      "h_val:  tensor(7.2194e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52285828666676\n",
      "loss: 73.52285828666676\n",
      "h_val:  tensor(7.1411e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52285692510623\n",
      "loss: 73.52285692510623\n",
      "h_val:  tensor(7.0633e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52285217152473\n",
      "loss: 73.52285217152473\n",
      "h_val:  tensor(6.9704e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52285018597519\n",
      "loss: 73.52285018597519\n",
      "h_val:  tensor(6.8664e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52284837148709\n",
      "loss: 73.52284837148709\n",
      "h_val:  tensor(6.7177e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52284523900903\n",
      "loss: 73.52284523900903\n",
      "h_val:  tensor(6.4370e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5228457951616\n",
      "loss: 73.5228457951616\n",
      "h_val:  tensor(6.5826e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5228426624621\n",
      "loss: 73.5228426624621\n",
      "h_val:  tensor(6.4667e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52283936520672\n",
      "loss: 73.52283936520672\n",
      "h_val:  tensor(6.4534e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52283775786763\n",
      "loss: 73.52283775786763\n",
      "h_val:  tensor(6.4521e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52283596132298\n",
      "loss: 73.52283596132298\n",
      "h_val:  tensor(6.4047e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52283486956522\n",
      "loss: 73.52283486956522\n",
      "h_val:  tensor(6.4047e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52283247329575\n",
      "loss: 73.52283247329575\n",
      "h_val:  tensor(6.3658e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52283094633248\n",
      "loss: 73.52283094633248\n",
      "h_val:  tensor(6.3139e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52282982637173\n",
      "loss: 73.52282982637173\n",
      "h_val:  tensor(6.2447e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5228282035498\n",
      "loss: 73.5228282035498\n",
      "h_val:  tensor(6.1692e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52282679354543\n",
      "loss: 73.52282679354543\n",
      "h_val:  tensor(6.1699e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52282563344562\n",
      "loss: 73.52282563344562\n",
      "h_val:  tensor(6.1642e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52282439281892\n",
      "loss: 73.52282439281892\n",
      "h_val:  tensor(6.1310e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52282278893962\n",
      "loss: 73.52282278893962\n",
      "h_val:  tensor(6.0002e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52282089465274\n",
      "loss: 73.52282089465274\n",
      "h_val:  tensor(5.9530e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52281963081869\n",
      "loss: 73.52281963081869\n",
      "h_val:  tensor(5.9159e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52281881918452\n",
      "loss: 73.52281881918452\n",
      "h_val:  tensor(5.8714e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52281815085324\n",
      "loss: 73.52281815085324\n",
      "h_val:  tensor(5.8350e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52281747311457\n",
      "loss: 73.52281747311457\n",
      "h_val:  tensor(5.8226e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52281691488565\n",
      "loss: 73.52281691488565\n",
      "h_val:  tensor(5.8102e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52281620260372\n",
      "loss: 73.52281620260372\n",
      "h_val:  tensor(5.7861e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52281522169923\n",
      "loss: 73.52281522169923\n",
      "h_val:  tensor(5.7567e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52281397174248\n",
      "loss: 73.52281397174248\n",
      "h_val:  tensor(5.7180e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.522812715188\n",
      "loss: 73.522812715188\n",
      "h_val:  tensor(5.6652e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52281157042383\n",
      "loss: 73.52281157042383\n",
      "h_val:  tensor(5.6350e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52281004423132\n",
      "loss: 73.52281004423132\n",
      "h_val:  tensor(5.5875e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52280875326274\n",
      "loss: 73.52280875326274\n",
      "h_val:  tensor(5.5691e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52280744615064\n",
      "loss: 73.52280744615064\n",
      "h_val:  tensor(5.5522e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52280623832641\n",
      "loss: 73.52280623832641\n",
      "h_val:  tensor(5.4829e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52280440166395\n",
      "loss: 73.52280440166395\n",
      "h_val:  tensor(5.4119e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52280346706851\n",
      "loss: 73.52280346706851\n",
      "h_val:  tensor(5.3902e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52280255805348\n",
      "loss: 73.52280255805348\n",
      "h_val:  tensor(5.3678e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52280213885517\n",
      "loss: 73.52280213885517\n",
      "h_val:  tensor(5.3192e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52280133628317\n",
      "loss: 73.52280133628317\n",
      "h_val:  tensor(5.2362e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52279991883104\n",
      "loss: 73.52279991883104\n",
      "h_val:  tensor(5.0810e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52279936722204\n",
      "loss: 73.52279936722204\n",
      "h_val:  tensor(5.1031e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52279726039693\n",
      "loss: 73.52279726039693\n",
      "h_val:  tensor(5.1185e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52279608674613\n",
      "loss: 73.52279608674613\n",
      "h_val:  tensor(5.1146e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5227947855577\n",
      "loss: 73.5227947855577\n",
      "h_val:  tensor(5.0855e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52279283564415\n",
      "loss: 73.52279283564415\n",
      "h_val:  tensor(4.9457e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52279442855024\n",
      "loss: 73.52279442855024\n",
      "h_val:  tensor(5.0306e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52279174874235\n",
      "loss: 73.52279174874235\n",
      "h_val:  tensor(4.9790e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52278965636201\n",
      "loss: 73.52278965636201\n",
      "h_val:  tensor(4.9406e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52278822071838\n",
      "loss: 73.52278822071838\n",
      "h_val:  tensor(4.8904e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52278634249392\n",
      "loss: 73.52278634249392\n",
      "h_val:  tensor(4.8443e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52278482734647\n",
      "loss: 73.52278482734647\n",
      "h_val:  tensor(4.8465e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52278290907611\n",
      "loss: 73.52278290907611\n",
      "h_val:  tensor(4.8534e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52278012972106\n",
      "loss: 73.52278012972106\n",
      "h_val:  tensor(4.8530e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52277829967841\n",
      "loss: 73.52277829967841\n",
      "h_val:  tensor(4.7766e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52277750769677\n",
      "loss: 73.52277750769677\n",
      "h_val:  tensor(4.7772e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52277519066766\n",
      "loss: 73.52277519066766\n",
      "h_val:  tensor(4.7559e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5227744295281\n",
      "loss: 73.5227744295281\n",
      "h_val:  tensor(4.6991e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52277320751325\n",
      "loss: 73.52277320751325\n",
      "h_val:  tensor(4.5813e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52277182677716\n",
      "loss: 73.52277182677716\n",
      "h_val:  tensor(4.5365e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52277028339773\n",
      "loss: 73.52277028339773\n",
      "h_val:  tensor(4.5232e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52276946453006\n",
      "loss: 73.52276946453006\n",
      "h_val:  tensor(4.5195e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52276852440853\n",
      "loss: 73.52276852440853\n",
      "h_val:  tensor(4.5059e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52276719663182\n",
      "loss: 73.52276719663182\n",
      "h_val:  tensor(4.5017e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52276525872652\n",
      "loss: 73.52276525872652\n",
      "h_val:  tensor(4.4990e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5227627625272\n",
      "loss: 73.5227627625272\n",
      "h_val:  tensor(4.4628e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52276105949718\n",
      "loss: 73.52276105949718\n",
      "h_val:  tensor(4.4608e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5227596540875\n",
      "loss: 73.5227596540875\n",
      "h_val:  tensor(4.4533e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52275853533587\n",
      "loss: 73.52275853533587\n",
      "h_val:  tensor(4.4345e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5227577793299\n",
      "loss: 73.5227577793299\n",
      "h_val:  tensor(4.4217e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52275707557013\n",
      "loss: 73.52275707557013\n",
      "h_val:  tensor(4.4240e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5227566013779\n",
      "loss: 73.5227566013779\n",
      "h_val:  tensor(4.4270e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52275593210159\n",
      "loss: 73.52275593210159\n",
      "h_val:  tensor(4.4471e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5227553568808\n",
      "loss: 73.5227553568808\n",
      "h_val:  tensor(4.4475e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52275465250841\n",
      "loss: 73.52275465250841\n",
      "h_val:  tensor(4.4467e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5227535900869\n",
      "loss: 73.5227535900869\n",
      "h_val:  tensor(4.4310e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52275242753032\n",
      "loss: 73.52275242753032\n",
      "h_val:  tensor(4.4145e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52275070691638\n",
      "loss: 73.52275070691638\n",
      "h_val:  tensor(4.3863e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52275016941056\n",
      "loss: 73.52275016941056\n",
      "h_val:  tensor(4.3958e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52274824021514\n",
      "loss: 73.52274824021514\n",
      "h_val:  tensor(4.4068e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52274758817735\n",
      "loss: 73.52274758817735\n",
      "h_val:  tensor(4.4269e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5227464142915\n",
      "loss: 73.5227464142915\n",
      "h_val:  tensor(4.4508e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52274492194807\n",
      "loss: 73.52274492194807\n",
      "h_val:  tensor(4.4886e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52274597624847\n",
      "loss: 73.52274597624847\n",
      "h_val:  tensor(4.4656e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52274416063601\n",
      "loss: 73.52274416063601\n",
      "h_val:  tensor(4.4719e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5227432701994\n",
      "loss: 73.5227432701994\n",
      "h_val:  tensor(4.4752e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52274271751456\n",
      "loss: 73.52274271751456\n",
      "h_val:  tensor(4.4739e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52274215652596\n",
      "loss: 73.52274215652596\n",
      "h_val:  tensor(4.4982e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52274161915668\n",
      "loss: 73.52274161915668\n",
      "h_val:  tensor(4.5016e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52274111365512\n",
      "loss: 73.52274111365512\n",
      "h_val:  tensor(4.5152e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52274036825291\n",
      "loss: 73.52274036825291\n",
      "h_val:  tensor(4.5383e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52273952938697\n",
      "loss: 73.52273952938697\n",
      "h_val:  tensor(4.5669e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52273806405148\n",
      "loss: 73.52273806405148\n",
      "h_val:  tensor(4.6180e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52273688846671\n",
      "loss: 73.52273688846671\n",
      "h_val:  tensor(4.6043e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52273573576991\n",
      "loss: 73.52273573576991\n",
      "h_val:  tensor(4.5817e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5227348368229\n",
      "loss: 73.5227348368229\n",
      "h_val:  tensor(4.5699e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52273416517968\n",
      "loss: 73.52273416517968\n",
      "h_val:  tensor(4.5544e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52273344367735\n",
      "loss: 73.52273344367735\n",
      "h_val:  tensor(4.5597e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5227324510929\n",
      "loss: 73.5227324510929\n",
      "h_val:  tensor(4.5702e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52273184563211\n",
      "loss: 73.52273184563211\n",
      "h_val:  tensor(4.5915e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52273083861547\n",
      "loss: 73.52273083861547\n",
      "h_val:  tensor(4.6009e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52273031018706\n",
      "loss: 73.52273031018706\n",
      "h_val:  tensor(4.6202e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.522729023615\n",
      "loss: 73.522729023615\n",
      "h_val:  tensor(4.6297e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52272832191869\n",
      "loss: 73.52272832191869\n",
      "h_val:  tensor(4.6420e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52272742265907\n",
      "loss: 73.52272742265907\n",
      "h_val:  tensor(4.6717e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52272581469794\n",
      "loss: 73.52272581469794\n",
      "h_val:  tensor(4.6879e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52272554413253\n",
      "loss: 73.52272554413253\n",
      "h_val:  tensor(4.7246e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52272218538353\n",
      "loss: 73.52272218538353\n",
      "h_val:  tensor(4.7352e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52272094132054\n",
      "loss: 73.52272094132054\n",
      "h_val:  tensor(4.7479e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52271939776575\n",
      "loss: 73.52271939776575\n",
      "h_val:  tensor(4.7565e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52271731602835\n",
      "loss: 73.52271731602835\n",
      "h_val:  tensor(4.7893e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52271666336208\n",
      "loss: 73.52271666336208\n",
      "h_val:  tensor(4.7586e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52271182197808\n",
      "loss: 73.52271182197808\n",
      "h_val:  tensor(4.7372e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52271039073082\n",
      "loss: 73.52271039073082\n",
      "h_val:  tensor(4.7098e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52270874393267\n",
      "loss: 73.52270874393267\n",
      "h_val:  tensor(4.6705e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52270677784531\n",
      "loss: 73.52270677784531\n",
      "h_val:  tensor(4.6323e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5227044119045\n",
      "loss: 73.5227044119045\n",
      "h_val:  tensor(4.6193e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5227014399711\n",
      "loss: 73.5227014399711\n",
      "h_val:  tensor(4.6331e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52269934626064\n",
      "loss: 73.52269934626064\n",
      "h_val:  tensor(4.6645e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5226969569056\n",
      "loss: 73.5226969569056\n",
      "h_val:  tensor(4.7821e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5226972177362\n",
      "loss: 73.5226972177362\n",
      "h_val:  tensor(4.7209e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52269509507742\n",
      "loss: 73.52269509507742\n",
      "h_val:  tensor(4.7750e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52269200350918\n",
      "loss: 73.52269200350918\n",
      "h_val:  tensor(4.8266e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52268950914045\n",
      "loss: 73.52268950914045\n",
      "h_val:  tensor(4.8770e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52268664457767\n",
      "loss: 73.52268664457767\n",
      "h_val:  tensor(4.9505e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52268451533632\n",
      "loss: 73.52268451533632\n",
      "h_val:  tensor(4.9424e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52268208584704\n",
      "loss: 73.52268208584704\n",
      "h_val:  tensor(4.9175e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52267964968586\n",
      "loss: 73.52267964968586\n",
      "h_val:  tensor(4.9115e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52267773981161\n",
      "loss: 73.52267773981161\n",
      "h_val:  tensor(4.9075e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52267447029583\n",
      "loss: 73.52267447029583\n",
      "h_val:  tensor(4.9313e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52267081723288\n",
      "loss: 73.52267081723288\n",
      "h_val:  tensor(4.9677e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52266869948318\n",
      "loss: 73.52266869948318\n",
      "h_val:  tensor(5.0120e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52266564634128\n",
      "loss: 73.52266564634128\n",
      "h_val:  tensor(5.1097e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52266343589852\n",
      "loss: 73.52266343589852\n",
      "h_val:  tensor(5.1088e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52266105944312\n",
      "loss: 73.52266105944312\n",
      "h_val:  tensor(5.1201e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52265819018545\n",
      "loss: 73.52265819018545\n",
      "h_val:  tensor(5.1522e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52265645123481\n",
      "loss: 73.52265645123481\n",
      "h_val:  tensor(5.3143e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52265236206446\n",
      "loss: 73.52265236206446\n",
      "h_val:  tensor(5.4825e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52265146891071\n",
      "loss: 73.52265146891071\n",
      "h_val:  tensor(5.4709e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5226487037007\n",
      "loss: 73.5226487037007\n",
      "h_val:  tensor(5.5108e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52264744389562\n",
      "loss: 73.52264744389562\n",
      "h_val:  tensor(5.6258e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52264537246796\n",
      "loss: 73.52264537246796\n",
      "h_val:  tensor(5.7929e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52264263591928\n",
      "loss: 73.52264263591928\n",
      "h_val:  tensor(5.9914e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52264721016316\n",
      "loss: 73.52264721016316\n",
      "h_val:  tensor(5.8533e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5226415036726\n",
      "loss: 73.5226415036726\n",
      "h_val:  tensor(5.9437e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52263959666448\n",
      "loss: 73.52263959666448\n",
      "h_val:  tensor(5.9488e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52263823060134\n",
      "loss: 73.52263823060134\n",
      "h_val:  tensor(5.9034e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52263674847254\n",
      "loss: 73.52263674847254\n",
      "h_val:  tensor(5.8901e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52263681199565\n",
      "loss: 73.52263681199565\n",
      "h_val:  tensor(5.8969e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52263608476719\n",
      "loss: 73.52263608476719\n",
      "h_val:  tensor(5.8574e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5226353277423\n",
      "loss: 73.5226353277423\n",
      "h_val:  tensor(5.8419e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52263464413547\n",
      "loss: 73.52263464413547\n",
      "h_val:  tensor(5.8704e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5226340709454\n",
      "loss: 73.5226340709454\n",
      "h_val:  tensor(5.9030e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52263344413903\n",
      "loss: 73.52263344413903\n",
      "h_val:  tensor(5.9517e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52263271691292\n",
      "loss: 73.52263271691292\n",
      "h_val:  tensor(6.0670e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52263131361973\n",
      "loss: 73.52263131361973\n",
      "h_val:  tensor(6.1122e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52262992766242\n",
      "loss: 73.52262992766242\n",
      "h_val:  tensor(6.1199e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52262847889276\n",
      "loss: 73.52262847889276\n",
      "h_val:  tensor(6.1673e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52262749852123\n",
      "loss: 73.52262749852123\n",
      "h_val:  tensor(6.1633e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52262612279603\n",
      "loss: 73.52262612279603\n",
      "h_val:  tensor(6.1873e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52262548139981\n",
      "loss: 73.52262548139981\n",
      "h_val:  tensor(6.2775e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52262438588284\n",
      "loss: 73.52262438588284\n",
      "h_val:  tensor(6.3266e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5226243834332\n",
      "loss: 73.5226243834332\n",
      "h_val:  tensor(6.3020e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52262397335733\n",
      "loss: 73.52262397335733\n",
      "h_val:  tensor(6.3616e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52262338927784\n",
      "loss: 73.52262338927784\n",
      "h_val:  tensor(6.3735e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5226229910346\n",
      "loss: 73.5226229910346\n",
      "h_val:  tensor(6.3591e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5226226013698\n",
      "loss: 73.5226226013698\n",
      "h_val:  tensor(6.3224e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5226223406703\n",
      "loss: 73.5226223406703\n",
      "h_val:  tensor(6.3068e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5226219426491\n",
      "loss: 73.5226219426491\n",
      "h_val:  tensor(6.2959e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52262161253131\n",
      "loss: 73.52262161253131\n",
      "h_val:  tensor(6.2990e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52262124485837\n",
      "loss: 73.52262124485837\n",
      "h_val:  tensor(6.3306e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52262052542137\n",
      "loss: 73.52262052542137\n",
      "h_val:  tensor(6.4055e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5226202122789\n",
      "loss: 73.5226202122789\n",
      "h_val:  tensor(6.4358e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52261877414011\n",
      "loss: 73.52261877414011\n",
      "h_val:  tensor(6.4646e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52261816394584\n",
      "loss: 73.52261816394584\n",
      "h_val:  tensor(6.5017e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52261740768667\n",
      "loss: 73.52261740768667\n",
      "h_val:  tensor(6.5785e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52261753331172\n",
      "loss: 73.52261753331172\n",
      "h_val:  tensor(6.5377e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52261693325336\n",
      "loss: 73.52261693325336\n",
      "h_val:  tensor(6.5519e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52261615273441\n",
      "loss: 73.52261615273441\n",
      "h_val:  tensor(6.5581e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52261538075112\n",
      "loss: 73.52261538075112\n",
      "h_val:  tensor(6.5646e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52261478663492\n",
      "loss: 73.52261478663492\n",
      "h_val:  tensor(6.5926e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52261435382275\n",
      "loss: 73.52261435382275\n",
      "h_val:  tensor(6.5975e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52261390786626\n",
      "loss: 73.52261390786626\n",
      "h_val:  tensor(6.6113e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52261357062237\n",
      "loss: 73.52261357062237\n",
      "h_val:  tensor(6.6160e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52261329435552\n",
      "loss: 73.52261329435552\n",
      "h_val:  tensor(6.5969e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52261272687602\n",
      "loss: 73.52261272687602\n",
      "h_val:  tensor(6.5806e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52261207666244\n",
      "loss: 73.52261207666244\n",
      "h_val:  tensor(6.5447e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52261150886623\n",
      "loss: 73.52261150886623\n",
      "h_val:  tensor(6.5074e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52261088853551\n",
      "loss: 73.52261088853551\n",
      "h_val:  tensor(6.4734e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52261030246244\n",
      "loss: 73.52261030246244\n",
      "h_val:  tensor(6.4725e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52260973928706\n",
      "loss: 73.52260973928706\n",
      "h_val:  tensor(6.4923e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52260902779214\n",
      "loss: 73.52260902779214\n",
      "h_val:  tensor(6.4818e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52260857223871\n",
      "loss: 73.52260857223871\n",
      "h_val:  tensor(6.4633e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52260807108375\n",
      "loss: 73.52260807108375\n",
      "h_val:  tensor(6.3714e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52260709087957\n",
      "loss: 73.52260709087957\n",
      "h_val:  tensor(6.3384e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52260645274613\n",
      "loss: 73.52260645274613\n",
      "h_val:  tensor(6.3400e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52260608422198\n",
      "loss: 73.52260608422198\n",
      "h_val:  tensor(6.3421e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52260560663565\n",
      "loss: 73.52260560663565\n",
      "h_val:  tensor(6.3415e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52260519836615\n",
      "loss: 73.52260519836615\n",
      "h_val:  tensor(6.3530e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52260486246833\n",
      "loss: 73.52260486246833\n",
      "h_val:  tensor(6.3422e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52260433488433\n",
      "loss: 73.52260433488433\n",
      "h_val:  tensor(6.3290e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52260393546297\n",
      "loss: 73.52260393546297\n",
      "h_val:  tensor(6.3005e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5226034382163\n",
      "loss: 73.5226034382163\n",
      "h_val:  tensor(6.2296e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52260283634165\n",
      "loss: 73.52260283634165\n",
      "h_val:  tensor(6.1639e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52260216387772\n",
      "loss: 73.52260216387772\n",
      "h_val:  tensor(6.1519e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52260178371255\n",
      "loss: 73.52260178371255\n",
      "h_val:  tensor(6.1175e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52260116718894\n",
      "loss: 73.52260116718894\n",
      "h_val:  tensor(6.1198e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52260068815065\n",
      "loss: 73.52260068815065\n",
      "h_val:  tensor(6.1024e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52260014541488\n",
      "loss: 73.52260014541488\n",
      "h_val:  tensor(6.0766e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52259944473599\n",
      "loss: 73.52259944473599\n",
      "h_val:  tensor(6.0539e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52259901384515\n",
      "loss: 73.52259901384515\n",
      "h_val:  tensor(5.9909e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52259810173317\n",
      "loss: 73.52259810173317\n",
      "h_val:  tensor(5.9024e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52259846053182\n",
      "loss: 73.52259846053182\n",
      "h_val:  tensor(5.9543e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52259775530673\n",
      "loss: 73.52259775530673\n",
      "h_val:  tensor(5.9333e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.5225972411349\n",
      "loss: 73.5225972411349\n",
      "h_val:  tensor(5.9320e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52259682708801\n",
      "loss: 73.52259682708801\n",
      "h_val:  tensor(5.9379e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52259632447235\n",
      "loss: 73.52259632447235\n",
      "h_val:  tensor(5.9752e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52259569045101\n",
      "loss: 73.52259569045101\n",
      "h_val:  tensor(5.9837e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52259499038433\n",
      "loss: 73.52259499038433\n",
      "h_val:  tensor(5.9870e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52259433120703\n",
      "loss: 73.52259433120703\n",
      "h_val:  tensor(5.9787e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52259375392983\n",
      "loss: 73.52259375392983\n",
      "h_val:  tensor(5.9447e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52259308348577\n",
      "loss: 73.52259308348577\n",
      "h_val:  tensor(5.9041e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52259239883804\n",
      "loss: 73.52259239883804\n",
      "h_val:  tensor(5.8516e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52259196976357\n",
      "loss: 73.52259196976357\n",
      "h_val:  tensor(5.8561e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52259151664506\n",
      "loss: 73.52259151664506\n",
      "h_val:  tensor(5.8702e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52259111151886\n",
      "loss: 73.52259111151886\n",
      "h_val:  tensor(5.8762e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52259069656526\n",
      "loss: 73.52259069656526\n",
      "h_val:  tensor(5.9082e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52259065222565\n",
      "loss: 73.52259065222565\n",
      "h_val:  tensor(5.8928e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52259040134821\n",
      "loss: 73.52259040134821\n",
      "h_val:  tensor(5.8789e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52258995711337\n",
      "loss: 73.52258995711337\n",
      "h_val:  tensor(5.8601e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52258962044449\n",
      "loss: 73.52258962044449\n",
      "h_val:  tensor(5.8421e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52258924441819\n",
      "loss: 73.52258924441819\n",
      "h_val:  tensor(5.8211e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52258893902578\n",
      "loss: 73.52258893902578\n",
      "h_val:  tensor(5.8305e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52258862641547\n",
      "loss: 73.52258862641547\n",
      "h_val:  tensor(5.8615e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52258820649902\n",
      "loss: 73.52258820649902\n",
      "h_val:  tensor(5.8829e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52258796341106\n",
      "loss: 73.52258796341106\n",
      "h_val:  tensor(5.9246e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52258770914636\n",
      "loss: 73.52258770914636\n",
      "h_val:  tensor(5.9223e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52258749835315\n",
      "loss: 73.52258749835315\n",
      "h_val:  tensor(5.9120e+285, grad_fn=<SubBackward0>)\n",
      "L_risk: 73.52258734334703\n",
      "loss: 73.52258734334703\n",
      "h_new:  5.912010445558147e+285\n",
      "[[ 0.74622298  7.01254337]\n",
      " [ 2.33961614 25.64379679]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "x = np.random.normal(10,2, 100)\n",
    "\n",
    "np.random.seed(24)\n",
    "l = np.random.normal(0,1, 100) \n",
    "\n",
    "# marker\n",
    "np.random.seed(25)\n",
    "b = np.random.normal(0,1, 100) \n",
    "\n",
    "\n",
    "y = [x**2 + l for x, l in zip(x, l)]\n",
    "\n",
    "X = np.column_stack((x, y))\n",
    "\n",
    "n = X.shape[0]\n",
    "d = X.shape[1]\n",
    "\n",
    "print(n)\n",
    "print(d)\n",
    "\n",
    "model = NotearsRKHS(n, d, \"gaussian\")\n",
    "#W_est, output = notears_nonlinear(model, X)\n",
    "output = learning(model, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = np.array(y)\n",
    "y_hat = output[:, 1].detach().numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 6))  # Optional: specifies the figure size\n",
    "\n",
    "\n",
    "# Plot predicted values\n",
    "plt.scatter(x, y_hat, label='Predicted Values', color='red', marker='s')  # Plot x vs. y_hat\n",
    "\n",
    "# Plot actual values\n",
    "plt.scatter(x, y, label='Actual Values', color='blue', marker='o')  # Plot x vs. y\n",
    "\n",
    "plt.legend()  # Show legend to differentiate between actual and predicted values\n",
    "plt.title('Actual vs Predicted Values')  # Optional: Adds a title to the plot\n",
    "plt.xlabel('X-axis label')  # Optional: Label for the x-axis\n",
    "plt.ylabel('Y-axis label')  # Optional: Label for the y-axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "2\n",
      "h_val:  tensor(0., grad_fn=<SubBackward0>)\n",
      "L_risk: 0.7508787428908255\n",
      "loss: 0.7508787428908255\n",
      "h_val:  tensor(38.0748, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.9999191573739542\n",
      "loss: 0.9999191573739542\n",
      "h_val:  tensor(1.1240, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.21299428599063952\n",
      "loss: 0.21299428599063952\n",
      "h_val:  tensor(1.0670, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.17851682260466112\n",
      "loss: 0.17851682260466112\n",
      "h_val:  tensor(0.9699, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13902772882069775\n",
      "loss: 0.13902772882069775\n",
      "h_val:  tensor(1.3087, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.04913937383266307\n",
      "loss: 0.04913937383266307\n",
      "h_val:  tensor(1.3396, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.045638578058927036\n",
      "loss: 0.045638578058927036\n",
      "h_val:  tensor(1.3367, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.03981456627639504\n",
      "loss: 0.03981456627639504\n",
      "h_val:  tensor(1.2937, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.030493832410050183\n",
      "loss: 0.030493832410050183\n",
      "h_val:  tensor(1.2858, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.01835669070431346\n",
      "loss: 0.01835669070431346\n",
      "h_val:  tensor(1.5012, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.009962354711484704\n",
      "loss: 0.009962354711484704\n",
      "h_val:  tensor(1.4436, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.008394478659417353\n",
      "loss: 0.008394478659417353\n",
      "h_val:  tensor(1.3905, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.008031601762942646\n",
      "loss: 0.008031601762942646\n",
      "h_val:  tensor(1.3668, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.007866300017041495\n",
      "loss: 0.007866300017041495\n",
      "h_val:  tensor(1.2927, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0072506751385382675\n",
      "loss: 0.0072506751385382675\n",
      "h_val:  tensor(1.1830, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.006702819897744414\n",
      "loss: 0.006702819897744414\n",
      "h_val:  tensor(1.1873, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.005497461408745328\n",
      "loss: 0.005497461408745328\n",
      "h_val:  tensor(1.2095, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0038538698890320557\n",
      "loss: 0.0038538698890320557\n",
      "h_val:  tensor(1.2254, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.00348614347123106\n",
      "loss: 0.00348614347123106\n",
      "h_val:  tensor(1.2290, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0034378669436670953\n",
      "loss: 0.0034378669436670953\n",
      "h_val:  tensor(1.2285, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0034208717559419156\n",
      "loss: 0.0034208717559419156\n",
      "h_val:  tensor(1.2273, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0034141275351074283\n",
      "loss: 0.0034141275351074283\n",
      "h_val:  tensor(1.2223, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.003390986744426067\n",
      "loss: 0.003390986744426067\n",
      "h_val:  tensor(1.1988, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.003260529136604914\n",
      "loss: 0.003260529136604914\n",
      "h_val:  tensor(1.1648, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.003043820591441821\n",
      "loss: 0.003043820591441821\n",
      "h_val:  tensor(1.1364, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0027471217579987995\n",
      "loss: 0.0027471217579987995\n",
      "h_val:  tensor(1.1333, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0036508843569301487\n",
      "loss: 0.0036508843569301487\n",
      "h_val:  tensor(1.1328, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0026732337289525534\n",
      "loss: 0.0026732337289525534\n",
      "h_val:  tensor(1.1249, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.002512773074575406\n",
      "loss: 0.002512773074575406\n",
      "h_val:  tensor(1.1404, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0024824929155700015\n",
      "loss: 0.0024824929155700015\n",
      "h_val:  tensor(1.1414, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0024780251511569706\n",
      "loss: 0.0024780251511569706\n",
      "h_val:  tensor(1.1426, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0024773525169386633\n",
      "loss: 0.0024773525169386633\n",
      "h_val:  tensor(1.1443, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.002476741849982796\n",
      "loss: 0.002476741849982796\n",
      "h_val:  tensor(1.1449, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0024763130753703956\n",
      "loss: 0.0024763130753703956\n",
      "h_val:  tensor(1.1448, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.00247595139835692\n",
      "loss: 0.00247595139835692\n",
      "h_val:  tensor(1.1442, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0024747619458087788\n",
      "loss: 0.0024747619458087788\n",
      "h_val:  tensor(1.1442, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0024753687313743547\n",
      "loss: 0.0024753687313743547\n",
      "h_val:  tensor(1.1442, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0024736789697411025\n",
      "loss: 0.0024736789697411025\n",
      "h_val:  tensor(1.1442, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.002471490379759306\n",
      "loss: 0.002471490379759306\n",
      "h_val:  tensor(1.1462, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.00245548165652918\n",
      "loss: 0.00245548165652918\n",
      "h_val:  tensor(1.1502, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.002445178632234392\n",
      "loss: 0.002445178632234392\n",
      "h_val:  tensor(1.1554, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.002429970665619844\n",
      "loss: 0.002429970665619844\n",
      "h_val:  tensor(1.1708, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.002414191702790655\n",
      "loss: 0.002414191702790655\n",
      "h_val:  tensor(1.1707, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0023901119795565083\n",
      "loss: 0.0023901119795565083\n",
      "h_val:  tensor(1.1667, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.002356128655382419\n",
      "loss: 0.002356128655382419\n",
      "h_val:  tensor(1.1835, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.002423315011427902\n",
      "loss: 0.002423315011427902\n",
      "h_val:  tensor(1.1702, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0023505895119611334\n",
      "loss: 0.0023505895119611334\n",
      "h_val:  tensor(1.1690, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.002341776373425997\n",
      "loss: 0.002341776373425997\n",
      "h_val:  tensor(1.1684, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0023322350457757983\n",
      "loss: 0.0023322350457757983\n",
      "h_val:  tensor(1.1674, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.002316470216965844\n",
      "loss: 0.002316470216965844\n",
      "h_val:  tensor(1.1648, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0022881387713095678\n",
      "loss: 0.0022881387713095678\n",
      "h_val:  tensor(1.1574, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.002242400796042926\n",
      "loss: 0.002242400796042926\n",
      "h_val:  tensor(1.1484, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.002168330239264663\n",
      "loss: 0.002168330239264663\n",
      "h_val:  tensor(1.1380, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0020818417054627894\n",
      "loss: 0.0020818417054627894\n",
      "h_val:  tensor(1.1148, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0020141782867664163\n",
      "loss: 0.0020141782867664163\n",
      "h_val:  tensor(1.1165, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0019952872115784667\n",
      "loss: 0.0019952872115784667\n",
      "h_val:  tensor(1.1116, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0019813657384550585\n",
      "loss: 0.0019813657384550585\n",
      "h_val:  tensor(1.1113, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0019713492238465132\n",
      "loss: 0.0019713492238465132\n",
      "h_val:  tensor(1.1117, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001966114313939223\n",
      "loss: 0.001966114313939223\n",
      "h_val:  tensor(1.1109, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.00196343179694878\n",
      "loss: 0.00196343179694878\n",
      "h_val:  tensor(1.1114, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0019602997839717616\n",
      "loss: 0.0019602997839717616\n",
      "h_val:  tensor(1.1105, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001958851035656078\n",
      "loss: 0.001958851035656078\n",
      "h_val:  tensor(1.1093, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001958290948524185\n",
      "loss: 0.001958290948524185\n",
      "h_val:  tensor(1.1074, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0019589557953682956\n",
      "loss: 0.0019589557953682956\n",
      "h_val:  tensor(1.1087, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001958116043283463\n",
      "loss: 0.001958116043283463\n",
      "h_val:  tensor(1.1079, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001957795106111273\n",
      "loss: 0.001957795106111273\n",
      "h_val:  tensor(1.1071, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001957408757496566\n",
      "loss: 0.001957408757496566\n",
      "h_val:  tensor(1.1064, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0019570009802074894\n",
      "loss: 0.0019570009802074894\n",
      "h_val:  tensor(1.1060, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0019564826335562568\n",
      "loss: 0.0019564826335562568\n",
      "h_val:  tensor(1.1044, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001956212429684455\n",
      "loss: 0.001956212429684455\n",
      "h_val:  tensor(1.1079, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0019568059593372917\n",
      "loss: 0.0019568059593372917\n",
      "h_val:  tensor(1.1060, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0019549733191538384\n",
      "loss: 0.0019549733191538384\n",
      "h_val:  tensor(1.1064, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001953759113130097\n",
      "loss: 0.001953759113130097\n",
      "h_val:  tensor(1.1068, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0019531438339834644\n",
      "loss: 0.0019531438339834644\n",
      "h_val:  tensor(1.1072, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0019524682461494552\n",
      "loss: 0.0019524682461494552\n",
      "h_val:  tensor(1.1075, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0019514598136903373\n",
      "loss: 0.0019514598136903373\n",
      "h_val:  tensor(1.1080, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0019499656220097461\n",
      "loss: 0.0019499656220097461\n",
      "h_val:  tensor(1.1075, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0019466690997752725\n",
      "loss: 0.0019466690997752725\n",
      "h_val:  tensor(1.1061, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0019419503774680818\n",
      "loss: 0.0019419503774680818\n",
      "h_val:  tensor(1.1034, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0019362675149995523\n",
      "loss: 0.0019362675149995523\n",
      "h_val:  tensor(1.1015, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0019305324229681303\n",
      "loss: 0.0019305324229681303\n",
      "h_val:  tensor(1.0985, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0019222764970219523\n",
      "loss: 0.0019222764970219523\n",
      "h_val:  tensor(1.0956, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001921430660041758\n",
      "loss: 0.001921430660041758\n",
      "h_val:  tensor(1.0969, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0019172656829863004\n",
      "loss: 0.0019172656829863004\n",
      "h_val:  tensor(1.0953, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0019131505100547345\n",
      "loss: 0.0019131505100547345\n",
      "h_val:  tensor(1.0933, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001908603059880483\n",
      "loss: 0.001908603059880483\n",
      "h_val:  tensor(1.0909, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0019027942068919456\n",
      "loss: 0.0019027942068919456\n",
      "h_val:  tensor(1.0837, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0018896899058389064\n",
      "loss: 0.0018896899058389064\n",
      "h_val:  tensor(1.0691, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0018834599307194766\n",
      "loss: 0.0018834599307194766\n",
      "h_val:  tensor(1.0654, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0018697770336298884\n",
      "loss: 0.0018697770336298884\n",
      "h_val:  tensor(1.0665, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0018668695712111417\n",
      "loss: 0.0018668695712111417\n",
      "h_val:  tensor(1.0654, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0018657223865101227\n",
      "loss: 0.0018657223865101227\n",
      "h_val:  tensor(1.0647, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0018653491900403206\n",
      "loss: 0.0018653491900403206\n",
      "h_val:  tensor(1.0616, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.00186325696508659\n",
      "loss: 0.00186325696508659\n",
      "h_val:  tensor(1.0575, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.00185967820025032\n",
      "loss: 0.00185967820025032\n",
      "h_val:  tensor(1.0499, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0018537425591029022\n",
      "loss: 0.0018537425591029022\n",
      "h_val:  tensor(1.0372, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0018594725064134986\n",
      "loss: 0.0018594725064134986\n",
      "h_val:  tensor(1.0447, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0018493106525965318\n",
      "loss: 0.0018493106525965318\n",
      "h_val:  tensor(1.0445, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0018428757539375182\n",
      "loss: 0.0018428757539375182\n",
      "h_val:  tensor(1.0451, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0018358637825734146\n",
      "loss: 0.0018358637825734146\n",
      "h_val:  tensor(1.0459, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0018324327728214788\n",
      "loss: 0.0018324327728214788\n",
      "h_val:  tensor(1.0457, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0018225956061715997\n",
      "loss: 0.0018225956061715997\n",
      "h_val:  tensor(1.0439, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001839252954925605\n",
      "loss: 0.001839252954925605\n",
      "h_val:  tensor(1.0451, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0018182544307688488\n",
      "loss: 0.0018182544307688488\n",
      "h_val:  tensor(1.0435, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0018131952025356706\n",
      "loss: 0.0018131952025356706\n",
      "h_val:  tensor(1.0411, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0018097522843899168\n",
      "loss: 0.0018097522843899168\n",
      "h_val:  tensor(1.0396, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001807706491208199\n",
      "loss: 0.001807706491208199\n",
      "h_val:  tensor(1.0374, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0018031256391917245\n",
      "loss: 0.0018031256391917245\n",
      "h_val:  tensor(1.0445, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0018007602350840679\n",
      "loss: 0.0018007602350840679\n",
      "h_val:  tensor(1.0445, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017979223155573797\n",
      "loss: 0.0017979223155573797\n",
      "h_val:  tensor(1.0467, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017972464528839783\n",
      "loss: 0.0017972464528839783\n",
      "h_val:  tensor(1.0480, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017964023412563941\n",
      "loss: 0.0017964023412563941\n",
      "h_val:  tensor(1.0492, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017959503750893035\n",
      "loss: 0.0017959503750893035\n",
      "h_val:  tensor(1.0505, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017951683617391126\n",
      "loss: 0.0017951683617391126\n",
      "h_val:  tensor(1.0509, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001793929436221523\n",
      "loss: 0.001793929436221523\n",
      "h_val:  tensor(1.0501, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.00179566496262149\n",
      "loss: 0.00179566496262149\n",
      "h_val:  tensor(1.0506, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017933505544109412\n",
      "loss: 0.0017933505544109412\n",
      "h_val:  tensor(1.0492, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017922805496410608\n",
      "loss: 0.0017922805496410608\n",
      "h_val:  tensor(1.0473, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017915467146848224\n",
      "loss: 0.0017915467146848224\n",
      "h_val:  tensor(1.0449, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017905867991753175\n",
      "loss: 0.0017905867991753175\n",
      "h_val:  tensor(1.0446, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017895173361179159\n",
      "loss: 0.0017895173361179159\n",
      "h_val:  tensor(1.0448, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017883794410567667\n",
      "loss: 0.0017883794410567667\n",
      "h_val:  tensor(1.0462, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017878589135176476\n",
      "loss: 0.0017878589135176476\n",
      "h_val:  tensor(1.0470, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001787837936022455\n",
      "loss: 0.001787837936022455\n",
      "h_val:  tensor(1.0466, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017877392610264806\n",
      "loss: 0.0017877392610264806\n",
      "h_val:  tensor(1.0471, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017876751842447521\n",
      "loss: 0.0017876751842447521\n",
      "h_val:  tensor(1.0477, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017875231938471589\n",
      "loss: 0.0017875231938471589\n",
      "h_val:  tensor(1.0487, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017870993575077035\n",
      "loss: 0.0017870993575077035\n",
      "h_val:  tensor(1.0499, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017861746407865611\n",
      "loss: 0.0017861746407865611\n",
      "h_val:  tensor(1.0514, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017845387240481847\n",
      "loss: 0.0017845387240481847\n",
      "h_val:  tensor(1.0508, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017854701525108628\n",
      "loss: 0.0017854701525108628\n",
      "h_val:  tensor(1.0511, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017833704741072758\n",
      "loss: 0.0017833704741072758\n",
      "h_val:  tensor(1.0502, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017816501464160213\n",
      "loss: 0.0017816501464160213\n",
      "h_val:  tensor(1.0475, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001780527364021633\n",
      "loss: 0.001780527364021633\n",
      "h_val:  tensor(1.0470, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017804856407945043\n",
      "loss: 0.0017804856407945043\n",
      "h_val:  tensor(1.0468, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017804577706407852\n",
      "loss: 0.0017804577706407852\n",
      "h_val:  tensor(1.0466, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017803985027502343\n",
      "loss: 0.0017803985027502343\n",
      "h_val:  tensor(1.0462, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017803437966729967\n",
      "loss: 0.0017803437966729967\n",
      "h_val:  tensor(1.0461, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001780226685857977\n",
      "loss: 0.001780226685857977\n",
      "h_val:  tensor(1.0461, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017800474253579994\n",
      "loss: 0.0017800474253579994\n",
      "h_val:  tensor(1.0459, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001779844359414272\n",
      "loss: 0.001779844359414272\n",
      "h_val:  tensor(1.0454, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017795492490559656\n",
      "loss: 0.0017795492490559656\n",
      "h_val:  tensor(1.0442, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017791078512813436\n",
      "loss: 0.0017791078512813436\n",
      "h_val:  tensor(1.0422, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017794479512160625\n",
      "loss: 0.0017794479512160625\n",
      "h_val:  tensor(1.0434, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017788717106931619\n",
      "loss: 0.0017788717106931619\n",
      "h_val:  tensor(1.0426, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017785322313348976\n",
      "loss: 0.0017785322313348976\n",
      "h_val:  tensor(1.0418, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017781704776520203\n",
      "loss: 0.0017781704776520203\n",
      "h_val:  tensor(1.0415, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001777783834205515\n",
      "loss: 0.001777783834205515\n",
      "h_val:  tensor(1.0417, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001777258389133231\n",
      "loss: 0.001777258389133231\n",
      "h_val:  tensor(1.0442, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017774270657642266\n",
      "loss: 0.0017774270657642266\n",
      "h_val:  tensor(1.0428, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017769484538299072\n",
      "loss: 0.0017769484538299072\n",
      "h_val:  tensor(1.0439, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017764593138329093\n",
      "loss: 0.0017764593138329093\n",
      "h_val:  tensor(1.0442, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017765570443209276\n",
      "loss: 0.0017765570443209276\n",
      "h_val:  tensor(1.0440, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017761970143834048\n",
      "loss: 0.0017761970143834048\n",
      "h_val:  tensor(1.0454, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001775739987655858\n",
      "loss: 0.001775739987655858\n",
      "h_val:  tensor(1.0467, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017751539660826121\n",
      "loss: 0.0017751539660826121\n",
      "h_val:  tensor(1.0471, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017747469421742963\n",
      "loss: 0.0017747469421742963\n",
      "h_val:  tensor(1.0473, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017746636815552629\n",
      "loss: 0.0017746636815552629\n",
      "h_val:  tensor(1.0467, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017744638463600265\n",
      "loss: 0.0017744638463600265\n",
      "h_val:  tensor(1.0463, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017743997639634622\n",
      "loss: 0.0017743997639634622\n",
      "h_val:  tensor(1.0459, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017742595908651203\n",
      "loss: 0.0017742595908651203\n",
      "h_val:  tensor(1.0455, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017740677855568943\n",
      "loss: 0.0017740677855568943\n",
      "h_val:  tensor(1.0462, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017740343304307345\n",
      "loss: 0.0017740343304307345\n",
      "h_val:  tensor(1.0459, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017738801699489712\n",
      "loss: 0.0017738801699489712\n",
      "h_val:  tensor(1.0460, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017736274235299536\n",
      "loss: 0.0017736274235299536\n",
      "h_val:  tensor(1.0465, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017732090190539407\n",
      "loss: 0.0017732090190539407\n",
      "h_val:  tensor(1.0468, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001772704309207728\n",
      "loss: 0.001772704309207728\n",
      "h_val:  tensor(1.0467, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001771848013637068\n",
      "loss: 0.001771848013637068\n",
      "h_val:  tensor(1.0463, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017713729358406398\n",
      "loss: 0.0017713729358406398\n",
      "h_val:  tensor(1.0455, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017704252313174116\n",
      "loss: 0.0017704252313174116\n",
      "h_val:  tensor(1.0447, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017700884260264255\n",
      "loss: 0.0017700884260264255\n",
      "h_val:  tensor(1.0443, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017699293305987857\n",
      "loss: 0.0017699293305987857\n",
      "h_val:  tensor(1.0444, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017698869808208493\n",
      "loss: 0.0017698869808208493\n",
      "h_val:  tensor(1.0443, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017695761250700744\n",
      "loss: 0.0017695761250700744\n",
      "h_val:  tensor(1.0445, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017694463675726104\n",
      "loss: 0.0017694463675726104\n",
      "h_val:  tensor(1.0453, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017691746008845863\n",
      "loss: 0.0017691746008845863\n",
      "h_val:  tensor(1.0465, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017687883471529287\n",
      "loss: 0.0017687883471529287\n",
      "h_val:  tensor(1.0472, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017688912855014323\n",
      "loss: 0.0017688912855014323\n",
      "h_val:  tensor(1.0468, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017686259104845636\n",
      "loss: 0.0017686259104845636\n",
      "h_val:  tensor(1.0479, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017683857755175\n",
      "loss: 0.0017683857755175\n",
      "h_val:  tensor(1.0479, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017683223352035471\n",
      "loss: 0.0017683223352035471\n",
      "h_val:  tensor(1.0477, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001768247740757113\n",
      "loss: 0.001768247740757113\n",
      "h_val:  tensor(1.0476, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017681936167815687\n",
      "loss: 0.0017681936167815687\n",
      "h_val:  tensor(1.0476, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017681039501998225\n",
      "loss: 0.0017681039501998225\n",
      "h_val:  tensor(1.0474, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017679968534263255\n",
      "loss: 0.0017679968534263255\n",
      "h_val:  tensor(1.0471, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017678472777590605\n",
      "loss: 0.0017678472777590605\n",
      "h_val:  tensor(1.0460, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001768501512420785\n",
      "loss: 0.001768501512420785\n",
      "h_val:  tensor(1.0469, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017678249449069876\n",
      "loss: 0.0017678249449069876\n",
      "h_val:  tensor(1.0468, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017677637253756063\n",
      "loss: 0.0017677637253756063\n",
      "h_val:  tensor(1.0468, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017677152636274528\n",
      "loss: 0.0017677152636274528\n",
      "h_val:  tensor(1.0469, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017676815729337797\n",
      "loss: 0.0017676815729337797\n",
      "h_val:  tensor(1.0469, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001767666808599141\n",
      "loss: 0.001767666808599141\n",
      "h_val:  tensor(1.0469, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017676539092094064\n",
      "loss: 0.0017676539092094064\n",
      "h_val:  tensor(1.0469, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017676416508223608\n",
      "loss: 0.0017676416508223608\n",
      "h_val:  tensor(1.0469, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017676279556515532\n",
      "loss: 0.0017676279556515532\n",
      "h_val:  tensor(1.0467, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017676137594192832\n",
      "loss: 0.0017676137594192832\n",
      "h_val:  tensor(1.0464, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017675915517350192\n",
      "loss: 0.0017675915517350192\n",
      "h_val:  tensor(1.0462, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017675646369882112\n",
      "loss: 0.0017675646369882112\n",
      "h_val:  tensor(1.0460, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.00176752308526541\n",
      "loss: 0.00176752308526541\n",
      "h_val:  tensor(1.0457, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017674092690988673\n",
      "loss: 0.0017674092690988673\n",
      "h_val:  tensor(1.0454, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017673035784417444\n",
      "loss: 0.0017673035784417444\n",
      "h_val:  tensor(1.0451, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017670752591145164\n",
      "loss: 0.0017670752591145164\n",
      "h_val:  tensor(1.0447, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001766539384085567\n",
      "loss: 0.001766539384085567\n",
      "h_val:  tensor(1.0442, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017661975476597769\n",
      "loss: 0.0017661975476597769\n",
      "h_val:  tensor(1.0430, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001765847106201815\n",
      "loss: 0.001765847106201815\n",
      "h_val:  tensor(1.0418, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017664096769863745\n",
      "loss: 0.0017664096769863745\n",
      "h_val:  tensor(1.0427, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017657577915787025\n",
      "loss: 0.0017657577915787025\n",
      "h_val:  tensor(1.0419, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017656165667110754\n",
      "loss: 0.0017656165667110754\n",
      "h_val:  tensor(1.0415, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001765550815885687\n",
      "loss: 0.001765550815885687\n",
      "h_val:  tensor(1.0410, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017654546618039184\n",
      "loss: 0.0017654546618039184\n",
      "h_val:  tensor(1.0400, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017652215435542986\n",
      "loss: 0.0017652215435542986\n",
      "h_val:  tensor(1.0383, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017651744084328843\n",
      "loss: 0.0017651744084328843\n",
      "h_val:  tensor(1.0391, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017649669017554802\n",
      "loss: 0.0017649669017554802\n",
      "h_val:  tensor(1.0381, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017644651194832666\n",
      "loss: 0.0017644651194832666\n",
      "h_val:  tensor(1.0361, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001763371117761061\n",
      "loss: 0.001763371117761061\n",
      "h_val:  tensor(1.0358, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017629751596243493\n",
      "loss: 0.0017629751596243493\n",
      "h_val:  tensor(1.0361, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017626394826712698\n",
      "loss: 0.0017626394826712698\n",
      "h_val:  tensor(1.0364, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017623824405608277\n",
      "loss: 0.0017623824405608277\n",
      "h_val:  tensor(1.0365, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017617560740572366\n",
      "loss: 0.0017617560740572366\n",
      "h_val:  tensor(1.0357, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017612679725861856\n",
      "loss: 0.0017612679725861856\n",
      "h_val:  tensor(1.0345, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017607510903645696\n",
      "loss: 0.0017607510903645696\n",
      "h_val:  tensor(1.0338, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017603124992307117\n",
      "loss: 0.0017603124992307117\n",
      "h_val:  tensor(1.0339, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017597594653781094\n",
      "loss: 0.0017597594653781094\n",
      "h_val:  tensor(1.0346, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017595172021920048\n",
      "loss: 0.0017595172021920048\n",
      "h_val:  tensor(1.0343, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017594356801671222\n",
      "loss: 0.0017594356801671222\n",
      "h_val:  tensor(1.0353, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017588127852801198\n",
      "loss: 0.0017588127852801198\n",
      "h_val:  tensor(1.0356, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017584487775080765\n",
      "loss: 0.0017584487775080765\n",
      "h_val:  tensor(1.0371, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017579395792363586\n",
      "loss: 0.0017579395792363586\n",
      "h_val:  tensor(1.0363, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017575666409871131\n",
      "loss: 0.0017575666409871131\n",
      "h_val:  tensor(1.0362, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001757416615427275\n",
      "loss: 0.001757416615427275\n",
      "h_val:  tensor(1.0361, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017573670347521813\n",
      "loss: 0.0017573670347521813\n",
      "h_val:  tensor(1.0362, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001757335452955954\n",
      "loss: 0.001757335452955954\n",
      "h_val:  tensor(1.0366, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001757323371861107\n",
      "loss: 0.001757323371861107\n",
      "h_val:  tensor(1.0366, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001757284734993763\n",
      "loss: 0.001757284734993763\n",
      "h_val:  tensor(1.0367, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017572721423954856\n",
      "loss: 0.0017572721423954856\n",
      "h_val:  tensor(1.0369, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017572450072243769\n",
      "loss: 0.0017572450072243769\n",
      "h_val:  tensor(1.0372, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001757205601218127\n",
      "loss: 0.001757205601218127\n",
      "h_val:  tensor(1.0372, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001757134544219507\n",
      "loss: 0.001757134544219507\n",
      "h_val:  tensor(1.0374, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017570160732219438\n",
      "loss: 0.0017570160732219438\n",
      "h_val:  tensor(1.0380, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017568683140986765\n",
      "loss: 0.0017568683140986765\n",
      "h_val:  tensor(1.0373, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017565630649754476\n",
      "loss: 0.0017565630649754476\n",
      "h_val:  tensor(1.0361, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017562072334593106\n",
      "loss: 0.0017562072334593106\n",
      "h_val:  tensor(1.0352, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017560059042379525\n",
      "loss: 0.0017560059042379525\n",
      "h_val:  tensor(1.0344, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017560013041912862\n",
      "loss: 0.0017560013041912862\n",
      "h_val:  tensor(1.0348, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017559114088669474\n",
      "loss: 0.0017559114088669474\n",
      "h_val:  tensor(1.0346, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017558115432200242\n",
      "loss: 0.0017558115432200242\n",
      "h_val:  tensor(1.0345, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017556854188898144\n",
      "loss: 0.0017556854188898144\n",
      "h_val:  tensor(1.0342, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017556430508713294\n",
      "loss: 0.0017556430508713294\n",
      "h_val:  tensor(1.0343, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017555660355114107\n",
      "loss: 0.0017555660355114107\n",
      "h_val:  tensor(1.0343, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001755553424465521\n",
      "loss: 0.001755553424465521\n",
      "h_val:  tensor(1.0342, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017555380503144226\n",
      "loss: 0.0017555380503144226\n",
      "h_val:  tensor(1.0341, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017555157612073686\n",
      "loss: 0.0017555157612073686\n",
      "h_val:  tensor(1.0336, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017554710172675543\n",
      "loss: 0.0017554710172675543\n",
      "h_val:  tensor(1.0335, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017553879062214076\n",
      "loss: 0.0017553879062214076\n",
      "h_val:  tensor(1.0334, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017552083496673945\n",
      "loss: 0.0017552083496673945\n",
      "h_val:  tensor(1.0336, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017549305342388095\n",
      "loss: 0.0017549305342388095\n",
      "h_val:  tensor(1.0340, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.00175458147472351\n",
      "loss: 0.00175458147472351\n",
      "h_val:  tensor(1.0354, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.00175416700198701\n",
      "loss: 0.00175416700198701\n",
      "h_val:  tensor(1.0358, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017535339820738528\n",
      "loss: 0.0017535339820738528\n",
      "h_val:  tensor(1.0368, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017528709540332397\n",
      "loss: 0.0017528709540332397\n",
      "h_val:  tensor(1.0373, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017524945595541563\n",
      "loss: 0.0017524945595541563\n",
      "h_val:  tensor(1.0371, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017515351855259787\n",
      "loss: 0.0017515351855259787\n",
      "h_val:  tensor(1.0356, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001752589274255765\n",
      "loss: 0.001752589274255765\n",
      "h_val:  tensor(1.0366, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017512494866155233\n",
      "loss: 0.0017512494866155233\n",
      "h_val:  tensor(1.0357, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017507776795252276\n",
      "loss: 0.0017507776795252276\n",
      "h_val:  tensor(1.0356, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017506469886612427\n",
      "loss: 0.0017506469886612427\n",
      "h_val:  tensor(1.0353, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017505754023154755\n",
      "loss: 0.0017505754023154755\n",
      "h_val:  tensor(1.0350, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017504930917743428\n",
      "loss: 0.0017504930917743428\n",
      "h_val:  tensor(1.0347, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017503086801698006\n",
      "loss: 0.0017503086801698006\n",
      "h_val:  tensor(1.0346, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017499708900475767\n",
      "loss: 0.0017499708900475767\n",
      "h_val:  tensor(1.0347, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017496312221715522\n",
      "loss: 0.0017496312221715522\n",
      "h_val:  tensor(1.0351, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001749187024408593\n",
      "loss: 0.001749187024408593\n",
      "h_val:  tensor(1.0358, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017489523348810754\n",
      "loss: 0.0017489523348810754\n",
      "h_val:  tensor(1.0365, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001748770227797076\n",
      "loss: 0.001748770227797076\n",
      "h_val:  tensor(1.0367, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.00174866277434807\n",
      "loss: 0.00174866277434807\n",
      "h_val:  tensor(1.0370, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017484630921497956\n",
      "loss: 0.0017484630921497956\n",
      "h_val:  tensor(1.0368, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017482292529429728\n",
      "loss: 0.0017482292529429728\n",
      "h_val:  tensor(1.0363, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.00174806040847591\n",
      "loss: 0.00174806040847591\n",
      "h_val:  tensor(1.0354, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017478611858178707\n",
      "loss: 0.0017478611858178707\n",
      "h_val:  tensor(1.0350, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001747736152649788\n",
      "loss: 0.001747736152649788\n",
      "h_val:  tensor(1.0347, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017475623225376537\n",
      "loss: 0.0017475623225376537\n",
      "h_val:  tensor(1.0341, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017471866037212247\n",
      "loss: 0.0017471866037212247\n",
      "h_val:  tensor(1.0338, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017470220989527097\n",
      "loss: 0.0017470220989527097\n",
      "h_val:  tensor(1.0332, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017467429668929471\n",
      "loss: 0.0017467429668929471\n",
      "h_val:  tensor(1.0324, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017463972456381175\n",
      "loss: 0.0017463972456381175\n",
      "h_val:  tensor(1.0317, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017458597854375932\n",
      "loss: 0.0017458597854375932\n",
      "h_val:  tensor(1.0294, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001743973946115582\n",
      "loss: 0.001743973946115582\n",
      "h_val:  tensor(1.0279, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017419311613546143\n",
      "loss: 0.0017419311613546143\n",
      "h_val:  tensor(1.0275, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001741545197305581\n",
      "loss: 0.001741545197305581\n",
      "h_val:  tensor(1.0280, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017375307772105888\n",
      "loss: 0.0017375307772105888\n",
      "h_val:  tensor(1.0292, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001736212900906151\n",
      "loss: 0.001736212900906151\n",
      "h_val:  tensor(1.0309, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017349582090631054\n",
      "loss: 0.0017349582090631054\n",
      "h_val:  tensor(1.0321, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.00173324664473277\n",
      "loss: 0.00173324664473277\n",
      "h_val:  tensor(1.0353, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017294649652877493\n",
      "loss: 0.0017294649652877493\n",
      "h_val:  tensor(1.0334, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017350088674581868\n",
      "loss: 0.0017350088674581868\n",
      "h_val:  tensor(1.0346, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017278699576905687\n",
      "loss: 0.0017278699576905687\n",
      "h_val:  tensor(1.0335, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017267885308823155\n",
      "loss: 0.0017267885308823155\n",
      "h_val:  tensor(1.0321, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017263100155408737\n",
      "loss: 0.0017263100155408737\n",
      "h_val:  tensor(1.0317, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017263095025462616\n",
      "loss: 0.0017263095025462616\n",
      "h_val:  tensor(1.0319, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017262224259786377\n",
      "loss: 0.0017262224259786377\n",
      "h_val:  tensor(1.0318, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017261204985384564\n",
      "loss: 0.0017261204985384564\n",
      "h_val:  tensor(1.0312, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001725460989837368\n",
      "loss: 0.001725460989837368\n",
      "h_val:  tensor(1.0310, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001725047471355327\n",
      "loss: 0.001725047471355327\n",
      "h_val:  tensor(1.0313, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017243830047970878\n",
      "loss: 0.0017243830047970878\n",
      "h_val:  tensor(1.0326, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017235575823945346\n",
      "loss: 0.0017235575823945346\n",
      "h_val:  tensor(1.0335, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017230174507617965\n",
      "loss: 0.0017230174507617965\n",
      "h_val:  tensor(1.0340, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017228237101335492\n",
      "loss: 0.0017228237101335492\n",
      "h_val:  tensor(1.0339, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001722771711692613\n",
      "loss: 0.001722771711692613\n",
      "h_val:  tensor(1.0338, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017226793314397827\n",
      "loss: 0.0017226793314397827\n",
      "h_val:  tensor(1.0335, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017224527880158535\n",
      "loss: 0.0017224527880158535\n",
      "h_val:  tensor(1.0331, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017221861514364512\n",
      "loss: 0.0017221861514364512\n",
      "h_val:  tensor(1.0326, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001721842385621486\n",
      "loss: 0.001721842385621486\n",
      "h_val:  tensor(1.0324, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001721448839909164\n",
      "loss: 0.001721448839909164\n",
      "h_val:  tensor(1.0310, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017234008019416222\n",
      "loss: 0.0017234008019416222\n",
      "h_val:  tensor(1.0321, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017212983805168663\n",
      "loss: 0.0017212983805168663\n",
      "h_val:  tensor(1.0322, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017209844989614855\n",
      "loss: 0.0017209844989614855\n",
      "h_val:  tensor(1.0321, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017206390052619453\n",
      "loss: 0.0017206390052619453\n",
      "h_val:  tensor(1.0312, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017200791521873944\n",
      "loss: 0.0017200791521873944\n",
      "h_val:  tensor(1.0303, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001719886232723516\n",
      "loss: 0.001719886232723516\n",
      "h_val:  tensor(1.0300, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001719587748313133\n",
      "loss: 0.001719587748313133\n",
      "h_val:  tensor(1.0297, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017194844474555863\n",
      "loss: 0.0017194844474555863\n",
      "h_val:  tensor(1.0295, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017193412871825675\n",
      "loss: 0.0017193412871825675\n",
      "h_val:  tensor(1.0292, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017189926795812744\n",
      "loss: 0.0017189926795812744\n",
      "h_val:  tensor(1.0291, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017183054360142332\n",
      "loss: 0.0017183054360142332\n",
      "h_val:  tensor(1.0296, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017170864972614723\n",
      "loss: 0.0017170864972614723\n",
      "h_val:  tensor(1.0342, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017191030632761272\n",
      "loss: 0.0017191030632761272\n",
      "h_val:  tensor(1.0312, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017162273140594797\n",
      "loss: 0.0017162273140594797\n",
      "h_val:  tensor(1.0329, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017146667440192664\n",
      "loss: 0.0017146667440192664\n",
      "h_val:  tensor(1.0346, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017139447879359747\n",
      "loss: 0.0017139447879359747\n",
      "h_val:  tensor(1.0350, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001713868167105075\n",
      "loss: 0.001713868167105075\n",
      "h_val:  tensor(1.0352, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001713781958530654\n",
      "loss: 0.001713781958530654\n",
      "h_val:  tensor(1.0357, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017136200458407688\n",
      "loss: 0.0017136200458407688\n",
      "h_val:  tensor(1.0365, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017133353280695705\n",
      "loss: 0.0017133353280695705\n",
      "h_val:  tensor(1.0374, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017130423803141948\n",
      "loss: 0.0017130423803141948\n",
      "h_val:  tensor(1.0377, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017128452935822826\n",
      "loss: 0.0017128452935822826\n",
      "h_val:  tensor(1.0377, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017126810144442867\n",
      "loss: 0.0017126810144442867\n",
      "h_val:  tensor(1.0377, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017124787706742017\n",
      "loss: 0.0017124787706742017\n",
      "h_val:  tensor(1.0377, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017120167829822364\n",
      "loss: 0.0017120167829822364\n",
      "h_val:  tensor(1.0386, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017118211715564367\n",
      "loss: 0.0017118211715564367\n",
      "h_val:  tensor(1.0388, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017114338751116768\n",
      "loss: 0.0017114338751116768\n",
      "h_val:  tensor(1.0389, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017112927433158903\n",
      "loss: 0.0017112927433158903\n",
      "h_val:  tensor(1.0390, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017110977820133585\n",
      "loss: 0.0017110977820133585\n",
      "h_val:  tensor(1.0389, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017108806164300788\n",
      "loss: 0.0017108806164300788\n",
      "h_val:  tensor(1.0389, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017107885489206914\n",
      "loss: 0.0017107885489206914\n",
      "h_val:  tensor(1.0388, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017107000947592153\n",
      "loss: 0.0017107000947592153\n",
      "h_val:  tensor(1.0387, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017106795679911354\n",
      "loss: 0.0017106795679911354\n",
      "h_val:  tensor(1.0388, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017106482413520615\n",
      "loss: 0.0017106482413520615\n",
      "h_val:  tensor(1.0391, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017105991305144102\n",
      "loss: 0.0017105991305144102\n",
      "h_val:  tensor(1.0396, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001710565762294609\n",
      "loss: 0.001710565762294609\n",
      "h_val:  tensor(1.0399, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017105383631184756\n",
      "loss: 0.0017105383631184756\n",
      "h_val:  tensor(1.0400, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017105187580551116\n",
      "loss: 0.0017105187580551116\n",
      "h_val:  tensor(1.0402, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017105016741644147\n",
      "loss: 0.0017105016741644147\n",
      "h_val:  tensor(1.0403, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017104845199334222\n",
      "loss: 0.0017104845199334222\n",
      "h_val:  tensor(1.0404, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017104706254383174\n",
      "loss: 0.0017104706254383174\n",
      "h_val:  tensor(1.0405, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017104554504020944\n",
      "loss: 0.0017104554504020944\n",
      "h_val:  tensor(1.0407, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017104475322268075\n",
      "loss: 0.0017104475322268075\n",
      "h_val:  tensor(1.0408, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001710433344552022\n",
      "loss: 0.001710433344552022\n",
      "h_val:  tensor(1.0409, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017104153887197967\n",
      "loss: 0.0017104153887197967\n",
      "h_val:  tensor(1.0410, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017103765925197217\n",
      "loss: 0.0017103765925197217\n",
      "h_val:  tensor(1.0411, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017103337104484736\n",
      "loss: 0.0017103337104484736\n",
      "h_val:  tensor(1.0408, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017102649249383936\n",
      "loss: 0.0017102649249383936\n",
      "h_val:  tensor(1.0407, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.001710221686834271\n",
      "loss: 0.001710221686834271\n",
      "h_val:  tensor(1.0405, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017101931792545754\n",
      "loss: 0.0017101931792545754\n",
      "h_val:  tensor(1.0403, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017101877118562742\n",
      "loss: 0.0017101877118562742\n",
      "h_val:  tensor(1.0402, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017101845701812171\n",
      "loss: 0.0017101845701812171\n",
      "h_val:  tensor(1.0400, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017101796636866558\n",
      "loss: 0.0017101796636866558\n",
      "h_val:  tensor(1.0398, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.0017101779413234174\n",
      "loss: 0.0017101779413234174\n",
      "h_new:  1.0397741390645412\n",
      "[[0.78957985 0.50867994]\n",
      " [0.43437849 0.31355367]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "x = np.random.normal(0, 1, 100) \n",
    "#x = np.clip(x, -1.5, 1.5)\n",
    "\n",
    "np.random.seed(24)\n",
    "l = np.random.normal(0, 0.01, 100) \n",
    "\n",
    "\n",
    "y = [np.sin(x) + l for x, l in zip(x, l)]\n",
    "#z = [np.log(np.abs(x)) + b for x, b in zip(x, b)]\n",
    "\n",
    "X = np.column_stack((x, y))\n",
    "n = X.shape[0]\n",
    "d = X.shape[1]\n",
    "\n",
    "print(n)\n",
    "print(d)\n",
    "\n",
    "model = NotearsRKHS(n, d, \"gaussian\")\n",
    "output = learning(model, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.array(x)\n",
    "x_hat = output[:, 0].detach().numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 6))  # Optional: specifies the figure size\n",
    "\n",
    "\n",
    "# Plot predicted values\n",
    "plt.scatter(x, y, label='Predicted Values', color='red', marker='s')  # Plot x vs. y_hat\n",
    "\n",
    "# Plot actual values\n",
    "plt.scatter(x_hat, y, label='Actual Values', color='blue', marker='o')  # Plot x vs. y\n",
    "\n",
    "plt.legend()  # Show legend to differentiate between actual and predicted values\n",
    "plt.title('Actual vs Predicted Values')  # Optional: Adds a title to the plot\n",
    "plt.xlabel('X-axis label')  # Optional: Label for the x-axis\n",
    "plt.ylabel('Y-axis label')  # Optional: Label for the y-axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_val:  tensor(0., grad_fn=<SubBackward0>)\n",
      "L_risk: 23.660786211446553\n",
      "loss: 23.660786211446553\n",
      "h_val:  tensor(0.0506, grad_fn=<SubBackward0>)\n",
      "L_risk: 20.670070344408717\n",
      "loss: 20.670070344408717\n",
      "h_val:  tensor(8.0254, grad_fn=<SubBackward0>)\n",
      "L_risk: 4.289068977586932\n",
      "loss: 4.289068977586932\n",
      "h_val:  tensor(14.2022, grad_fn=<SubBackward0>)\n",
      "L_risk: 2.6536057295833113\n",
      "loss: 2.6536057295833113\n",
      "h_val:  tensor(8.2513, grad_fn=<SubBackward0>)\n",
      "L_risk: 1.071245924228759\n",
      "loss: 1.071245924228759\n",
      "h_val:  tensor(7.3832, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.8260319039509655\n",
      "loss: 0.8260319039509655\n",
      "h_val:  tensor(7.4425, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.6751066902340511\n",
      "loss: 0.6751066902340511\n",
      "h_val:  tensor(7.9027, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.5358229988749932\n",
      "loss: 0.5358229988749932\n",
      "h_val:  tensor(7.8754, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.4364262919648422\n",
      "loss: 0.4364262919648422\n",
      "h_val:  tensor(8.2584, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.3915303908796848\n",
      "loss: 0.3915303908796848\n",
      "h_val:  tensor(7.8006, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.36812214664600107\n",
      "loss: 0.36812214664600107\n",
      "h_val:  tensor(7.0898, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.32345263201518765\n",
      "loss: 0.32345263201518765\n",
      "h_val:  tensor(6.7075, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.27125954661457796\n",
      "loss: 0.27125954661457796\n",
      "h_val:  tensor(6.8528, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.2186118414883071\n",
      "loss: 0.2186118414883071\n",
      "h_val:  tensor(8.2093, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.22558014760110295\n",
      "loss: 0.22558014760110295\n",
      "h_val:  tensor(7.2605, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1966641877243997\n",
      "loss: 0.1966641877243997\n",
      "h_val:  tensor(7.6594, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.18112186840590122\n",
      "loss: 0.18112186840590122\n",
      "h_val:  tensor(7.8057, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1785126537506721\n",
      "loss: 0.1785126537506721\n",
      "h_val:  tensor(7.7861, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.17440885192698619\n",
      "loss: 0.17440885192698619\n",
      "h_val:  tensor(7.7145, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.17306066590559804\n",
      "loss: 0.17306066590559804\n",
      "h_val:  tensor(7.5890, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.17138148534065722\n",
      "loss: 0.17138148534065722\n",
      "h_val:  tensor(7.4498, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.16997587045965937\n",
      "loss: 0.16997587045965937\n",
      "h_val:  tensor(7.3336, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.16997500229643275\n",
      "loss: 0.16997500229643275\n",
      "h_val:  tensor(7.3850, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.169198785332211\n",
      "loss: 0.169198785332211\n",
      "h_val:  tensor(7.2853, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1681058359704531\n",
      "loss: 0.1681058359704531\n",
      "h_val:  tensor(7.2238, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1673524773817067\n",
      "loss: 0.1673524773817067\n",
      "h_val:  tensor(7.1061, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.16608149364374591\n",
      "loss: 0.16608149364374591\n",
      "h_val:  tensor(6.7809, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1643844043472164\n",
      "loss: 0.1643844043472164\n",
      "h_val:  tensor(6.3210, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1632903029924933\n",
      "loss: 0.1632903029924933\n",
      "h_val:  tensor(6.2309, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.16152218068054291\n",
      "loss: 0.16152218068054291\n",
      "h_val:  tensor(6.1300, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.16092444541220374\n",
      "loss: 0.16092444541220374\n",
      "h_val:  tensor(5.9848, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.16040699205110615\n",
      "loss: 0.16040699205110615\n",
      "h_val:  tensor(5.7083, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.15969607143416636\n",
      "loss: 0.15969607143416636\n",
      "h_val:  tensor(5.5187, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.15893252599923366\n",
      "loss: 0.15893252599923366\n",
      "h_val:  tensor(5.4396, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.15819660999017982\n",
      "loss: 0.15819660999017982\n",
      "h_val:  tensor(5.3165, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.15735025743545558\n",
      "loss: 0.15735025743545558\n",
      "h_val:  tensor(5.0804, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.15662410460122486\n",
      "loss: 0.15662410460122486\n",
      "h_val:  tensor(4.9394, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1557860994848719\n",
      "loss: 0.1557860994848719\n",
      "h_val:  tensor(4.8818, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.15535619251472246\n",
      "loss: 0.15535619251472246\n",
      "h_val:  tensor(4.8243, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.15507462757831053\n",
      "loss: 0.15507462757831053\n",
      "h_val:  tensor(4.6973, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.15450990647571403\n",
      "loss: 0.15450990647571403\n",
      "h_val:  tensor(4.5369, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.15387162031620255\n",
      "loss: 0.15387162031620255\n",
      "h_val:  tensor(4.3976, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.15345914389886287\n",
      "loss: 0.15345914389886287\n",
      "h_val:  tensor(4.3765, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.15317556575593702\n",
      "loss: 0.15317556575593702\n",
      "h_val:  tensor(4.3105, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1527414743480948\n",
      "loss: 0.1527414743480948\n",
      "h_val:  tensor(4.1913, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.15223413431052493\n",
      "loss: 0.15223413431052493\n",
      "h_val:  tensor(3.9447, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.15138616764914786\n",
      "loss: 0.15138616764914786\n",
      "h_val:  tensor(3.6070, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1505026903334215\n",
      "loss: 0.1505026903334215\n",
      "h_val:  tensor(3.4339, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.14996782024071129\n",
      "loss: 0.14996782024071129\n",
      "h_val:  tensor(3.4160, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.14965897291863345\n",
      "loss: 0.14965897291863345\n",
      "h_val:  tensor(3.4208, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1494990921210143\n",
      "loss: 0.1494990921210143\n",
      "h_val:  tensor(3.3763, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.14900504554470062\n",
      "loss: 0.14900504554470062\n",
      "h_val:  tensor(3.2605, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1483205158471313\n",
      "loss: 0.1483205158471313\n",
      "h_val:  tensor(3.0779, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.14758911188842777\n",
      "loss: 0.14758911188842777\n",
      "h_val:  tensor(3.0031, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.14696917620331573\n",
      "loss: 0.14696917620331573\n",
      "h_val:  tensor(2.8123, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.14605800289765478\n",
      "loss: 0.14605800289765478\n",
      "h_val:  tensor(2.7446, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1453980715798664\n",
      "loss: 0.1453980715798664\n",
      "h_val:  tensor(2.7136, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1449400399029153\n",
      "loss: 0.1449400399029153\n",
      "h_val:  tensor(2.6324, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.14432715726122372\n",
      "loss: 0.14432715726122372\n",
      "h_val:  tensor(2.6167, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.14467803374505708\n",
      "loss: 0.14467803374505708\n",
      "h_val:  tensor(2.6267, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.14423322381574172\n",
      "loss: 0.14423322381574172\n",
      "h_val:  tensor(2.6195, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1441264016169166\n",
      "loss: 0.1441264016169166\n",
      "h_val:  tensor(2.5973, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.14383088590523443\n",
      "loss: 0.14383088590523443\n",
      "h_val:  tensor(2.5683, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.14372912408275287\n",
      "loss: 0.14372912408275287\n",
      "h_val:  tensor(2.5653, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.14357274317521268\n",
      "loss: 0.14357274317521268\n",
      "h_val:  tensor(2.5483, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.14340582354147888\n",
      "loss: 0.14340582354147888\n",
      "h_val:  tensor(2.5192, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1432586978030648\n",
      "loss: 0.1432586978030648\n",
      "h_val:  tensor(2.4679, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.14297964582363762\n",
      "loss: 0.14297964582363762\n",
      "h_val:  tensor(2.4177, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.14285819242372874\n",
      "loss: 0.14285819242372874\n",
      "h_val:  tensor(2.3915, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.14260995096390702\n",
      "loss: 0.14260995096390702\n",
      "h_val:  tensor(2.4047, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1425362431310048\n",
      "loss: 0.1425362431310048\n",
      "h_val:  tensor(2.4103, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.14240958687756441\n",
      "loss: 0.14240958687756441\n",
      "h_val:  tensor(2.4006, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.14220402437237564\n",
      "loss: 0.14220402437237564\n",
      "h_val:  tensor(2.3720, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1419140140835527\n",
      "loss: 0.1419140140835527\n",
      "h_val:  tensor(2.3314, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1415072935132725\n",
      "loss: 0.1415072935132725\n",
      "h_val:  tensor(2.2420, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.14079461423813303\n",
      "loss: 0.14079461423813303\n",
      "h_val:  tensor(2.1774, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1404062058760106\n",
      "loss: 0.1404062058760106\n",
      "h_val:  tensor(2.1494, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1399174568516244\n",
      "loss: 0.1399174568516244\n",
      "h_val:  tensor(2.1192, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13935098754000175\n",
      "loss: 0.13935098754000175\n",
      "h_val:  tensor(2.1029, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1389438708032592\n",
      "loss: 0.1389438708032592\n",
      "h_val:  tensor(2.0852, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13862656625833772\n",
      "loss: 0.13862656625833772\n",
      "h_val:  tensor(2.0799, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1383065435516637\n",
      "loss: 0.1383065435516637\n",
      "h_val:  tensor(2.0814, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13814716310347128\n",
      "loss: 0.13814716310347128\n",
      "h_val:  tensor(2.0777, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1378864938636763\n",
      "loss: 0.1378864938636763\n",
      "h_val:  tensor(2.0669, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.137544494153151\n",
      "loss: 0.137544494153151\n",
      "h_val:  tensor(2.0491, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.136965464786707\n",
      "loss: 0.136965464786707\n",
      "h_val:  tensor(2.0453, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13650990149975376\n",
      "loss: 0.13650990149975376\n",
      "h_val:  tensor(2.0369, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1360310489728737\n",
      "loss: 0.1360310489728737\n",
      "h_val:  tensor(2.0352, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13551768527275285\n",
      "loss: 0.13551768527275285\n",
      "h_val:  tensor(2.0355, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13514210178864108\n",
      "loss: 0.13514210178864108\n",
      "h_val:  tensor(2.0475, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13502672843194582\n",
      "loss: 0.13502672843194582\n",
      "h_val:  tensor(2.0455, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1347123707605535\n",
      "loss: 0.1347123707605535\n",
      "h_val:  tensor(2.0414, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1346608998203695\n",
      "loss: 0.1346608998203695\n",
      "h_val:  tensor(2.0338, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1345736691881682\n",
      "loss: 0.1345736691881682\n",
      "h_val:  tensor(2.0216, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13446803457516038\n",
      "loss: 0.13446803457516038\n",
      "h_val:  tensor(2.0147, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13436474943962576\n",
      "loss: 0.13436474943962576\n",
      "h_val:  tensor(2.0089, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13425602970316847\n",
      "loss: 0.13425602970316847\n",
      "h_val:  tensor(2.0026, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13414766326668706\n",
      "loss: 0.13414766326668706\n",
      "h_val:  tensor(1.9883, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1342070751500363\n",
      "loss: 0.1342070751500363\n",
      "h_val:  tensor(1.9948, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13402218433471755\n",
      "loss: 0.13402218433471755\n",
      "h_val:  tensor(1.9886, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13386940649840387\n",
      "loss: 0.13386940649840387\n",
      "h_val:  tensor(1.9822, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13382219031522016\n",
      "loss: 0.13382219031522016\n",
      "h_val:  tensor(1.9817, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13379938285573084\n",
      "loss: 0.13379938285573084\n",
      "h_val:  tensor(1.9790, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13377169627871915\n",
      "loss: 0.13377169627871915\n",
      "h_val:  tensor(1.9695, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1337179796376402\n",
      "loss: 0.1337179796376402\n",
      "h_val:  tensor(1.9513, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13369511925001423\n",
      "loss: 0.13369511925001423\n",
      "h_val:  tensor(1.9482, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13363479986578342\n",
      "loss: 0.13363479986578342\n",
      "h_val:  tensor(1.9450, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13361426123277712\n",
      "loss: 0.13361426123277712\n",
      "h_val:  tensor(1.9384, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13359137927067782\n",
      "loss: 0.13359137927067782\n",
      "h_val:  tensor(1.9265, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1335665577170127\n",
      "loss: 0.1335665577170127\n",
      "h_val:  tensor(1.9201, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1335383127607842\n",
      "loss: 0.1335383127607842\n",
      "h_val:  tensor(1.9152, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.133513757836467\n",
      "loss: 0.133513757836467\n",
      "h_val:  tensor(1.9103, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13348758030767294\n",
      "loss: 0.13348758030767294\n",
      "h_val:  tensor(1.9032, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1334723402658189\n",
      "loss: 0.1334723402658189\n",
      "h_val:  tensor(1.9028, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1334510974858853\n",
      "loss: 0.1334510974858853\n",
      "h_val:  tensor(1.9029, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1334426852601738\n",
      "loss: 0.1334426852601738\n",
      "h_val:  tensor(1.8984, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1334109752739615\n",
      "loss: 0.1334109752739615\n",
      "h_val:  tensor(1.8882, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13339076178698367\n",
      "loss: 0.13339076178698367\n",
      "h_val:  tensor(1.8824, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13334701186670764\n",
      "loss: 0.13334701186670764\n",
      "h_val:  tensor(1.8772, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1333221891767825\n",
      "loss: 0.1333221891767825\n",
      "h_val:  tensor(1.8653, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13328461541063077\n",
      "loss: 0.13328461541063077\n",
      "h_val:  tensor(1.8500, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13327465002421895\n",
      "loss: 0.13327465002421895\n",
      "h_val:  tensor(1.8447, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1332162631927106\n",
      "loss: 0.1332162631927106\n",
      "h_val:  tensor(1.8418, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13319698760383414\n",
      "loss: 0.13319698760383414\n",
      "h_val:  tensor(1.8364, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13317607774700607\n",
      "loss: 0.13317607774700607\n",
      "h_val:  tensor(1.8246, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13315382907065418\n",
      "loss: 0.13315382907065418\n",
      "h_val:  tensor(1.8206, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13312192222115268\n",
      "loss: 0.13312192222115268\n",
      "h_val:  tensor(1.8091, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13307478018033117\n",
      "loss: 0.13307478018033117\n",
      "h_val:  tensor(1.8013, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1330533270455216\n",
      "loss: 0.1330533270455216\n",
      "h_val:  tensor(1.7834, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13303062538717195\n",
      "loss: 0.13303062538717195\n",
      "h_val:  tensor(1.7796, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1329954783293855\n",
      "loss: 0.1329954783293855\n",
      "h_val:  tensor(1.7737, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13296848698003738\n",
      "loss: 0.13296848698003738\n",
      "h_val:  tensor(1.7656, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1329490421614674\n",
      "loss: 0.1329490421614674\n",
      "h_val:  tensor(1.7569, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1329395336160841\n",
      "loss: 0.1329395336160841\n",
      "h_val:  tensor(1.7508, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13291559108460044\n",
      "loss: 0.13291559108460044\n",
      "h_val:  tensor(1.7478, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13290534795411982\n",
      "loss: 0.13290534795411982\n",
      "h_val:  tensor(1.7447, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13289626087723308\n",
      "loss: 0.13289626087723308\n",
      "h_val:  tensor(1.7412, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13287949386863293\n",
      "loss: 0.13287949386863293\n",
      "h_val:  tensor(1.7356, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13285398355014041\n",
      "loss: 0.13285398355014041\n",
      "h_val:  tensor(1.7339, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13283092888019327\n",
      "loss: 0.13283092888019327\n",
      "h_val:  tensor(1.7344, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13280989181323613\n",
      "loss: 0.13280989181323613\n",
      "h_val:  tensor(1.7338, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1327853287133431\n",
      "loss: 0.1327853287133431\n",
      "h_val:  tensor(1.7271, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13277897881141892\n",
      "loss: 0.13277897881141892\n",
      "h_val:  tensor(1.7270, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1327620434392863\n",
      "loss: 0.1327620434392863\n",
      "h_val:  tensor(1.7250, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1327570546724624\n",
      "loss: 0.1327570546724624\n",
      "h_val:  tensor(1.7215, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13275009522677209\n",
      "loss: 0.13275009522677209\n",
      "h_val:  tensor(1.7149, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1327330799371635\n",
      "loss: 0.1327330799371635\n",
      "h_val:  tensor(1.7010, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13270125213519068\n",
      "loss: 0.13270125213519068\n",
      "h_val:  tensor(1.6803, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1326821631962675\n",
      "loss: 0.1326821631962675\n",
      "h_val:  tensor(1.6829, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13264613485528814\n",
      "loss: 0.13264613485528814\n",
      "h_val:  tensor(1.6829, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13262272325120453\n",
      "loss: 0.13262272325120453\n",
      "h_val:  tensor(1.6801, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.132607695025959\n",
      "loss: 0.132607695025959\n",
      "h_val:  tensor(1.6703, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1325989533103174\n",
      "loss: 0.1325989533103174\n",
      "h_val:  tensor(1.6678, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1325725922234721\n",
      "loss: 0.1325725922234721\n",
      "h_val:  tensor(1.6656, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1325632304512315\n",
      "loss: 0.1325632304512315\n",
      "h_val:  tensor(1.6626, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13255368679392449\n",
      "loss: 0.13255368679392449\n",
      "h_val:  tensor(1.6602, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1325400755701028\n",
      "loss: 0.1325400755701028\n",
      "h_val:  tensor(1.6571, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1325498564035486\n",
      "loss: 0.1325498564035486\n",
      "h_val:  tensor(1.6588, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13252803775277258\n",
      "loss: 0.13252803775277258\n",
      "h_val:  tensor(1.6577, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1325104738668747\n",
      "loss: 0.1325104738668747\n",
      "h_val:  tensor(1.6582, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13249760918898623\n",
      "loss: 0.13249760918898623\n",
      "h_val:  tensor(1.6579, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13247629444767736\n",
      "loss: 0.13247629444767736\n",
      "h_val:  tensor(1.6586, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.132462827375426\n",
      "loss: 0.132462827375426\n",
      "h_val:  tensor(1.6550, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13243608939630377\n",
      "loss: 0.13243608939630377\n",
      "h_val:  tensor(1.6516, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1324215391730067\n",
      "loss: 0.1324215391730067\n",
      "h_val:  tensor(1.6476, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1324069348300228\n",
      "loss: 0.1324069348300228\n",
      "h_val:  tensor(1.6426, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1323912560199973\n",
      "loss: 0.1323912560199973\n",
      "h_val:  tensor(1.6379, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1323779835392348\n",
      "loss: 0.1323779835392348\n",
      "h_val:  tensor(1.6385, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13236957254111134\n",
      "loss: 0.13236957254111134\n",
      "h_val:  tensor(1.6369, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13235194564440256\n",
      "loss: 0.13235194564440256\n",
      "h_val:  tensor(1.6325, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13233261446098604\n",
      "loss: 0.13233261446098604\n",
      "h_val:  tensor(1.6193, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1323068932149602\n",
      "loss: 0.1323068932149602\n",
      "h_val:  tensor(1.6105, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13228582653395082\n",
      "loss: 0.13228582653395082\n",
      "h_val:  tensor(1.6031, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13227098600183299\n",
      "loss: 0.13227098600183299\n",
      "h_val:  tensor(1.5974, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13225773379338462\n",
      "loss: 0.13225773379338462\n",
      "h_val:  tensor(1.5870, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13224809224107062\n",
      "loss: 0.13224809224107062\n",
      "h_val:  tensor(1.5854, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13223811359985027\n",
      "loss: 0.13223811359985027\n",
      "h_val:  tensor(1.5865, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13223437440366434\n",
      "loss: 0.13223437440366434\n",
      "h_val:  tensor(1.5865, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13221963062192982\n",
      "loss: 0.13221963062192982\n",
      "h_val:  tensor(1.5782, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1322200062298388\n",
      "loss: 0.1322200062298388\n",
      "h_val:  tensor(1.5824, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1322107112135757\n",
      "loss: 0.1322107112135757\n",
      "h_val:  tensor(1.5778, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13219367335783644\n",
      "loss: 0.13219367335783644\n",
      "h_val:  tensor(1.5666, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1321693536304267\n",
      "loss: 0.1321693536304267\n",
      "h_val:  tensor(1.5561, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13214970835164794\n",
      "loss: 0.13214970835164794\n",
      "h_val:  tensor(1.5450, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13213652139281107\n",
      "loss: 0.13213652139281107\n",
      "h_val:  tensor(1.5421, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1321243818121446\n",
      "loss: 0.1321243818121446\n",
      "h_val:  tensor(1.5435, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13211488082689524\n",
      "loss: 0.13211488082689524\n",
      "h_val:  tensor(1.5432, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13209996262825263\n",
      "loss: 0.13209996262825263\n",
      "h_val:  tensor(1.5393, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13208929095912786\n",
      "loss: 0.13208929095912786\n",
      "h_val:  tensor(1.5315, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13207536011978804\n",
      "loss: 0.13207536011978804\n",
      "h_val:  tensor(1.5227, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13206022259430034\n",
      "loss: 0.13206022259430034\n",
      "h_val:  tensor(1.5088, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1320460426665528\n",
      "loss: 0.1320460426665528\n",
      "h_val:  tensor(1.5018, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13202919760307363\n",
      "loss: 0.13202919760307363\n",
      "h_val:  tensor(1.5000, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1320184407017199\n",
      "loss: 0.1320184407017199\n",
      "h_val:  tensor(1.4936, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1319995408111923\n",
      "loss: 0.1319995408111923\n",
      "h_val:  tensor(1.4851, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1319889171351906\n",
      "loss: 0.1319889171351906\n",
      "h_val:  tensor(1.4797, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13197096978984943\n",
      "loss: 0.13197096978984943\n",
      "h_val:  tensor(1.4766, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13196186910442914\n",
      "loss: 0.13196186910442914\n",
      "h_val:  tensor(1.4731, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13195302598747874\n",
      "loss: 0.13195302598747874\n",
      "h_val:  tensor(1.4650, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.131947380340981\n",
      "loss: 0.131947380340981\n",
      "h_val:  tensor(1.4646, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13193657460915623\n",
      "loss: 0.13193657460915623\n",
      "h_val:  tensor(1.4619, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13192183560457982\n",
      "loss: 0.13192183560457982\n",
      "h_val:  tensor(1.4576, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1319080245440445\n",
      "loss: 0.1319080245440445\n",
      "h_val:  tensor(1.4479, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1318831082033556\n",
      "loss: 0.1318831082033556\n",
      "h_val:  tensor(1.4254, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13188914996109113\n",
      "loss: 0.13188914996109113\n",
      "h_val:  tensor(1.4373, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1318657637694076\n",
      "loss: 0.1318657637694076\n",
      "h_val:  tensor(1.4316, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13185134287403746\n",
      "loss: 0.13185134287403746\n",
      "h_val:  tensor(1.4285, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1318444655310958\n",
      "loss: 0.1318444655310958\n",
      "h_val:  tensor(1.4253, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13183654652453622\n",
      "loss: 0.13183654652453622\n",
      "h_val:  tensor(1.4173, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1318276067955299\n",
      "loss: 0.1318276067955299\n",
      "h_val:  tensor(1.4147, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13181878974713312\n",
      "loss: 0.13181878974713312\n",
      "h_val:  tensor(1.4121, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1318101569700147\n",
      "loss: 0.1318101569700147\n",
      "h_val:  tensor(1.4065, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13179896948220668\n",
      "loss: 0.13179896948220668\n",
      "h_val:  tensor(1.3886, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13178215609413574\n",
      "loss: 0.13178215609413574\n",
      "h_val:  tensor(1.3827, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1317549742412268\n",
      "loss: 0.1317549742412268\n",
      "h_val:  tensor(1.3696, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13172593720892573\n",
      "loss: 0.13172593720892573\n",
      "h_val:  tensor(1.3574, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13170450031050188\n",
      "loss: 0.13170450031050188\n",
      "h_val:  tensor(1.3345, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1316669243475954\n",
      "loss: 0.1316669243475954\n",
      "h_val:  tensor(1.3076, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13172909763235355\n",
      "loss: 0.13172909763235355\n",
      "h_val:  tensor(1.3268, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13165580091882967\n",
      "loss: 0.13165580091882967\n",
      "h_val:  tensor(1.3241, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13164799121983822\n",
      "loss: 0.13164799121983822\n",
      "h_val:  tensor(1.3237, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13164421929143455\n",
      "loss: 0.13164421929143455\n",
      "h_val:  tensor(1.3220, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13163900469849657\n",
      "loss: 0.13163900469849657\n",
      "h_val:  tensor(1.3179, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13163306356716567\n",
      "loss: 0.13163306356716567\n",
      "h_val:  tensor(1.3140, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13162562277710055\n",
      "loss: 0.13162562277710055\n",
      "h_val:  tensor(1.3107, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13162003974545852\n",
      "loss: 0.13162003974545852\n",
      "h_val:  tensor(1.3067, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13161332565489833\n",
      "loss: 0.13161332565489833\n",
      "h_val:  tensor(1.3022, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13161178171213242\n",
      "loss: 0.13161178171213242\n",
      "h_val:  tensor(1.3033, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1316056855876964\n",
      "loss: 0.1316056855876964\n",
      "h_val:  tensor(1.3038, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13160320871432443\n",
      "loss: 0.13160320871432443\n",
      "h_val:  tensor(1.3036, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13160116499348068\n",
      "loss: 0.13160116499348068\n",
      "h_val:  tensor(1.3023, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13159772519608456\n",
      "loss: 0.13159772519608456\n",
      "h_val:  tensor(1.2970, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13159248478130894\n",
      "loss: 0.13159248478130894\n",
      "h_val:  tensor(1.2947, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13158413503297606\n",
      "loss: 0.13158413503297606\n",
      "h_val:  tensor(1.2917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13157678263653624\n",
      "loss: 0.13157678263653624\n",
      "h_val:  tensor(1.2866, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13156772799012725\n",
      "loss: 0.13156772799012725\n",
      "h_val:  tensor(1.2790, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13155734886382922\n",
      "loss: 0.13155734886382922\n",
      "h_val:  tensor(1.2674, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13154829041735033\n",
      "loss: 0.13154829041735033\n",
      "h_val:  tensor(1.2676, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13153983885329795\n",
      "loss: 0.13153983885329795\n",
      "h_val:  tensor(1.2672, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13153518355176191\n",
      "loss: 0.13153518355176191\n",
      "h_val:  tensor(1.2651, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13153039379326587\n",
      "loss: 0.13153039379326587\n",
      "h_val:  tensor(1.2596, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1315226541910093\n",
      "loss: 0.1315226541910093\n",
      "h_val:  tensor(1.2557, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13151597183523822\n",
      "loss: 0.13151597183523822\n",
      "h_val:  tensor(1.2521, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13150916459178957\n",
      "loss: 0.13150916459178957\n",
      "h_val:  tensor(1.2502, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13150219003836178\n",
      "loss: 0.13150219003836178\n",
      "h_val:  tensor(1.2490, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13149320150249272\n",
      "loss: 0.13149320150249272\n",
      "h_val:  tensor(1.2474, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1314804040391842\n",
      "loss: 0.1314804040391842\n",
      "h_val:  tensor(1.2447, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.131467733645112\n",
      "loss: 0.131467733645112\n",
      "h_val:  tensor(1.2429, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13145850698088743\n",
      "loss: 0.13145850698088743\n",
      "h_val:  tensor(1.2373, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13144266510053929\n",
      "loss: 0.13144266510053929\n",
      "h_val:  tensor(1.2353, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1314391623684845\n",
      "loss: 0.1314391623684845\n",
      "h_val:  tensor(1.2320, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13143360304393611\n",
      "loss: 0.13143360304393611\n",
      "h_val:  tensor(1.2306, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13143024164541625\n",
      "loss: 0.13143024164541625\n",
      "h_val:  tensor(1.2284, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1314253201807708\n",
      "loss: 0.1314253201807708\n",
      "h_val:  tensor(1.2258, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13141950533415148\n",
      "loss: 0.13141950533415148\n",
      "h_val:  tensor(1.2228, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13141270747615533\n",
      "loss: 0.13141270747615533\n",
      "h_val:  tensor(1.2214, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13140798867448222\n",
      "loss: 0.13140798867448222\n",
      "h_val:  tensor(1.2195, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13140575533358406\n",
      "loss: 0.13140575533358406\n",
      "h_val:  tensor(1.2190, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1314023751766587\n",
      "loss: 0.1314023751766587\n",
      "h_val:  tensor(1.2182, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1314010711167098\n",
      "loss: 0.1314010711167098\n",
      "h_val:  tensor(1.2163, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13139885365232531\n",
      "loss: 0.13139885365232531\n",
      "h_val:  tensor(1.2152, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1313969904823479\n",
      "loss: 0.1313969904823479\n",
      "h_val:  tensor(1.2146, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1313972110342688\n",
      "loss: 0.1313972110342688\n",
      "h_val:  tensor(1.2149, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13139605141096566\n",
      "loss: 0.13139605141096566\n",
      "h_val:  tensor(1.2147, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13139519622047155\n",
      "loss: 0.13139519622047155\n",
      "h_val:  tensor(1.2145, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13139431036725846\n",
      "loss: 0.13139431036725846\n",
      "h_val:  tensor(1.2139, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13139266799601385\n",
      "loss: 0.13139266799601385\n",
      "h_val:  tensor(1.2127, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13139041912027044\n",
      "loss: 0.13139041912027044\n",
      "h_val:  tensor(1.2115, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13138742327062614\n",
      "loss: 0.13138742327062614\n",
      "h_val:  tensor(1.2095, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13138397477918856\n",
      "loss: 0.13138397477918856\n",
      "h_val:  tensor(1.2095, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1313806715751021\n",
      "loss: 0.1313806715751021\n",
      "h_val:  tensor(1.2098, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13137765683517735\n",
      "loss: 0.13137765683517735\n",
      "h_val:  tensor(1.2100, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1313763972383891\n",
      "loss: 0.1313763972383891\n",
      "h_val:  tensor(1.2096, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13137383510624134\n",
      "loss: 0.13137383510624134\n",
      "h_val:  tensor(1.2089, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13137211045736907\n",
      "loss: 0.13137211045736907\n",
      "h_val:  tensor(1.2079, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1313699435274145\n",
      "loss: 0.1313699435274145\n",
      "h_val:  tensor(1.2056, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13136609146667333\n",
      "loss: 0.13136609146667333\n",
      "h_val:  tensor(1.2025, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13136168118976657\n",
      "loss: 0.13136168118976657\n",
      "h_val:  tensor(1.2005, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13135589909232637\n",
      "loss: 0.13135589909232637\n",
      "h_val:  tensor(1.1995, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1313519105027045\n",
      "loss: 0.1313519105027045\n",
      "h_val:  tensor(1.1989, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13134968415707518\n",
      "loss: 0.13134968415707518\n",
      "h_val:  tensor(1.1981, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13134767308323006\n",
      "loss: 0.13134767308323006\n",
      "h_val:  tensor(1.1976, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13134601799744058\n",
      "loss: 0.13134601799744058\n",
      "h_val:  tensor(1.1959, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13134296457588546\n",
      "loss: 0.13134296457588546\n",
      "h_val:  tensor(1.1943, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1313412704587878\n",
      "loss: 0.1313412704587878\n",
      "h_val:  tensor(1.1939, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13133948387372302\n",
      "loss: 0.13133948387372302\n",
      "h_val:  tensor(1.1927, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13133703463603286\n",
      "loss: 0.13133703463603286\n",
      "h_val:  tensor(1.1914, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13133501549393078\n",
      "loss: 0.13133501549393078\n",
      "h_val:  tensor(1.1883, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1313291660667749\n",
      "loss: 0.1313291660667749\n",
      "h_val:  tensor(1.1856, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1313256659690018\n",
      "loss: 0.1313256659690018\n",
      "h_val:  tensor(1.1859, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1313226729796444\n",
      "loss: 0.1313226729796444\n",
      "h_val:  tensor(1.1861, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13132100317741618\n",
      "loss: 0.13132100317741618\n",
      "h_val:  tensor(1.1857, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13131907221926536\n",
      "loss: 0.13131907221926536\n",
      "h_val:  tensor(1.1828, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13131774764961818\n",
      "loss: 0.13131774764961818\n",
      "h_val:  tensor(1.1828, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13131485295005538\n",
      "loss: 0.13131485295005538\n",
      "h_val:  tensor(1.1823, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13131382136316028\n",
      "loss: 0.13131382136316028\n",
      "h_val:  tensor(1.1808, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13131219021097978\n",
      "loss: 0.13131219021097978\n",
      "h_val:  tensor(1.1784, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1313097229572668\n",
      "loss: 0.1313097229572668\n",
      "h_val:  tensor(1.1762, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1313065919400647\n",
      "loss: 0.1313065919400647\n",
      "h_val:  tensor(1.1726, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1313020323170322\n",
      "loss: 0.1313020323170322\n",
      "h_val:  tensor(1.1704, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13130220443531643\n",
      "loss: 0.13130220443531643\n",
      "h_val:  tensor(1.1715, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13130110653958255\n",
      "loss: 0.13130110653958255\n",
      "h_val:  tensor(1.1711, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13130008588010555\n",
      "loss: 0.13130008588010555\n",
      "h_val:  tensor(1.1706, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1312989891921841\n",
      "loss: 0.1312989891921841\n",
      "h_val:  tensor(1.1701, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1312985182682102\n",
      "loss: 0.1312985182682102\n",
      "h_val:  tensor(1.1699, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1312979519663193\n",
      "loss: 0.1312979519663193\n",
      "h_val:  tensor(1.1697, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13129751612139376\n",
      "loss: 0.13129751612139376\n",
      "h_val:  tensor(1.1693, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1312968298958711\n",
      "loss: 0.1312968298958711\n",
      "h_val:  tensor(1.1688, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1312955824452962\n",
      "loss: 0.1312955824452962\n",
      "h_val:  tensor(1.1680, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13129348075179462\n",
      "loss: 0.13129348075179462\n",
      "h_val:  tensor(1.1658, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13129179321726975\n",
      "loss: 0.13129179321726975\n",
      "h_val:  tensor(1.1659, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13128967543122863\n",
      "loss: 0.13128967543122863\n",
      "h_val:  tensor(1.1662, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13128867578789627\n",
      "loss: 0.13128867578789627\n",
      "h_val:  tensor(1.1660, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1312877338503897\n",
      "loss: 0.1312877338503897\n",
      "h_val:  tensor(1.1661, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13128665359133782\n",
      "loss: 0.13128665359133782\n",
      "h_val:  tensor(1.1655, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13128514412919562\n",
      "loss: 0.13128514412919562\n",
      "h_val:  tensor(1.1652, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13128452128478443\n",
      "loss: 0.13128452128478443\n",
      "h_val:  tensor(1.1651, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13128403870543434\n",
      "loss: 0.13128403870543434\n",
      "h_val:  tensor(1.1649, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13128325092525936\n",
      "loss: 0.13128325092525936\n",
      "h_val:  tensor(1.1650, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13128208575810943\n",
      "loss: 0.13128208575810943\n",
      "h_val:  tensor(1.1657, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13127832627019614\n",
      "loss: 0.13127832627019614\n",
      "h_val:  tensor(1.1670, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13128030049253372\n",
      "loss: 0.13128030049253372\n",
      "h_val:  tensor(1.1662, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13127668592350805\n",
      "loss: 0.13127668592350805\n",
      "h_val:  tensor(1.1668, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13127455034830882\n",
      "loss: 0.13127455034830882\n",
      "h_val:  tensor(1.1673, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13127362282805263\n",
      "loss: 0.13127362282805263\n",
      "h_val:  tensor(1.1673, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13127308772312185\n",
      "loss: 0.13127308772312185\n",
      "h_val:  tensor(1.1672, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13127280666071386\n",
      "loss: 0.13127280666071386\n",
      "h_val:  tensor(1.1671, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13127225816866436\n",
      "loss: 0.13127225816866436\n",
      "h_val:  tensor(1.1670, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13127179404507142\n",
      "loss: 0.13127179404507142\n",
      "h_val:  tensor(1.1670, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13127121338107173\n",
      "loss: 0.13127121338107173\n",
      "h_val:  tensor(1.1668, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1312704426449522\n",
      "loss: 0.1312704426449522\n",
      "h_val:  tensor(1.1666, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13126982402226048\n",
      "loss: 0.13126982402226048\n",
      "h_val:  tensor(1.1664, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13126901558139198\n",
      "loss: 0.13126901558139198\n",
      "h_val:  tensor(1.1659, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13126820670707745\n",
      "loss: 0.13126820670707745\n",
      "h_val:  tensor(1.1660, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13126731591674662\n",
      "loss: 0.13126731591674662\n",
      "h_val:  tensor(1.1660, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13126688220509128\n",
      "loss: 0.13126688220509128\n",
      "h_val:  tensor(1.1658, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1312663440938155\n",
      "loss: 0.1312663440938155\n",
      "h_val:  tensor(1.1658, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1312659981582013\n",
      "loss: 0.1312659981582013\n",
      "h_val:  tensor(1.1656, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13126498684878682\n",
      "loss: 0.13126498684878682\n",
      "h_val:  tensor(1.1654, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13126440666440156\n",
      "loss: 0.13126440666440156\n",
      "h_val:  tensor(1.1655, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13126380970029394\n",
      "loss: 0.13126380970029394\n",
      "h_val:  tensor(1.1658, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13126280976737278\n",
      "loss: 0.13126280976737278\n",
      "h_val:  tensor(1.1676, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13126322811876964\n",
      "loss: 0.13126322811876964\n",
      "h_val:  tensor(1.1666, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13126192923202992\n",
      "loss: 0.13126192923202992\n",
      "h_val:  tensor(1.1673, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13126051289011964\n",
      "loss: 0.13126051289011964\n",
      "h_val:  tensor(1.1677, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1312596959300272\n",
      "loss: 0.1312596959300272\n",
      "h_val:  tensor(1.1681, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13125863851086006\n",
      "loss: 0.13125863851086006\n",
      "h_val:  tensor(1.1684, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13125838516132063\n",
      "loss: 0.13125838516132063\n",
      "h_val:  tensor(1.1681, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13125744538230283\n",
      "loss: 0.13125744538230283\n",
      "h_val:  tensor(1.1680, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13125701357542663\n",
      "loss: 0.13125701357542663\n",
      "h_val:  tensor(1.1678, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13125655292203622\n",
      "loss: 0.13125655292203622\n",
      "h_val:  tensor(1.1678, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13125576273789766\n",
      "loss: 0.13125576273789766\n",
      "h_val:  tensor(1.1681, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13125761787670517\n",
      "loss: 0.13125761787670517\n",
      "h_val:  tensor(1.1679, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13125526648919864\n",
      "loss: 0.13125526648919864\n",
      "h_val:  tensor(1.1681, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13125432857592245\n",
      "loss: 0.13125432857592245\n",
      "h_val:  tensor(1.1683, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13125353390451328\n",
      "loss: 0.13125353390451328\n",
      "h_val:  tensor(1.1686, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13125216404568968\n",
      "loss: 0.13125216404568968\n",
      "h_val:  tensor(1.1685, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13125027254973734\n",
      "loss: 0.13125027254973734\n",
      "h_val:  tensor(1.1688, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13124865860947119\n",
      "loss: 0.13124865860947119\n",
      "h_val:  tensor(1.1685, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13124691814661865\n",
      "loss: 0.13124691814661865\n",
      "h_val:  tensor(1.1682, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1312460939977505\n",
      "loss: 0.1312460939977505\n",
      "h_val:  tensor(1.1681, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13124548099086833\n",
      "loss: 0.13124548099086833\n",
      "h_val:  tensor(1.1685, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1312450384079303\n",
      "loss: 0.1312450384079303\n",
      "h_val:  tensor(1.1683, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13124347969359826\n",
      "loss: 0.13124347969359826\n",
      "h_val:  tensor(1.1684, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13124287652946845\n",
      "loss: 0.13124287652946845\n",
      "h_val:  tensor(1.1686, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13124208724979694\n",
      "loss: 0.13124208724979694\n",
      "h_val:  tensor(1.1688, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13124088539415457\n",
      "loss: 0.13124088539415457\n",
      "h_val:  tensor(1.1697, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13123736499057284\n",
      "loss: 0.13123736499057284\n",
      "h_val:  tensor(1.1700, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1312432251227207\n",
      "loss: 0.1312432251227207\n",
      "h_val:  tensor(1.1697, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13123584317599843\n",
      "loss: 0.13123584317599843\n",
      "h_val:  tensor(1.1706, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13123290377444313\n",
      "loss: 0.13123290377444313\n",
      "h_val:  tensor(1.1711, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13123080602631446\n",
      "loss: 0.13123080602631446\n",
      "h_val:  tensor(1.1712, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1312300490110952\n",
      "loss: 0.1312300490110952\n",
      "h_val:  tensor(1.1712, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13122843364102973\n",
      "loss: 0.13122843364102973\n",
      "h_val:  tensor(1.1710, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13122729709291767\n",
      "loss: 0.13122729709291767\n",
      "h_val:  tensor(1.1707, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1312253314107604\n",
      "loss: 0.1312253314107604\n",
      "h_val:  tensor(1.1701, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13122192870224256\n",
      "loss: 0.13122192870224256\n",
      "h_val:  tensor(1.1670, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13121692903905333\n",
      "loss: 0.13121692903905333\n",
      "h_val:  tensor(1.1658, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13120897942553136\n",
      "loss: 0.13120897942553136\n",
      "h_val:  tensor(1.1646, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13120363557634124\n",
      "loss: 0.13120363557634124\n",
      "h_val:  tensor(1.1633, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13119975766391315\n",
      "loss: 0.13119975766391315\n",
      "h_val:  tensor(1.1615, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13120190752707273\n",
      "loss: 0.13120190752707273\n",
      "h_val:  tensor(1.1627, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13119877150649512\n",
      "loss: 0.13119877150649512\n",
      "h_val:  tensor(1.1623, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13119737818173205\n",
      "loss: 0.13119737818173205\n",
      "h_val:  tensor(1.1619, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13119564367075062\n",
      "loss: 0.13119564367075062\n",
      "h_val:  tensor(1.1616, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311942792688214\n",
      "loss: 0.1311942792688214\n",
      "h_val:  tensor(1.1616, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13119209130559892\n",
      "loss: 0.13119209130559892\n",
      "h_val:  tensor(1.1611, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13118932664383195\n",
      "loss: 0.13118932664383195\n",
      "h_val:  tensor(1.1609, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13118656147859284\n",
      "loss: 0.13118656147859284\n",
      "h_val:  tensor(1.1614, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13118529194970815\n",
      "loss: 0.13118529194970815\n",
      "h_val:  tensor(1.1613, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311842240945394\n",
      "loss: 0.1311842240945394\n",
      "h_val:  tensor(1.1613, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13118392437822815\n",
      "loss: 0.13118392437822815\n",
      "h_val:  tensor(1.1617, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13118320885843002\n",
      "loss: 0.13118320885843002\n",
      "h_val:  tensor(1.1621, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13118275636711585\n",
      "loss: 0.13118275636711585\n",
      "h_val:  tensor(1.1632, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13118312145418098\n",
      "loss: 0.13118312145418098\n",
      "h_val:  tensor(1.1625, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311825560739953\n",
      "loss: 0.1311825560739953\n",
      "h_val:  tensor(1.1627, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13118234423189593\n",
      "loss: 0.13118234423189593\n",
      "h_val:  tensor(1.1629, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13118196786239777\n",
      "loss: 0.13118196786239777\n",
      "h_val:  tensor(1.1632, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311813294295195\n",
      "loss: 0.1311813294295195\n",
      "h_val:  tensor(1.1634, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13118045033111697\n",
      "loss: 0.13118045033111697\n",
      "h_val:  tensor(1.1637, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13117903857546695\n",
      "loss: 0.13117903857546695\n",
      "h_val:  tensor(1.1641, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13117791881809682\n",
      "loss: 0.13117791881809682\n",
      "h_val:  tensor(1.1644, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311767968082036\n",
      "loss: 0.1311767968082036\n",
      "h_val:  tensor(1.1655, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13117551882309217\n",
      "loss: 0.13117551882309217\n",
      "h_val:  tensor(1.1656, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13117472854774054\n",
      "loss: 0.13117472854774054\n",
      "h_val:  tensor(1.1661, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311733160716083\n",
      "loss: 0.1311733160716083\n",
      "h_val:  tensor(1.1667, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311724160268747\n",
      "loss: 0.1311724160268747\n",
      "h_val:  tensor(1.1670, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13117106283817082\n",
      "loss: 0.13117106283817082\n",
      "h_val:  tensor(1.1681, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13116756473991462\n",
      "loss: 0.13116756473991462\n",
      "h_val:  tensor(1.1683, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13116627081331378\n",
      "loss: 0.13116627081331378\n",
      "h_val:  tensor(1.1684, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13116484482258187\n",
      "loss: 0.13116484482258187\n",
      "h_val:  tensor(1.1686, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13116310012531338\n",
      "loss: 0.13116310012531338\n",
      "h_val:  tensor(1.1685, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13116184168865025\n",
      "loss: 0.13116184168865025\n",
      "h_val:  tensor(1.1687, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13116107128471863\n",
      "loss: 0.13116107128471863\n",
      "h_val:  tensor(1.1695, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13115978371575063\n",
      "loss: 0.13115978371575063\n",
      "h_val:  tensor(1.1699, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13115889193668928\n",
      "loss: 0.13115889193668928\n",
      "h_val:  tensor(1.1710, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13115806218698373\n",
      "loss: 0.13115806218698373\n",
      "h_val:  tensor(1.1713, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13115761029339468\n",
      "loss: 0.13115761029339468\n",
      "h_val:  tensor(1.1718, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13115699262332944\n",
      "loss: 0.13115699262332944\n",
      "h_val:  tensor(1.1730, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311565791647349\n",
      "loss: 0.1311565791647349\n",
      "h_val:  tensor(1.1731, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13115602364782494\n",
      "loss: 0.13115602364782494\n",
      "h_val:  tensor(1.1734, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13115560049269956\n",
      "loss: 0.13115560049269956\n",
      "h_val:  tensor(1.1737, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311552933215274\n",
      "loss: 0.1311552933215274\n",
      "h_val:  tensor(1.1748, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.131154342793379\n",
      "loss: 0.131154342793379\n",
      "h_val:  tensor(1.1756, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311533670803489\n",
      "loss: 0.1311533670803489\n",
      "h_val:  tensor(1.1757, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311525136408479\n",
      "loss: 0.1311525136408479\n",
      "h_val:  tensor(1.1760, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311510807196546\n",
      "loss: 0.1311510807196546\n",
      "h_val:  tensor(1.1760, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13115015339072278\n",
      "loss: 0.13115015339072278\n",
      "h_val:  tensor(1.1762, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13114941931805146\n",
      "loss: 0.13114941931805146\n",
      "h_val:  tensor(1.1766, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13114832839486834\n",
      "loss: 0.13114832839486834\n",
      "h_val:  tensor(1.1773, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311480716961962\n",
      "loss: 0.1311480716961962\n",
      "h_val:  tensor(1.1771, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13114737254403835\n",
      "loss: 0.13114737254403835\n",
      "h_val:  tensor(1.1770, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13114685461002978\n",
      "loss: 0.13114685461002978\n",
      "h_val:  tensor(1.1771, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13114606393644967\n",
      "loss: 0.13114606393644967\n",
      "h_val:  tensor(1.1771, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13114535643984696\n",
      "loss: 0.13114535643984696\n",
      "h_val:  tensor(1.1775, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311457316861729\n",
      "loss: 0.1311457316861729\n",
      "h_val:  tensor(1.1772, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13114505225647918\n",
      "loss: 0.13114505225647918\n",
      "h_val:  tensor(1.1772, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13114463660538747\n",
      "loss: 0.13114463660538747\n",
      "h_val:  tensor(1.1772, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311443260457254\n",
      "loss: 0.1311443260457254\n",
      "h_val:  tensor(1.1771, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311438011766964\n",
      "loss: 0.1311438011766964\n",
      "h_val:  tensor(1.1769, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13114298345977637\n",
      "loss: 0.13114298345977637\n",
      "h_val:  tensor(1.1774, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.131142037090134\n",
      "loss: 0.131142037090134\n",
      "h_val:  tensor(1.1775, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311412563312956\n",
      "loss: 0.1311412563312956\n",
      "h_val:  tensor(1.1779, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13114066306152128\n",
      "loss: 0.13114066306152128\n",
      "h_val:  tensor(1.1782, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13114029525855772\n",
      "loss: 0.13114029525855772\n",
      "h_val:  tensor(1.1790, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311395923003876\n",
      "loss: 0.1311395923003876\n",
      "h_val:  tensor(1.1806, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13113835630413706\n",
      "loss: 0.13113835630413706\n",
      "h_val:  tensor(1.1820, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13113904110575986\n",
      "loss: 0.13113904110575986\n",
      "h_val:  tensor(1.1811, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13113788704932833\n",
      "loss: 0.13113788704932833\n",
      "h_val:  tensor(1.1817, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13113735176415414\n",
      "loss: 0.13113735176415414\n",
      "h_val:  tensor(1.1818, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13113694288775393\n",
      "loss: 0.13113694288775393\n",
      "h_val:  tensor(1.1821, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13113640695997014\n",
      "loss: 0.13113640695997014\n",
      "h_val:  tensor(1.1820, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13113587926844422\n",
      "loss: 0.13113587926844422\n",
      "h_val:  tensor(1.1821, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311353210461268\n",
      "loss: 0.1311353210461268\n",
      "h_val:  tensor(1.1821, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13113461912664615\n",
      "loss: 0.13113461912664615\n",
      "h_val:  tensor(1.1822, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13113396385812925\n",
      "loss: 0.13113396385812925\n",
      "h_val:  tensor(1.1822, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13113310414459486\n",
      "loss: 0.13113310414459486\n",
      "h_val:  tensor(1.1821, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.131131914328319\n",
      "loss: 0.131131914328319\n",
      "h_val:  tensor(1.1815, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311309537188554\n",
      "loss: 0.1311309537188554\n",
      "h_val:  tensor(1.1813, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13113011622706033\n",
      "loss: 0.13113011622706033\n",
      "h_val:  tensor(1.1805, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112897397749912\n",
      "loss: 0.13112897397749912\n",
      "h_val:  tensor(1.1801, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112872700911254\n",
      "loss: 0.13112872700911254\n",
      "h_val:  tensor(1.1798, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112840411204693\n",
      "loss: 0.13112840411204693\n",
      "h_val:  tensor(1.1799, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311283211613809\n",
      "loss: 0.1311283211613809\n",
      "h_val:  tensor(1.1799, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112817230110707\n",
      "loss: 0.13112817230110707\n",
      "h_val:  tensor(1.1801, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112796629464538\n",
      "loss: 0.13112796629464538\n",
      "h_val:  tensor(1.1801, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112784507006806\n",
      "loss: 0.13112784507006806\n",
      "h_val:  tensor(1.1801, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112765513523633\n",
      "loss: 0.13112765513523633\n",
      "h_val:  tensor(1.1801, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112750522581465\n",
      "loss: 0.13112750522581465\n",
      "h_val:  tensor(1.1801, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112727441458177\n",
      "loss: 0.13112727441458177\n",
      "h_val:  tensor(1.1801, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311268765398182\n",
      "loss: 0.1311268765398182\n",
      "h_val:  tensor(1.1799, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112745428786965\n",
      "loss: 0.13112745428786965\n",
      "h_val:  tensor(1.1800, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112667854080637\n",
      "loss: 0.13112667854080637\n",
      "h_val:  tensor(1.1802, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311262515152616\n",
      "loss: 0.1311262515152616\n",
      "h_val:  tensor(1.1803, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112597904645454\n",
      "loss: 0.13112597904645454\n",
      "h_val:  tensor(1.1803, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311258417789539\n",
      "loss: 0.1311258417789539\n",
      "h_val:  tensor(1.1802, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112570677041352\n",
      "loss: 0.13112570677041352\n",
      "h_val:  tensor(1.1801, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112548355670253\n",
      "loss: 0.13112548355670253\n",
      "h_val:  tensor(1.1795, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112540127954572\n",
      "loss: 0.13112540127954572\n",
      "h_val:  tensor(1.1793, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112500202270183\n",
      "loss: 0.13112500202270183\n",
      "h_val:  tensor(1.1791, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112486960400663\n",
      "loss: 0.13112486960400663\n",
      "h_val:  tensor(1.1786, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112467700330033\n",
      "loss: 0.13112467700330033\n",
      "h_val:  tensor(1.1779, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311243464624225\n",
      "loss: 0.1311243464624225\n",
      "h_val:  tensor(1.1764, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112423775302812\n",
      "loss: 0.13112423775302812\n",
      "h_val:  tensor(1.1770, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112398663465963\n",
      "loss: 0.13112398663465963\n",
      "h_val:  tensor(1.1772, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311239202122599\n",
      "loss: 0.1311239202122599\n",
      "h_val:  tensor(1.1772, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112380999327716\n",
      "loss: 0.13112380999327716\n",
      "h_val:  tensor(1.1772, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112365166026277\n",
      "loss: 0.13112365166026277\n",
      "h_val:  tensor(1.1768, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311239987485751\n",
      "loss: 0.1311239987485751\n",
      "h_val:  tensor(1.1771, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311236027989223\n",
      "loss: 0.1311236027989223\n",
      "h_val:  tensor(1.1770, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112350314854793\n",
      "loss: 0.13112350314854793\n",
      "h_val:  tensor(1.1770, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112342412795552\n",
      "loss: 0.13112342412795552\n",
      "h_val:  tensor(1.1772, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112330463382493\n",
      "loss: 0.13112330463382493\n",
      "h_val:  tensor(1.1773, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311232156560546\n",
      "loss: 0.1311232156560546\n",
      "h_val:  tensor(1.1774, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112315221958865\n",
      "loss: 0.13112315221958865\n",
      "h_val:  tensor(1.1776, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311230889372733\n",
      "loss: 0.1311230889372733\n",
      "h_val:  tensor(1.1777, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112304068437722\n",
      "loss: 0.13112304068437722\n",
      "h_val:  tensor(1.1779, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311229414311356\n",
      "loss: 0.1311229414311356\n",
      "h_val:  tensor(1.1779, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112287759969407\n",
      "loss: 0.13112287759969407\n",
      "h_val:  tensor(1.1778, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112279935099794\n",
      "loss: 0.13112279935099794\n",
      "h_val:  tensor(1.1776, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112269394465803\n",
      "loss: 0.13112269394465803\n",
      "h_val:  tensor(1.1775, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311225767691044\n",
      "loss: 0.1311225767691044\n",
      "h_val:  tensor(1.1775, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112236281734277\n",
      "loss: 0.13112236281734277\n",
      "h_val:  tensor(1.1772, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112238247637534\n",
      "loss: 0.13112238247637534\n",
      "h_val:  tensor(1.1773, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112221181980305\n",
      "loss: 0.13112221181980305\n",
      "h_val:  tensor(1.1775, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311220610478377\n",
      "loss: 0.1311220610478377\n",
      "h_val:  tensor(1.1777, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112198400646596\n",
      "loss: 0.13112198400646596\n",
      "h_val:  tensor(1.1778, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112194144137065\n",
      "loss: 0.13112194144137065\n",
      "h_val:  tensor(1.1778, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112189858660286\n",
      "loss: 0.13112189858660286\n",
      "h_val:  tensor(1.1778, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311218309038779\n",
      "loss: 0.1311218309038779\n",
      "h_val:  tensor(1.1778, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112179704470847\n",
      "loss: 0.13112179704470847\n",
      "h_val:  tensor(1.1780, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112176822051883\n",
      "loss: 0.13112176822051883\n",
      "h_val:  tensor(1.1780, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311216820252223\n",
      "loss: 0.1311216820252223\n",
      "h_val:  tensor(1.1780, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112164391054176\n",
      "loss: 0.13112164391054176\n",
      "h_val:  tensor(1.1782, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112158349391084\n",
      "loss: 0.13112158349391084\n",
      "h_val:  tensor(1.1784, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311214832048817\n",
      "loss: 0.1311214832048817\n",
      "h_val:  tensor(1.1789, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112134498543931\n",
      "loss: 0.13112134498543931\n",
      "h_val:  tensor(1.1794, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112122182177455\n",
      "loss: 0.13112122182177455\n",
      "h_val:  tensor(1.1793, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311211597620383\n",
      "loss: 0.1311211597620383\n",
      "h_val:  tensor(1.1794, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112110632309407\n",
      "loss: 0.13112110632309407\n",
      "h_val:  tensor(1.1795, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311210506942988\n",
      "loss: 0.1311210506942988\n",
      "h_val:  tensor(1.1796, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112096765743683\n",
      "loss: 0.13112096765743683\n",
      "h_val:  tensor(1.1799, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311208252267134\n",
      "loss: 0.1311208252267134\n",
      "h_val:  tensor(1.1802, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112070199925943\n",
      "loss: 0.13112070199925943\n",
      "h_val:  tensor(1.1806, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311204707943891\n",
      "loss: 0.1311204707943891\n",
      "h_val:  tensor(1.1815, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112023432170225\n",
      "loss: 0.13112023432170225\n",
      "h_val:  tensor(1.1818, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13112004345174674\n",
      "loss: 0.13112004345174674\n",
      "h_val:  tensor(1.1817, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311199710918894\n",
      "loss: 0.1311199710918894\n",
      "h_val:  tensor(1.1816, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311199364015011\n",
      "loss: 0.1311199364015011\n",
      "h_val:  tensor(1.1816, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111989637998295\n",
      "loss: 0.13111989637998295\n",
      "h_val:  tensor(1.1816, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111978196891266\n",
      "loss: 0.13111978196891266\n",
      "h_val:  tensor(1.1822, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311201171023596\n",
      "loss: 0.1311201171023596\n",
      "h_val:  tensor(1.1818, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311197265480564\n",
      "loss: 0.1311197265480564\n",
      "h_val:  tensor(1.1819, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111959781212237\n",
      "loss: 0.13111959781212237\n",
      "h_val:  tensor(1.1822, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311194038315271\n",
      "loss: 0.1311194038315271\n",
      "h_val:  tensor(1.1825, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111924817071804\n",
      "loss: 0.13111924817071804\n",
      "h_val:  tensor(1.1829, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111910882549643\n",
      "loss: 0.13111910882549643\n",
      "h_val:  tensor(1.1828, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111896485333824\n",
      "loss: 0.13111896485333824\n",
      "h_val:  tensor(1.1827, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111888571305758\n",
      "loss: 0.13111888571305758\n",
      "h_val:  tensor(1.1826, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311188377796894\n",
      "loss: 0.1311188377796894\n",
      "h_val:  tensor(1.1824, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111878036602803\n",
      "loss: 0.13111878036602803\n",
      "h_val:  tensor(1.1824, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111873228731727\n",
      "loss: 0.13111873228731727\n",
      "h_val:  tensor(1.1824, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111869651478902\n",
      "loss: 0.13111869651478902\n",
      "h_val:  tensor(1.1824, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311186699421958\n",
      "loss: 0.1311186699421958\n",
      "h_val:  tensor(1.1824, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111864201786902\n",
      "loss: 0.13111864201786902\n",
      "h_val:  tensor(1.1822, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111860186820867\n",
      "loss: 0.13111860186820867\n",
      "h_val:  tensor(1.1823, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111851854885126\n",
      "loss: 0.13111851854885126\n",
      "h_val:  tensor(1.1822, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111847990882983\n",
      "loss: 0.13111847990882983\n",
      "h_val:  tensor(1.1821, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111843939974296\n",
      "loss: 0.13111843939974296\n",
      "h_val:  tensor(1.1820, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111838558808528\n",
      "loss: 0.13111838558808528\n",
      "h_val:  tensor(1.1820, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111830523190263\n",
      "loss: 0.13111830523190263\n",
      "h_val:  tensor(1.1820, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111822051263977\n",
      "loss: 0.13111822051263977\n",
      "h_val:  tensor(1.1822, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311181705392274\n",
      "loss: 0.1311181705392274\n",
      "h_val:  tensor(1.1825, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311180756988835\n",
      "loss: 0.1311180756988835\n",
      "h_val:  tensor(1.1829, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111796540530074\n",
      "loss: 0.13111796540530074\n",
      "h_val:  tensor(1.1837, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311176870297409\n",
      "loss: 0.1311176870297409\n",
      "h_val:  tensor(1.1845, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111743564035888\n",
      "loss: 0.13111743564035888\n",
      "h_val:  tensor(1.1845, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311171553936599\n",
      "loss: 0.1311171553936599\n",
      "h_val:  tensor(1.1843, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111678251011\n",
      "loss: 0.13111678251011\n",
      "h_val:  tensor(1.1846, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311166505542436\n",
      "loss: 0.1311166505542436\n",
      "h_val:  tensor(1.1846, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111642466038584\n",
      "loss: 0.13111642466038584\n",
      "h_val:  tensor(1.1847, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111632798109732\n",
      "loss: 0.13111632798109732\n",
      "h_val:  tensor(1.1847, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311162768779388\n",
      "loss: 0.1311162768779388\n",
      "h_val:  tensor(1.1848, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111611173967136\n",
      "loss: 0.13111611173967136\n",
      "h_val:  tensor(1.1849, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111595149265465\n",
      "loss: 0.13111595149265465\n",
      "h_val:  tensor(1.1848, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111559875099474\n",
      "loss: 0.13111559875099474\n",
      "h_val:  tensor(1.1846, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111533451932564\n",
      "loss: 0.13111533451932564\n",
      "h_val:  tensor(1.1845, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311151496569857\n",
      "loss: 0.1311151496569857\n",
      "h_val:  tensor(1.1841, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111496701652733\n",
      "loss: 0.13111496701652733\n",
      "h_val:  tensor(1.1841, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111477684797454\n",
      "loss: 0.13111477684797454\n",
      "h_val:  tensor(1.1842, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111470602534225\n",
      "loss: 0.13111470602534225\n",
      "h_val:  tensor(1.1842, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111465240865283\n",
      "loss: 0.13111465240865283\n",
      "h_val:  tensor(1.1842, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111459868255568\n",
      "loss: 0.13111459868255568\n",
      "h_val:  tensor(1.1842, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111453406262252\n",
      "loss: 0.13111453406262252\n",
      "h_val:  tensor(1.1842, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311144460409465\n",
      "loss: 0.1311144460409465\n",
      "h_val:  tensor(1.1840, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311143658138429\n",
      "loss: 0.1311143658138429\n",
      "h_val:  tensor(1.1841, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311142347665877\n",
      "loss: 0.1311142347665877\n",
      "h_val:  tensor(1.1841, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111414468187674\n",
      "loss: 0.13111414468187674\n",
      "h_val:  tensor(1.1843, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111385752646457\n",
      "loss: 0.13111385752646457\n",
      "h_val:  tensor(1.1845, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111387370752842\n",
      "loss: 0.13111387370752842\n",
      "h_val:  tensor(1.1844, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311137627254198\n",
      "loss: 0.1311137627254198\n",
      "h_val:  tensor(1.1844, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311136273689962\n",
      "loss: 0.1311136273689962\n",
      "h_val:  tensor(1.1845, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111348861544733\n",
      "loss: 0.13111348861544733\n",
      "h_val:  tensor(1.1844, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311133791851443\n",
      "loss: 0.1311133791851443\n",
      "h_val:  tensor(1.1844, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111324723467443\n",
      "loss: 0.13111324723467443\n",
      "h_val:  tensor(1.1844, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111303426698645\n",
      "loss: 0.13111303426698645\n",
      "h_val:  tensor(1.1844, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311128503540374\n",
      "loss: 0.1311128503540374\n",
      "h_val:  tensor(1.1845, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311125197270657\n",
      "loss: 0.1311125197270657\n",
      "h_val:  tensor(1.1844, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311121185193049\n",
      "loss: 0.1311121185193049\n",
      "h_val:  tensor(1.1844, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111182169601865\n",
      "loss: 0.13111182169601865\n",
      "h_val:  tensor(1.1843, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111151409962177\n",
      "loss: 0.13111151409962177\n",
      "h_val:  tensor(1.1841, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111134574176242\n",
      "loss: 0.13111134574176242\n",
      "h_val:  tensor(1.1841, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311112196019464\n",
      "loss: 0.1311112196019464\n",
      "h_val:  tensor(1.1840, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111104944875532\n",
      "loss: 0.13111104944875532\n",
      "h_val:  tensor(1.1838, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111096670045128\n",
      "loss: 0.13111096670045128\n",
      "h_val:  tensor(1.1838, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111095017163316\n",
      "loss: 0.13111095017163316\n",
      "h_val:  tensor(1.1838, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111086547530393\n",
      "loss: 0.13111086547530393\n",
      "h_val:  tensor(1.1837, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111072715626576\n",
      "loss: 0.13111072715626576\n",
      "h_val:  tensor(1.1837, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111067643804142\n",
      "loss: 0.13111067643804142\n",
      "h_val:  tensor(1.1838, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311105482480207\n",
      "loss: 0.1311105482480207\n",
      "h_val:  tensor(1.1840, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111046356986752\n",
      "loss: 0.13111046356986752\n",
      "h_val:  tensor(1.1841, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311103240555977\n",
      "loss: 0.1311103240555977\n",
      "h_val:  tensor(1.1842, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13111014384146188\n",
      "loss: 0.13111014384146188\n",
      "h_val:  tensor(1.1843, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.131110046516626\n",
      "loss: 0.131110046516626\n",
      "h_val:  tensor(1.1845, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110986663690186\n",
      "loss: 0.13110986663690186\n",
      "h_val:  tensor(1.1850, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110977096123977\n",
      "loss: 0.13110977096123977\n",
      "h_val:  tensor(1.1852, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311095920401299\n",
      "loss: 0.1311095920401299\n",
      "h_val:  tensor(1.1854, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311094890798746\n",
      "loss: 0.1311094890798746\n",
      "h_val:  tensor(1.1857, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110937629490657\n",
      "loss: 0.13110937629490657\n",
      "h_val:  tensor(1.1861, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110919259225282\n",
      "loss: 0.13110919259225282\n",
      "h_val:  tensor(1.1877, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110917608785744\n",
      "loss: 0.13110917608785744\n",
      "h_val:  tensor(1.1869, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110900531581052\n",
      "loss: 0.13110900531581052\n",
      "h_val:  tensor(1.1876, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311086843071328\n",
      "loss: 0.1311086843071328\n",
      "h_val:  tensor(1.1881, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110843888970244\n",
      "loss: 0.13110843888970244\n",
      "h_val:  tensor(1.1890, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110817929154783\n",
      "loss: 0.13110817929154783\n",
      "h_val:  tensor(1.1891, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110790874175945\n",
      "loss: 0.13110790874175945\n",
      "h_val:  tensor(1.1892, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110773545874022\n",
      "loss: 0.13110773545874022\n",
      "h_val:  tensor(1.1897, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110754246105374\n",
      "loss: 0.13110754246105374\n",
      "h_val:  tensor(1.1902, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110747024372235\n",
      "loss: 0.13110747024372235\n",
      "h_val:  tensor(1.1903, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110733801813468\n",
      "loss: 0.13110733801813468\n",
      "h_val:  tensor(1.1906, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110721438491407\n",
      "loss: 0.13110721438491407\n",
      "h_val:  tensor(1.1909, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110712139957348\n",
      "loss: 0.13110712139957348\n",
      "h_val:  tensor(1.1912, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110698489439648\n",
      "loss: 0.13110698489439648\n",
      "h_val:  tensor(1.1914, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110689568345102\n",
      "loss: 0.13110689568345102\n",
      "h_val:  tensor(1.1914, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110681439945868\n",
      "loss: 0.13110681439945868\n",
      "h_val:  tensor(1.1913, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110674059632646\n",
      "loss: 0.13110674059632646\n",
      "h_val:  tensor(1.1911, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110661009207233\n",
      "loss: 0.13110661009207233\n",
      "h_val:  tensor(1.1912, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110671200793644\n",
      "loss: 0.13110671200793644\n",
      "h_val:  tensor(1.1911, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110651794339492\n",
      "loss: 0.13110651794339492\n",
      "h_val:  tensor(1.1911, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110635232430345\n",
      "loss: 0.13110635232430345\n",
      "h_val:  tensor(1.1911, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110625252569158\n",
      "loss: 0.13110625252569158\n",
      "h_val:  tensor(1.1913, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311061269072832\n",
      "loss: 0.1311061269072832\n",
      "h_val:  tensor(1.1916, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311059883480091\n",
      "loss: 0.1311059883480091\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311057856114461\n",
      "loss: 0.1311057856114461\n",
      "h_val:  tensor(1.1921, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110544484983416\n",
      "loss: 0.13110544484983416\n",
      "h_val:  tensor(1.1931, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110550242721175\n",
      "loss: 0.13110550242721175\n",
      "h_val:  tensor(1.1926, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110528715426137\n",
      "loss: 0.13110528715426137\n",
      "h_val:  tensor(1.1928, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.131105092284028\n",
      "loss: 0.131105092284028\n",
      "h_val:  tensor(1.1933, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110489874586218\n",
      "loss: 0.13110489874586218\n",
      "h_val:  tensor(1.1940, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110472871007808\n",
      "loss: 0.13110472871007808\n",
      "h_val:  tensor(1.1946, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110457830295238\n",
      "loss: 0.13110457830295238\n",
      "h_val:  tensor(1.1947, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110448730491356\n",
      "loss: 0.13110448730491356\n",
      "h_val:  tensor(1.1950, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110433438820154\n",
      "loss: 0.13110433438820154\n",
      "h_val:  tensor(1.1955, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110415620463198\n",
      "loss: 0.13110415620463198\n",
      "h_val:  tensor(1.1964, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110387141147858\n",
      "loss: 0.13110387141147858\n",
      "h_val:  tensor(1.1980, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110345395215808\n",
      "loss: 0.13110345395215808\n",
      "h_val:  tensor(1.2004, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110388777972376\n",
      "loss: 0.13110388777972376\n",
      "h_val:  tensor(1.1988, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311033097696232\n",
      "loss: 0.1311033097696232\n",
      "h_val:  tensor(1.1991, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311031886251066\n",
      "loss: 0.1311031886251066\n",
      "h_val:  tensor(1.1992, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311031253777836\n",
      "loss: 0.1311031253777836\n",
      "h_val:  tensor(1.1992, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110303040495772\n",
      "loss: 0.13110303040495772\n",
      "h_val:  tensor(1.1996, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.131102875156505\n",
      "loss: 0.131102875156505\n",
      "h_val:  tensor(1.2001, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110276348229008\n",
      "loss: 0.13110276348229008\n",
      "h_val:  tensor(1.2003, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110263307031592\n",
      "loss: 0.13110263307031592\n",
      "h_val:  tensor(1.2004, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110255596560244\n",
      "loss: 0.13110255596560244\n",
      "h_val:  tensor(1.2006, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110246910019688\n",
      "loss: 0.13110246910019688\n",
      "h_val:  tensor(1.2010, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311022998061244\n",
      "loss: 0.1311022998061244\n",
      "h_val:  tensor(1.2012, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110206553681775\n",
      "loss: 0.13110206553681775\n",
      "h_val:  tensor(1.2012, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110190032284824\n",
      "loss: 0.13110190032284824\n",
      "h_val:  tensor(1.2011, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110170976341715\n",
      "loss: 0.13110170976341715\n",
      "h_val:  tensor(1.2010, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110153299037672\n",
      "loss: 0.13110153299037672\n",
      "h_val:  tensor(1.2005, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110146739946874\n",
      "loss: 0.13110146739946874\n",
      "h_val:  tensor(1.2006, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1311012844694942\n",
      "loss: 0.1311012844694942\n",
      "h_val:  tensor(1.2006, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110123037634835\n",
      "loss: 0.13110123037634835\n",
      "h_val:  tensor(1.2004, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110109597954614\n",
      "loss: 0.13110109597954614\n",
      "h_val:  tensor(1.2001, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110098696802322\n",
      "loss: 0.13110098696802322\n",
      "h_val:  tensor(1.1998, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110075929988457\n",
      "loss: 0.13110075929988457\n",
      "h_val:  tensor(1.1994, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110054226207973\n",
      "loss: 0.13110054226207973\n",
      "h_val:  tensor(1.1990, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110033968692092\n",
      "loss: 0.13110033968692092\n",
      "h_val:  tensor(1.1983, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.131100051699596\n",
      "loss: 0.131100051699596\n",
      "h_val:  tensor(1.1977, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13110021929464588\n",
      "loss: 0.13110021929464588\n",
      "h_val:  tensor(1.1981, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109992336927834\n",
      "loss: 0.13109992336927834\n",
      "h_val:  tensor(1.1980, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109981899230602\n",
      "loss: 0.13109981899230602\n",
      "h_val:  tensor(1.1980, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109976119518243\n",
      "loss: 0.13109976119518243\n",
      "h_val:  tensor(1.1981, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109968771045194\n",
      "loss: 0.13109968771045194\n",
      "h_val:  tensor(1.1981, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310995317508976\n",
      "loss: 0.1310995317508976\n",
      "h_val:  tensor(1.1979, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109932076835762\n",
      "loss: 0.13109932076835762\n",
      "h_val:  tensor(1.1977, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.131099126428967\n",
      "loss: 0.131099126428967\n",
      "h_val:  tensor(1.1975, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109897413872576\n",
      "loss: 0.13109897413872576\n",
      "h_val:  tensor(1.1974, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109886575826576\n",
      "loss: 0.13109886575826576\n",
      "h_val:  tensor(1.1973, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109874982917935\n",
      "loss: 0.13109874982917935\n",
      "h_val:  tensor(1.1975, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109869398533114\n",
      "loss: 0.13109869398533114\n",
      "h_val:  tensor(1.1974, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109865754968988\n",
      "loss: 0.13109865754968988\n",
      "h_val:  tensor(1.1974, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109863224063031\n",
      "loss: 0.13109863224063031\n",
      "h_val:  tensor(1.1974, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109858134655514\n",
      "loss: 0.13109858134655514\n",
      "h_val:  tensor(1.1975, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109853866886886\n",
      "loss: 0.13109853866886886\n",
      "h_val:  tensor(1.1975, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310984824860076\n",
      "loss: 0.1310984824860076\n",
      "h_val:  tensor(1.1974, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310984379766208\n",
      "loss: 0.1310984379766208\n",
      "h_val:  tensor(1.1974, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109837482992676\n",
      "loss: 0.13109837482992676\n",
      "h_val:  tensor(1.1972, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109828788138586\n",
      "loss: 0.13109828788138586\n",
      "h_val:  tensor(1.1967, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109820535003114\n",
      "loss: 0.13109820535003114\n",
      "h_val:  tensor(1.1966, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109810690629792\n",
      "loss: 0.13109810690629792\n",
      "h_val:  tensor(1.1965, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310980083185511\n",
      "loss: 0.1310980083185511\n",
      "h_val:  tensor(1.1964, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310979441195916\n",
      "loss: 0.1310979441195916\n",
      "h_val:  tensor(1.1961, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109783739165934\n",
      "loss: 0.13109783739165934\n",
      "h_val:  tensor(1.1961, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109774444626307\n",
      "loss: 0.13109774444626307\n",
      "h_val:  tensor(1.1962, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109769679020633\n",
      "loss: 0.13109769679020633\n",
      "h_val:  tensor(1.1963, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109763645221478\n",
      "loss: 0.13109763645221478\n",
      "h_val:  tensor(1.1964, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109756653265747\n",
      "loss: 0.13109756653265747\n",
      "h_val:  tensor(1.1965, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.131097422695479\n",
      "loss: 0.131097422695479\n",
      "h_val:  tensor(1.1965, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109733956871258\n",
      "loss: 0.13109733956871258\n",
      "h_val:  tensor(1.1965, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109723741477863\n",
      "loss: 0.13109723741477863\n",
      "h_val:  tensor(1.1965, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109718688854222\n",
      "loss: 0.13109718688854222\n",
      "h_val:  tensor(1.1965, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109714757092963\n",
      "loss: 0.13109714757092963\n",
      "h_val:  tensor(1.1966, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310971392437953\n",
      "loss: 0.1310971392437953\n",
      "h_val:  tensor(1.1966, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109707559100142\n",
      "loss: 0.13109707559100142\n",
      "h_val:  tensor(1.1966, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109705353378168\n",
      "loss: 0.13109705353378168\n",
      "h_val:  tensor(1.1967, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109701457158882\n",
      "loss: 0.13109701457158882\n",
      "h_val:  tensor(1.1969, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109694808148006\n",
      "loss: 0.13109694808148006\n",
      "h_val:  tensor(1.1971, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310968185629169\n",
      "loss: 0.1310968185629169\n",
      "h_val:  tensor(1.1974, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109666292792446\n",
      "loss: 0.13109666292792446\n",
      "h_val:  tensor(1.1975, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109652884417097\n",
      "loss: 0.13109652884417097\n",
      "h_val:  tensor(1.1975, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109641650440934\n",
      "loss: 0.13109641650440934\n",
      "h_val:  tensor(1.1976, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310963345397486\n",
      "loss: 0.1310963345397486\n",
      "h_val:  tensor(1.1976, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109620427860433\n",
      "loss: 0.13109620427860433\n",
      "h_val:  tensor(1.1977, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109603162172329\n",
      "loss: 0.13109603162172329\n",
      "h_val:  tensor(1.1980, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109583000205943\n",
      "loss: 0.13109583000205943\n",
      "h_val:  tensor(1.1980, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109575541805468\n",
      "loss: 0.13109575541805468\n",
      "h_val:  tensor(1.1981, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109560046778473\n",
      "loss: 0.13109560046778473\n",
      "h_val:  tensor(1.1982, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109546591207585\n",
      "loss: 0.13109546591207585\n",
      "h_val:  tensor(1.1983, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109536519518594\n",
      "loss: 0.13109536519518594\n",
      "h_val:  tensor(1.1984, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109520308074216\n",
      "loss: 0.13109520308074216\n",
      "h_val:  tensor(1.1986, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109505144921285\n",
      "loss: 0.13109505144921285\n",
      "h_val:  tensor(1.1987, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310949124728322\n",
      "loss: 0.1310949124728322\n",
      "h_val:  tensor(1.1988, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109479989878797\n",
      "loss: 0.13109479989878797\n",
      "h_val:  tensor(1.1987, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109475493292677\n",
      "loss: 0.13109475493292677\n",
      "h_val:  tensor(1.1987, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109468302988833\n",
      "loss: 0.13109468302988833\n",
      "h_val:  tensor(1.1989, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109459922049205\n",
      "loss: 0.13109459922049205\n",
      "h_val:  tensor(1.1990, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109452476477101\n",
      "loss: 0.13109452476477101\n",
      "h_val:  tensor(1.1993, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109439651210467\n",
      "loss: 0.13109439651210467\n",
      "h_val:  tensor(1.1997, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310942796625689\n",
      "loss: 0.1310942796625689\n",
      "h_val:  tensor(1.1998, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109420169559713\n",
      "loss: 0.13109420169559713\n",
      "h_val:  tensor(1.1999, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310940994496846\n",
      "loss: 0.1310940994496846\n",
      "h_val:  tensor(1.2001, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109409521630605\n",
      "loss: 0.13109409521630605\n",
      "h_val:  tensor(1.2000, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310940676930887\n",
      "loss: 0.1310940676930887\n",
      "h_val:  tensor(1.2001, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.131094021754692\n",
      "loss: 0.131094021754692\n",
      "h_val:  tensor(1.2001, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109395666733004\n",
      "loss: 0.13109395666733004\n",
      "h_val:  tensor(1.2002, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109392217373833\n",
      "loss: 0.13109392217373833\n",
      "h_val:  tensor(1.2001, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109384882720096\n",
      "loss: 0.13109384882720096\n",
      "h_val:  tensor(1.2000, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109378016585238\n",
      "loss: 0.13109378016585238\n",
      "h_val:  tensor(1.1999, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109367144746473\n",
      "loss: 0.13109367144746473\n",
      "h_val:  tensor(1.1996, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109351328859442\n",
      "loss: 0.13109351328859442\n",
      "h_val:  tensor(1.1993, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310938815958609\n",
      "loss: 0.1310938815958609\n",
      "h_val:  tensor(1.1995, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109344836512046\n",
      "loss: 0.13109344836512046\n",
      "h_val:  tensor(1.1992, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109328576377674\n",
      "loss: 0.13109328576377674\n",
      "h_val:  tensor(1.1991, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109318862068867\n",
      "loss: 0.13109318862068867\n",
      "h_val:  tensor(1.1990, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310931039801721\n",
      "loss: 0.1310931039801721\n",
      "h_val:  tensor(1.1990, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109309820274392\n",
      "loss: 0.13109309820274392\n",
      "h_val:  tensor(1.1990, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109306894700443\n",
      "loss: 0.13109306894700443\n",
      "h_val:  tensor(1.1989, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109301979914556\n",
      "loss: 0.13109301979914556\n",
      "h_val:  tensor(1.1989, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109291480477767\n",
      "loss: 0.13109291480477767\n",
      "h_val:  tensor(1.1988, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109278645846004\n",
      "loss: 0.13109278645846004\n",
      "h_val:  tensor(1.1986, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109263436701227\n",
      "loss: 0.13109263436701227\n",
      "h_val:  tensor(1.1985, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109248297469497\n",
      "loss: 0.13109248297469497\n",
      "h_val:  tensor(1.1984, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109235357226684\n",
      "loss: 0.13109235357226684\n",
      "h_val:  tensor(1.1983, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109227796458728\n",
      "loss: 0.13109227796458728\n",
      "h_val:  tensor(1.1982, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310921827150573\n",
      "loss: 0.1310921827150573\n",
      "h_val:  tensor(1.1981, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310920430677748\n",
      "loss: 0.1310920430677748\n",
      "h_val:  tensor(1.1979, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109185332289175\n",
      "loss: 0.13109185332289175\n",
      "h_val:  tensor(1.1979, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109159573604026\n",
      "loss: 0.13109159573604026\n",
      "h_val:  tensor(1.1978, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310912418440675\n",
      "loss: 0.1310912418440675\n",
      "h_val:  tensor(1.1974, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109064873464976\n",
      "loss: 0.13109064873464976\n",
      "h_val:  tensor(1.1966, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13109006127295952\n",
      "loss: 0.13109006127295952\n",
      "h_val:  tensor(1.1948, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310899019369375\n",
      "loss: 0.1310899019369375\n",
      "h_val:  tensor(1.1951, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108946016913975\n",
      "loss: 0.13108946016913975\n",
      "h_val:  tensor(1.1952, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108935399287328\n",
      "loss: 0.13108935399287328\n",
      "h_val:  tensor(1.1949, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108920736156274\n",
      "loss: 0.13108920736156274\n",
      "h_val:  tensor(1.1944, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108916309165336\n",
      "loss: 0.13108916309165336\n",
      "h_val:  tensor(1.1942, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108892405546888\n",
      "loss: 0.13108892405546888\n",
      "h_val:  tensor(1.1940, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108883119195824\n",
      "loss: 0.13108883119195824\n",
      "h_val:  tensor(1.1937, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108869515046445\n",
      "loss: 0.13108869515046445\n",
      "h_val:  tensor(1.1933, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108844103758086\n",
      "loss: 0.13108844103758086\n",
      "h_val:  tensor(1.1923, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108827730606556\n",
      "loss: 0.13108827730606556\n",
      "h_val:  tensor(1.1922, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108795589087494\n",
      "loss: 0.13108795589087494\n",
      "h_val:  tensor(1.1922, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108783547813657\n",
      "loss: 0.13108783547813657\n",
      "h_val:  tensor(1.1921, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310876562727883\n",
      "loss: 0.1310876562727883\n",
      "h_val:  tensor(1.1919, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108760107442696\n",
      "loss: 0.13108760107442696\n",
      "h_val:  tensor(1.1918, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108743283969918\n",
      "loss: 0.13108743283969918\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310873618774039\n",
      "loss: 0.1310873618774039\n",
      "h_val:  tensor(1.1916, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310872951277856\n",
      "loss: 0.1310872951277856\n",
      "h_val:  tensor(1.1916, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108715944822658\n",
      "loss: 0.13108715944822658\n",
      "h_val:  tensor(1.1918, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310877602230574\n",
      "loss: 0.1310877602230574\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108707886626633\n",
      "loss: 0.13108707886626633\n",
      "h_val:  tensor(1.1918, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310868867147585\n",
      "loss: 0.1310868867147585\n",
      "h_val:  tensor(1.1921, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108673731010173\n",
      "loss: 0.13108673731010173\n",
      "h_val:  tensor(1.1926, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310866520094273\n",
      "loss: 0.1310866520094273\n",
      "h_val:  tensor(1.1926, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310865945028695\n",
      "loss: 0.1310865945028695\n",
      "h_val:  tensor(1.1926, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108656375179667\n",
      "loss: 0.13108656375179667\n",
      "h_val:  tensor(1.1925, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108649434137293\n",
      "loss: 0.13108649434137293\n",
      "h_val:  tensor(1.1925, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108642512821664\n",
      "loss: 0.13108642512821664\n",
      "h_val:  tensor(1.1925, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310863042500384\n",
      "loss: 0.1310863042500384\n",
      "h_val:  tensor(1.1925, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108615634985898\n",
      "loss: 0.13108615634985898\n",
      "h_val:  tensor(1.1925, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108596644195356\n",
      "loss: 0.13108596644195356\n",
      "h_val:  tensor(1.1923, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108583733970425\n",
      "loss: 0.13108583733970425\n",
      "h_val:  tensor(1.1923, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108566748962672\n",
      "loss: 0.13108566748962672\n",
      "h_val:  tensor(1.1921, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108553694732952\n",
      "loss: 0.13108553694732952\n",
      "h_val:  tensor(1.1919, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108539559870774\n",
      "loss: 0.13108539559870774\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310852173629897\n",
      "loss: 0.1310852173629897\n",
      "h_val:  tensor(1.1915, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310850377645071\n",
      "loss: 0.1310850377645071\n",
      "h_val:  tensor(1.1912, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108474879872134\n",
      "loss: 0.13108474879872134\n",
      "h_val:  tensor(1.1913, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108464686590754\n",
      "loss: 0.13108464686590754\n",
      "h_val:  tensor(1.1914, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108450185426976\n",
      "loss: 0.13108450185426976\n",
      "h_val:  tensor(1.1915, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310842539924618\n",
      "loss: 0.1310842539924618\n",
      "h_val:  tensor(1.1915, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310837303604657\n",
      "loss: 0.1310837303604657\n",
      "h_val:  tensor(1.1913, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310831666881607\n",
      "loss: 0.1310831666881607\n",
      "h_val:  tensor(1.1911, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108250685076916\n",
      "loss: 0.13108250685076916\n",
      "h_val:  tensor(1.1906, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108206542893497\n",
      "loss: 0.13108206542893497\n",
      "h_val:  tensor(1.1904, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310818913430116\n",
      "loss: 0.1310818913430116\n",
      "h_val:  tensor(1.1904, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108173146557361\n",
      "loss: 0.13108173146557361\n",
      "h_val:  tensor(1.1902, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108153518808627\n",
      "loss: 0.13108153518808627\n",
      "h_val:  tensor(1.1902, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310814693521699\n",
      "loss: 0.1310814693521699\n",
      "h_val:  tensor(1.1897, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108122615504214\n",
      "loss: 0.13108122615504214\n",
      "h_val:  tensor(1.1895, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108119858568987\n",
      "loss: 0.13108119858568987\n",
      "h_val:  tensor(1.1894, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310809825014766\n",
      "loss: 0.1310809825014766\n",
      "h_val:  tensor(1.1893, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108087082805311\n",
      "loss: 0.13108087082805311\n",
      "h_val:  tensor(1.1890, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108063317288782\n",
      "loss: 0.13108063317288782\n",
      "h_val:  tensor(1.1886, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108033533120061\n",
      "loss: 0.13108033533120061\n",
      "h_val:  tensor(1.1882, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13108027231882208\n",
      "loss: 0.13108027231882208\n",
      "h_val:  tensor(1.1882, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107994336407341\n",
      "loss: 0.13107994336407341\n",
      "h_val:  tensor(1.1883, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107986911750794\n",
      "loss: 0.13107986911750794\n",
      "h_val:  tensor(1.1883, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310797409791347\n",
      "loss: 0.1310797409791347\n",
      "h_val:  tensor(1.1882, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107968778156895\n",
      "loss: 0.13107968778156895\n",
      "h_val:  tensor(1.1881, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107950975181012\n",
      "loss: 0.13107950975181012\n",
      "h_val:  tensor(1.1880, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107942116994467\n",
      "loss: 0.13107942116994467\n",
      "h_val:  tensor(1.1879, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107928568049987\n",
      "loss: 0.13107928568049987\n",
      "h_val:  tensor(1.1876, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107904268395681\n",
      "loss: 0.13107904268395681\n",
      "h_val:  tensor(1.1870, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310792991328973\n",
      "loss: 0.1310792991328973\n",
      "h_val:  tensor(1.1874, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107885264756053\n",
      "loss: 0.13107885264756053\n",
      "h_val:  tensor(1.1871, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310784827415107\n",
      "loss: 0.1310784827415107\n",
      "h_val:  tensor(1.1871, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107817620948758\n",
      "loss: 0.13107817620948758\n",
      "h_val:  tensor(1.1869, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107790076224832\n",
      "loss: 0.13107790076224832\n",
      "h_val:  tensor(1.1870, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107762872750045\n",
      "loss: 0.13107762872750045\n",
      "h_val:  tensor(1.1868, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107736784073398\n",
      "loss: 0.13107736784073398\n",
      "h_val:  tensor(1.1864, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107695807867698\n",
      "loss: 0.13107695807867698\n",
      "h_val:  tensor(1.1857, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310766883984532\n",
      "loss: 0.1310766883984532\n",
      "h_val:  tensor(1.1855, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310764650695998\n",
      "loss: 0.1310764650695998\n",
      "h_val:  tensor(1.1844, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107601944913755\n",
      "loss: 0.13107601944913755\n",
      "h_val:  tensor(1.1840, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310758712287229\n",
      "loss: 0.1310758712287229\n",
      "h_val:  tensor(1.1839, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107566964436834\n",
      "loss: 0.13107566964436834\n",
      "h_val:  tensor(1.1835, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107535474020535\n",
      "loss: 0.13107535474020535\n",
      "h_val:  tensor(1.1833, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107521993734417\n",
      "loss: 0.13107521993734417\n",
      "h_val:  tensor(1.1827, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107491636834365\n",
      "loss: 0.13107491636834365\n",
      "h_val:  tensor(1.1825, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107460400248433\n",
      "loss: 0.13107460400248433\n",
      "h_val:  tensor(1.1827, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107451010834276\n",
      "loss: 0.13107451010834276\n",
      "h_val:  tensor(1.1827, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107439092342124\n",
      "loss: 0.13107439092342124\n",
      "h_val:  tensor(1.1829, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107424473450857\n",
      "loss: 0.13107424473450857\n",
      "h_val:  tensor(1.1830, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107403437129977\n",
      "loss: 0.13107403437129977\n",
      "h_val:  tensor(1.1831, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310738180845207\n",
      "loss: 0.1310738180845207\n",
      "h_val:  tensor(1.1831, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107362626276345\n",
      "loss: 0.13107362626276345\n",
      "h_val:  tensor(1.1831, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.131073466321461\n",
      "loss: 0.131073466321461\n",
      "h_val:  tensor(1.1832, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107333476157576\n",
      "loss: 0.13107333476157576\n",
      "h_val:  tensor(1.1833, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310732601125893\n",
      "loss: 0.1310732601125893\n",
      "h_val:  tensor(1.1833, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310732315033873\n",
      "loss: 0.1310732315033873\n",
      "h_val:  tensor(1.1834, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107320903269493\n",
      "loss: 0.13107320903269493\n",
      "h_val:  tensor(1.1834, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107316267210686\n",
      "loss: 0.13107316267210686\n",
      "h_val:  tensor(1.1835, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107313218640754\n",
      "loss: 0.13107313218640754\n",
      "h_val:  tensor(1.1835, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107309336827253\n",
      "loss: 0.13107309336827253\n",
      "h_val:  tensor(1.1835, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107305376127795\n",
      "loss: 0.13107305376127795\n",
      "h_val:  tensor(1.1834, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107303171482754\n",
      "loss: 0.13107303171482754\n",
      "h_val:  tensor(1.1833, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107301720362646\n",
      "loss: 0.13107301720362646\n",
      "h_val:  tensor(1.1833, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107299691155924\n",
      "loss: 0.13107299691155924\n",
      "h_val:  tensor(1.1833, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107299101297035\n",
      "loss: 0.13107299101297035\n",
      "h_val:  tensor(1.1833, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310729739613719\n",
      "loss: 0.1310729739613719\n",
      "h_val:  tensor(1.1833, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310729383039069\n",
      "loss: 0.1310729383039069\n",
      "h_val:  tensor(1.1834, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310728895914453\n",
      "loss: 0.1310728895914453\n",
      "h_val:  tensor(1.1834, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107282745276766\n",
      "loss: 0.13107282745276766\n",
      "h_val:  tensor(1.1837, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310727168438693\n",
      "loss: 0.1310727168438693\n",
      "h_val:  tensor(1.1839, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107264819663222\n",
      "loss: 0.13107264819663222\n",
      "h_val:  tensor(1.1840, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107258496145097\n",
      "loss: 0.13107258496145097\n",
      "h_val:  tensor(1.1842, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107252220092797\n",
      "loss: 0.13107252220092797\n",
      "h_val:  tensor(1.1844, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107247169937192\n",
      "loss: 0.13107247169937192\n",
      "h_val:  tensor(1.1847, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107241870131064\n",
      "loss: 0.13107241870131064\n",
      "h_val:  tensor(1.1848, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310723836652509\n",
      "loss: 0.1310723836652509\n",
      "h_val:  tensor(1.1850, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310723396703948\n",
      "loss: 0.1310723396703948\n",
      "h_val:  tensor(1.1850, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107237914426892\n",
      "loss: 0.13107237914426892\n",
      "h_val:  tensor(1.1850, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107232599843183\n",
      "loss: 0.13107232599843183\n",
      "h_val:  tensor(1.1851, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107230323011604\n",
      "loss: 0.13107230323011604\n",
      "h_val:  tensor(1.1854, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107224618050392\n",
      "loss: 0.13107224618050392\n",
      "h_val:  tensor(1.1856, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107221418004084\n",
      "loss: 0.13107221418004084\n",
      "h_val:  tensor(1.1857, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107217172781918\n",
      "loss: 0.13107217172781918\n",
      "h_val:  tensor(1.1857, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107214575622939\n",
      "loss: 0.13107214575622939\n",
      "h_val:  tensor(1.1858, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107211459225482\n",
      "loss: 0.13107211459225482\n",
      "h_val:  tensor(1.1860, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107208621948613\n",
      "loss: 0.13107208621948613\n",
      "h_val:  tensor(1.1860, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107204601343392\n",
      "loss: 0.13107204601343392\n",
      "h_val:  tensor(1.1860, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107202661729037\n",
      "loss: 0.13107202661729037\n",
      "h_val:  tensor(1.1861, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310720117050176\n",
      "loss: 0.1310720117050176\n",
      "h_val:  tensor(1.1861, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310719826795692\n",
      "loss: 0.1310719826795692\n",
      "h_val:  tensor(1.1862, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107194677951312\n",
      "loss: 0.13107194677951312\n",
      "h_val:  tensor(1.1862, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107189504200242\n",
      "loss: 0.13107189504200242\n",
      "h_val:  tensor(1.1861, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107186407199717\n",
      "loss: 0.13107186407199717\n",
      "h_val:  tensor(1.1861, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107181767656623\n",
      "loss: 0.13107181767656623\n",
      "h_val:  tensor(1.1862, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107176398839812\n",
      "loss: 0.13107176398839812\n",
      "h_val:  tensor(1.1864, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107182094594808\n",
      "loss: 0.13107182094594808\n",
      "h_val:  tensor(1.1862, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310717457468315\n",
      "loss: 0.1310717457468315\n",
      "h_val:  tensor(1.1863, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107172234169734\n",
      "loss: 0.13107172234169734\n",
      "h_val:  tensor(1.1863, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107170054356648\n",
      "loss: 0.13107170054356648\n",
      "h_val:  tensor(1.1864, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107166370018955\n",
      "loss: 0.13107166370018955\n",
      "h_val:  tensor(1.1864, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107167931257893\n",
      "loss: 0.13107167931257893\n",
      "h_val:  tensor(1.1864, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107164063334306\n",
      "loss: 0.13107164063334306\n",
      "h_val:  tensor(1.1865, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310716043827103\n",
      "loss: 0.1310716043827103\n",
      "h_val:  tensor(1.1866, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107153884123776\n",
      "loss: 0.13107153884123776\n",
      "h_val:  tensor(1.1867, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107150182348926\n",
      "loss: 0.13107150182348926\n",
      "h_val:  tensor(1.1868, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310714570960592\n",
      "loss: 0.1310714570960592\n",
      "h_val:  tensor(1.1870, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310714126281541\n",
      "loss: 0.1310714126281541\n",
      "h_val:  tensor(1.1872, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310713865345017\n",
      "loss: 0.1310713865345017\n",
      "h_val:  tensor(1.1872, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107136469617797\n",
      "loss: 0.13107136469617797\n",
      "h_val:  tensor(1.1871, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107130415397764\n",
      "loss: 0.13107130415397764\n",
      "h_val:  tensor(1.1872, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310712508108115\n",
      "loss: 0.1310712508108115\n",
      "h_val:  tensor(1.1871, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310712035614223\n",
      "loss: 0.1310712035614223\n",
      "h_val:  tensor(1.1871, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107104805495026\n",
      "loss: 0.13107104805495026\n",
      "h_val:  tensor(1.1871, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107097197020612\n",
      "loss: 0.13107097197020612\n",
      "h_val:  tensor(1.1871, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310708884907836\n",
      "loss: 0.1310708884907836\n",
      "h_val:  tensor(1.1872, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107091956897007\n",
      "loss: 0.13107091956897007\n",
      "h_val:  tensor(1.1871, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107085626406012\n",
      "loss: 0.13107085626406012\n",
      "h_val:  tensor(1.1870, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107081004393634\n",
      "loss: 0.13107081004393634\n",
      "h_val:  tensor(1.1869, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310707609583988\n",
      "loss: 0.1310707609583988\n",
      "h_val:  tensor(1.1869, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107072194997368\n",
      "loss: 0.13107072194997368\n",
      "h_val:  tensor(1.1869, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107067193426863\n",
      "loss: 0.13107067193426863\n",
      "h_val:  tensor(1.1869, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107060094640788\n",
      "loss: 0.13107060094640788\n",
      "h_val:  tensor(1.1872, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107053564450036\n",
      "loss: 0.13107053564450036\n",
      "h_val:  tensor(1.1873, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310704055523239\n",
      "loss: 0.1310704055523239\n",
      "h_val:  tensor(1.1873, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107035813242526\n",
      "loss: 0.13107035813242526\n",
      "h_val:  tensor(1.1875, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107023998003284\n",
      "loss: 0.13107023998003284\n",
      "h_val:  tensor(1.1878, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107007900134052\n",
      "loss: 0.13107007900134052\n",
      "h_val:  tensor(1.1884, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13107003376358245\n",
      "loss: 0.13107003376358245\n",
      "h_val:  tensor(1.1883, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106990687660564\n",
      "loss: 0.13106990687660564\n",
      "h_val:  tensor(1.1883, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310698808301422\n",
      "loss: 0.1310698808301422\n",
      "h_val:  tensor(1.1885, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106981773014564\n",
      "loss: 0.13106981773014564\n",
      "h_val:  tensor(1.1888, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310697000025814\n",
      "loss: 0.1310697000025814\n",
      "h_val:  tensor(1.1892, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106959702095886\n",
      "loss: 0.13106959702095886\n",
      "h_val:  tensor(1.1897, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106948512604524\n",
      "loss: 0.13106948512604524\n",
      "h_val:  tensor(1.1899, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106941273787634\n",
      "loss: 0.13106941273787634\n",
      "h_val:  tensor(1.1900, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106933765156548\n",
      "loss: 0.13106933765156548\n",
      "h_val:  tensor(1.1903, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106924305174547\n",
      "loss: 0.13106924305174547\n",
      "h_val:  tensor(1.1905, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106916489993634\n",
      "loss: 0.13106916489993634\n",
      "h_val:  tensor(1.1907, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310691100477882\n",
      "loss: 0.1310691100477882\n",
      "h_val:  tensor(1.1906, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106909167170772\n",
      "loss: 0.13106909167170772\n",
      "h_val:  tensor(1.1906, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106907686900662\n",
      "loss: 0.13106907686900662\n",
      "h_val:  tensor(1.1905, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106905918842007\n",
      "loss: 0.13106905918842007\n",
      "h_val:  tensor(1.1905, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310690078344137\n",
      "loss: 0.1310690078344137\n",
      "h_val:  tensor(1.1905, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106895144398376\n",
      "loss: 0.13106895144398376\n",
      "h_val:  tensor(1.1906, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310689107076908\n",
      "loss: 0.1310689107076908\n",
      "h_val:  tensor(1.1906, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310688790080221\n",
      "loss: 0.1310688790080221\n",
      "h_val:  tensor(1.1907, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310688631474488\n",
      "loss: 0.1310688631474488\n",
      "h_val:  tensor(1.1907, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106884798123167\n",
      "loss: 0.13106884798123167\n",
      "h_val:  tensor(1.1909, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106881180078364\n",
      "loss: 0.13106881180078364\n",
      "h_val:  tensor(1.1910, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106877351093524\n",
      "loss: 0.13106877351093524\n",
      "h_val:  tensor(1.1910, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106873991401913\n",
      "loss: 0.13106873991401913\n",
      "h_val:  tensor(1.1912, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106871052176938\n",
      "loss: 0.13106871052176938\n",
      "h_val:  tensor(1.1912, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106868882835596\n",
      "loss: 0.13106868882835596\n",
      "h_val:  tensor(1.1913, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310686706681932\n",
      "loss: 0.1310686706681932\n",
      "h_val:  tensor(1.1914, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106864144556404\n",
      "loss: 0.13106864144556404\n",
      "h_val:  tensor(1.1915, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106862165034772\n",
      "loss: 0.13106862165034772\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106859875450427\n",
      "loss: 0.13106859875450427\n",
      "h_val:  tensor(1.1919, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310685656294188\n",
      "loss: 0.1310685656294188\n",
      "h_val:  tensor(1.1921, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106856510647974\n",
      "loss: 0.13106856510647974\n",
      "h_val:  tensor(1.1920, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106855251490904\n",
      "loss: 0.13106855251490904\n",
      "h_val:  tensor(1.1920, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106854223559647\n",
      "loss: 0.13106854223559647\n",
      "h_val:  tensor(1.1920, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106852288449256\n",
      "loss: 0.13106852288449256\n",
      "h_val:  tensor(1.1920, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310685023206739\n",
      "loss: 0.1310685023206739\n",
      "h_val:  tensor(1.1920, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106847708568645\n",
      "loss: 0.13106847708568645\n",
      "h_val:  tensor(1.1921, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106842276506384\n",
      "loss: 0.13106842276506384\n",
      "h_val:  tensor(1.1922, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106839734702644\n",
      "loss: 0.13106839734702644\n",
      "h_val:  tensor(1.1923, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106835503003814\n",
      "loss: 0.13106835503003814\n",
      "h_val:  tensor(1.1925, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.131068326820198\n",
      "loss: 0.131068326820198\n",
      "h_val:  tensor(1.1924, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106829762772618\n",
      "loss: 0.13106829762772618\n",
      "h_val:  tensor(1.1924, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106826938200733\n",
      "loss: 0.13106826938200733\n",
      "h_val:  tensor(1.1925, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106824104127776\n",
      "loss: 0.13106824104127776\n",
      "h_val:  tensor(1.1927, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106818287967775\n",
      "loss: 0.13106818287967775\n",
      "h_val:  tensor(1.1929, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106812478610322\n",
      "loss: 0.13106812478610322\n",
      "h_val:  tensor(1.1930, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310680760618637\n",
      "loss: 0.1310680760618637\n",
      "h_val:  tensor(1.1932, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106803294618102\n",
      "loss: 0.13106803294618102\n",
      "h_val:  tensor(1.1932, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310680210350149\n",
      "loss: 0.1310680210350149\n",
      "h_val:  tensor(1.1932, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106800937308855\n",
      "loss: 0.13106800937308855\n",
      "h_val:  tensor(1.1933, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106799756882792\n",
      "loss: 0.13106799756882792\n",
      "h_val:  tensor(1.1933, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106799081827725\n",
      "loss: 0.13106799081827725\n",
      "h_val:  tensor(1.1934, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310679607667005\n",
      "loss: 0.1310679607667005\n",
      "h_val:  tensor(1.1937, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106791275507057\n",
      "loss: 0.13106791275507057\n",
      "h_val:  tensor(1.1939, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310678542547441\n",
      "loss: 0.1310678542547441\n",
      "h_val:  tensor(1.1943, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106780238876478\n",
      "loss: 0.13106780238876478\n",
      "h_val:  tensor(1.1942, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106775998712353\n",
      "loss: 0.13106775998712353\n",
      "h_val:  tensor(1.1942, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106774273277377\n",
      "loss: 0.13106774273277377\n",
      "h_val:  tensor(1.1941, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310677195823196\n",
      "loss: 0.1310677195823196\n",
      "h_val:  tensor(1.1942, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106769519566935\n",
      "loss: 0.13106769519566935\n",
      "h_val:  tensor(1.1943, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106767025411567\n",
      "loss: 0.13106767025411567\n",
      "h_val:  tensor(1.1945, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106757142982425\n",
      "loss: 0.13106757142982425\n",
      "h_val:  tensor(1.1943, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106765987740907\n",
      "loss: 0.13106765987740907\n",
      "h_val:  tensor(1.1944, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106752893654872\n",
      "loss: 0.13106752893654872\n",
      "h_val:  tensor(1.1944, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106747296829158\n",
      "loss: 0.13106747296829158\n",
      "h_val:  tensor(1.1943, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106743804403376\n",
      "loss: 0.13106743804403376\n",
      "h_val:  tensor(1.1942, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106741660976076\n",
      "loss: 0.13106741660976076\n",
      "h_val:  tensor(1.1942, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310673863367766\n",
      "loss: 0.1310673863367766\n",
      "h_val:  tensor(1.1943, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310673566849681\n",
      "loss: 0.1310673566849681\n",
      "h_val:  tensor(1.1945, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106732341886104\n",
      "loss: 0.13106732341886104\n",
      "h_val:  tensor(1.1947, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106726503770066\n",
      "loss: 0.13106726503770066\n",
      "h_val:  tensor(1.1953, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106728084455505\n",
      "loss: 0.13106728084455505\n",
      "h_val:  tensor(1.1949, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106721516312234\n",
      "loss: 0.13106721516312234\n",
      "h_val:  tensor(1.1951, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106714369438907\n",
      "loss: 0.13106714369438907\n",
      "h_val:  tensor(1.1952, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106710553927892\n",
      "loss: 0.13106710553927892\n",
      "h_val:  tensor(1.1951, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106707248017746\n",
      "loss: 0.13106707248017746\n",
      "h_val:  tensor(1.1953, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106705158253\n",
      "loss: 0.13106705158253\n",
      "h_val:  tensor(1.1953, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310670208427251\n",
      "loss: 0.1310670208427251\n",
      "h_val:  tensor(1.1953, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106698994247398\n",
      "loss: 0.13106698994247398\n",
      "h_val:  tensor(1.1954, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106696707219775\n",
      "loss: 0.13106696707219775\n",
      "h_val:  tensor(1.1955, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106690827642506\n",
      "loss: 0.13106690827642506\n",
      "h_val:  tensor(1.1957, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106683565726482\n",
      "loss: 0.13106683565726482\n",
      "h_val:  tensor(1.1958, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310667810647231\n",
      "loss: 0.1310667810647231\n",
      "h_val:  tensor(1.1958, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106671101635095\n",
      "loss: 0.13106671101635095\n",
      "h_val:  tensor(1.1958, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310666637703736\n",
      "loss: 0.1310666637703736\n",
      "h_val:  tensor(1.1958, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.131066616785851\n",
      "loss: 0.131066616785851\n",
      "h_val:  tensor(1.1957, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106657815515474\n",
      "loss: 0.13106657815515474\n",
      "h_val:  tensor(1.1957, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106655861083896\n",
      "loss: 0.13106655861083896\n",
      "h_val:  tensor(1.1957, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106653872415702\n",
      "loss: 0.13106653872415702\n",
      "h_val:  tensor(1.1956, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106651667856117\n",
      "loss: 0.13106651667856117\n",
      "h_val:  tensor(1.1956, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310664980395658\n",
      "loss: 0.1310664980395658\n",
      "h_val:  tensor(1.1954, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106653593329937\n",
      "loss: 0.13106653593329937\n",
      "h_val:  tensor(1.1955, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106648624818776\n",
      "loss: 0.13106648624818776\n",
      "h_val:  tensor(1.1955, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106646815974443\n",
      "loss: 0.13106646815974443\n",
      "h_val:  tensor(1.1955, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106644753641306\n",
      "loss: 0.13106644753641306\n",
      "h_val:  tensor(1.1955, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106643319786815\n",
      "loss: 0.13106643319786815\n",
      "h_val:  tensor(1.1955, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106643238046603\n",
      "loss: 0.13106643238046603\n",
      "h_val:  tensor(1.1955, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106642684875408\n",
      "loss: 0.13106642684875408\n",
      "h_val:  tensor(1.1955, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106641713192485\n",
      "loss: 0.13106641713192485\n",
      "h_val:  tensor(1.1955, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310664062090937\n",
      "loss: 0.1310664062090937\n",
      "h_val:  tensor(1.1956, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106639300758283\n",
      "loss: 0.13106639300758283\n",
      "h_val:  tensor(1.1957, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106638347161814\n",
      "loss: 0.13106638347161814\n",
      "h_val:  tensor(1.1957, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106637062182214\n",
      "loss: 0.13106637062182214\n",
      "h_val:  tensor(1.1958, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106635233988745\n",
      "loss: 0.13106635233988745\n",
      "h_val:  tensor(1.1959, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106633493442835\n",
      "loss: 0.13106633493442835\n",
      "h_val:  tensor(1.1961, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106631625159312\n",
      "loss: 0.13106631625159312\n",
      "h_val:  tensor(1.1962, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106629196474492\n",
      "loss: 0.13106629196474492\n",
      "h_val:  tensor(1.1962, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310662791052054\n",
      "loss: 0.1310662791052054\n",
      "h_val:  tensor(1.1963, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106625691921248\n",
      "loss: 0.13106625691921248\n",
      "h_val:  tensor(1.1964, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106623418875782\n",
      "loss: 0.13106623418875782\n",
      "h_val:  tensor(1.1966, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106621374317753\n",
      "loss: 0.13106621374317753\n",
      "h_val:  tensor(1.1967, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106618373381118\n",
      "loss: 0.13106618373381118\n",
      "h_val:  tensor(1.1967, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106616516155004\n",
      "loss: 0.13106616516155004\n",
      "h_val:  tensor(1.1967, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106613333940134\n",
      "loss: 0.13106613333940134\n",
      "h_val:  tensor(1.1968, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106608393139074\n",
      "loss: 0.13106608393139074\n",
      "h_val:  tensor(1.1969, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106603023140623\n",
      "loss: 0.13106603023140623\n",
      "h_val:  tensor(1.1970, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310659821100998\n",
      "loss: 0.1310659821100998\n",
      "h_val:  tensor(1.1970, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106595986927655\n",
      "loss: 0.13106595986927655\n",
      "h_val:  tensor(1.1973, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106596359682743\n",
      "loss: 0.13106596359682743\n",
      "h_val:  tensor(1.1972, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106593524863355\n",
      "loss: 0.13106593524863355\n",
      "h_val:  tensor(1.1973, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106589656918433\n",
      "loss: 0.13106589656918433\n",
      "h_val:  tensor(1.1974, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.131065869028365\n",
      "loss: 0.131065869028365\n",
      "h_val:  tensor(1.1975, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106584119179765\n",
      "loss: 0.13106584119179765\n",
      "h_val:  tensor(1.1976, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310658166998641\n",
      "loss: 0.1310658166998641\n",
      "h_val:  tensor(1.1976, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106579878461466\n",
      "loss: 0.13106579878461466\n",
      "h_val:  tensor(1.1977, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106577825948929\n",
      "loss: 0.13106577825948929\n",
      "h_val:  tensor(1.1978, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310657655389825\n",
      "loss: 0.1310657655389825\n",
      "h_val:  tensor(1.1978, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106575346076732\n",
      "loss: 0.13106575346076732\n",
      "h_val:  tensor(1.1978, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310657397249492\n",
      "loss: 0.1310657397249492\n",
      "h_val:  tensor(1.1979, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106573299805838\n",
      "loss: 0.13106573299805838\n",
      "h_val:  tensor(1.1979, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106572814567574\n",
      "loss: 0.13106572814567574\n",
      "h_val:  tensor(1.1979, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310657186587847\n",
      "loss: 0.1310657186587847\n",
      "h_val:  tensor(1.1979, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310657127713791\n",
      "loss: 0.1310657127713791\n",
      "h_val:  tensor(1.1979, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106569109069244\n",
      "loss: 0.13106569109069244\n",
      "h_val:  tensor(1.1978, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106565921858299\n",
      "loss: 0.13106565921858299\n",
      "h_val:  tensor(1.1978, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106561879140569\n",
      "loss: 0.13106561879140569\n",
      "h_val:  tensor(1.1978, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106558903954146\n",
      "loss: 0.13106558903954146\n",
      "h_val:  tensor(1.1977, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106556418631843\n",
      "loss: 0.13106556418631843\n",
      "h_val:  tensor(1.1977, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106555065029069\n",
      "loss: 0.13106555065029069\n",
      "h_val:  tensor(1.1977, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106552833878976\n",
      "loss: 0.13106552833878976\n",
      "h_val:  tensor(1.1978, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310655192377719\n",
      "loss: 0.1310655192377719\n",
      "h_val:  tensor(1.1978, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106550318485272\n",
      "loss: 0.13106550318485272\n",
      "h_val:  tensor(1.1978, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310654807969869\n",
      "loss: 0.1310654807969869\n",
      "h_val:  tensor(1.1978, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106545628465718\n",
      "loss: 0.13106545628465718\n",
      "h_val:  tensor(1.1979, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106541568793628\n",
      "loss: 0.13106541568793628\n",
      "h_val:  tensor(1.1982, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106571473382472\n",
      "loss: 0.13106571473382472\n",
      "h_val:  tensor(1.1979, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310654048327689\n",
      "loss: 0.1310654048327689\n",
      "h_val:  tensor(1.1980, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106537013756908\n",
      "loss: 0.13106537013756908\n",
      "h_val:  tensor(1.1980, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310653416749065\n",
      "loss: 0.1310653416749065\n",
      "h_val:  tensor(1.1980, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106531360045728\n",
      "loss: 0.13106531360045728\n",
      "h_val:  tensor(1.1980, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106529349044818\n",
      "loss: 0.13106529349044818\n",
      "h_val:  tensor(1.1980, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106526925534795\n",
      "loss: 0.13106526925534795\n",
      "h_val:  tensor(1.1980, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310652029546532\n",
      "loss: 0.1310652029546532\n",
      "h_val:  tensor(1.1980, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310651605567569\n",
      "loss: 0.1310651605567569\n",
      "h_val:  tensor(1.1977, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106519348846601\n",
      "loss: 0.13106519348846601\n",
      "h_val:  tensor(1.1979, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106513158245725\n",
      "loss: 0.13106513158245725\n",
      "h_val:  tensor(1.1979, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310650919134052\n",
      "loss: 0.1310650919134052\n",
      "h_val:  tensor(1.1979, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106507222305006\n",
      "loss: 0.13106507222305006\n",
      "h_val:  tensor(1.1979, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106504644257025\n",
      "loss: 0.13106504644257025\n",
      "h_val:  tensor(1.1979, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106503142104411\n",
      "loss: 0.13106503142104411\n",
      "h_val:  tensor(1.1979, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.131065003942816\n",
      "loss: 0.131065003942816\n",
      "h_val:  tensor(1.1979, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106495586254208\n",
      "loss: 0.13106495586254208\n",
      "h_val:  tensor(1.1980, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106491126541464\n",
      "loss: 0.13106491126541464\n",
      "h_val:  tensor(1.1981, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106482914091672\n",
      "loss: 0.13106482914091672\n",
      "h_val:  tensor(1.1982, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106483727971488\n",
      "loss: 0.13106483727971488\n",
      "h_val:  tensor(1.1982, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310647672997302\n",
      "loss: 0.1310647672997302\n",
      "h_val:  tensor(1.1982, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310646931741939\n",
      "loss: 0.1310646931741939\n",
      "h_val:  tensor(1.1982, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106463014733757\n",
      "loss: 0.13106463014733757\n",
      "h_val:  tensor(1.1982, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106459761195763\n",
      "loss: 0.13106459761195763\n",
      "h_val:  tensor(1.1981, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106456083226597\n",
      "loss: 0.13106456083226597\n",
      "h_val:  tensor(1.1980, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310645177247276\n",
      "loss: 0.1310645177247276\n",
      "h_val:  tensor(1.1979, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106447572761226\n",
      "loss: 0.13106447572761226\n",
      "h_val:  tensor(1.1977, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106443708020402\n",
      "loss: 0.13106443708020402\n",
      "h_val:  tensor(1.1976, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310643740527421\n",
      "loss: 0.1310643740527421\n",
      "h_val:  tensor(1.1974, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106432209338958\n",
      "loss: 0.13106432209338958\n",
      "h_val:  tensor(1.1972, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310642659131317\n",
      "loss: 0.1310642659131317\n",
      "h_val:  tensor(1.1969, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106419681704354\n",
      "loss: 0.13106419681704354\n",
      "h_val:  tensor(1.1964, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310642060724089\n",
      "loss: 0.1310642060724089\n",
      "h_val:  tensor(1.1967, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106415177066263\n",
      "loss: 0.13106415177066263\n",
      "h_val:  tensor(1.1965, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106410084479508\n",
      "loss: 0.13106410084479508\n",
      "h_val:  tensor(1.1965, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106407120310623\n",
      "loss: 0.13106407120310623\n",
      "h_val:  tensor(1.1965, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310640365256663\n",
      "loss: 0.1310640365256663\n",
      "h_val:  tensor(1.1964, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106399306495226\n",
      "loss: 0.13106399306495226\n",
      "h_val:  tensor(1.1964, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310639485116905\n",
      "loss: 0.1310639485116905\n",
      "h_val:  tensor(1.1962, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106385607582044\n",
      "loss: 0.13106385607582044\n",
      "h_val:  tensor(1.1961, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310637971552716\n",
      "loss: 0.1310637971552716\n",
      "h_val:  tensor(1.1962, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106371022804988\n",
      "loss: 0.13106371022804988\n",
      "h_val:  tensor(1.1963, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106362302513405\n",
      "loss: 0.13106362302513405\n",
      "h_val:  tensor(1.1965, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106355534619002\n",
      "loss: 0.13106355534619002\n",
      "h_val:  tensor(1.1968, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106345836241512\n",
      "loss: 0.13106345836241512\n",
      "h_val:  tensor(1.1970, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310636733965366\n",
      "loss: 0.1310636733965366\n",
      "h_val:  tensor(1.1968, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106343326779257\n",
      "loss: 0.13106343326779257\n",
      "h_val:  tensor(1.1969, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106339903746134\n",
      "loss: 0.13106339903746134\n",
      "h_val:  tensor(1.1969, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106336125361773\n",
      "loss: 0.13106336125361773\n",
      "h_val:  tensor(1.1969, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106325670362445\n",
      "loss: 0.13106325670362445\n",
      "h_val:  tensor(1.1967, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106324702510397\n",
      "loss: 0.13106324702510397\n",
      "h_val:  tensor(1.1968, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106319359214116\n",
      "loss: 0.13106319359214116\n",
      "h_val:  tensor(1.1967, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106311319940891\n",
      "loss: 0.13106311319940891\n",
      "h_val:  tensor(1.1964, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106296840826112\n",
      "loss: 0.13106296840826112\n",
      "h_val:  tensor(1.1963, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310628599035686\n",
      "loss: 0.1310628599035686\n",
      "h_val:  tensor(1.1959, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310627157921689\n",
      "loss: 0.1310627157921689\n",
      "h_val:  tensor(1.1960, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106259293820258\n",
      "loss: 0.13106259293820258\n",
      "h_val:  tensor(1.1960, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106254017965896\n",
      "loss: 0.13106254017965896\n",
      "h_val:  tensor(1.1961, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106249012246374\n",
      "loss: 0.13106249012246374\n",
      "h_val:  tensor(1.1962, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310624139238862\n",
      "loss: 0.1310624139238862\n",
      "h_val:  tensor(1.1963, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310622466165577\n",
      "loss: 0.1310622466165577\n",
      "h_val:  tensor(1.1963, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106210894587308\n",
      "loss: 0.13106210894587308\n",
      "h_val:  tensor(1.1962, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106194199463314\n",
      "loss: 0.13106194199463314\n",
      "h_val:  tensor(1.1961, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106181264522068\n",
      "loss: 0.13106181264522068\n",
      "h_val:  tensor(1.1960, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106172195536625\n",
      "loss: 0.13106172195536625\n",
      "h_val:  tensor(1.1958, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310616203620877\n",
      "loss: 0.1310616203620877\n",
      "h_val:  tensor(1.1959, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106152817579572\n",
      "loss: 0.13106152817579572\n",
      "h_val:  tensor(1.1960, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310614797497882\n",
      "loss: 0.1310614797497882\n",
      "h_val:  tensor(1.1962, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106132738085902\n",
      "loss: 0.13106132738085902\n",
      "h_val:  tensor(1.1964, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106123015863955\n",
      "loss: 0.13106123015863955\n",
      "h_val:  tensor(1.1965, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106110256146253\n",
      "loss: 0.13106110256146253\n",
      "h_val:  tensor(1.1966, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106101250931138\n",
      "loss: 0.13106101250931138\n",
      "h_val:  tensor(1.1967, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106088603166477\n",
      "loss: 0.13106088603166477\n",
      "h_val:  tensor(1.1970, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106077205812064\n",
      "loss: 0.13106077205812064\n",
      "h_val:  tensor(1.1972, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106056879803243\n",
      "loss: 0.13106056879803243\n",
      "h_val:  tensor(1.1973, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106039615352832\n",
      "loss: 0.13106039615352832\n",
      "h_val:  tensor(1.1975, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310602820837712\n",
      "loss: 0.1310602820837712\n",
      "h_val:  tensor(1.1974, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106035657412815\n",
      "loss: 0.13106035657412815\n",
      "h_val:  tensor(1.1974, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310602403426705\n",
      "loss: 0.1310602403426705\n",
      "h_val:  tensor(1.1975, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106018395780217\n",
      "loss: 0.13106018395780217\n",
      "h_val:  tensor(1.1975, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310601253590375\n",
      "loss: 0.1310601253590375\n",
      "h_val:  tensor(1.1974, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106006995872857\n",
      "loss: 0.13106006995872857\n",
      "h_val:  tensor(1.1973, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13106001094962474\n",
      "loss: 0.13106001094962474\n",
      "h_val:  tensor(1.1971, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105991512327694\n",
      "loss: 0.13105991512327694\n",
      "h_val:  tensor(1.1970, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105982723467005\n",
      "loss: 0.13105982723467005\n",
      "h_val:  tensor(1.1973, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310598371145718\n",
      "loss: 0.1310598371145718\n",
      "h_val:  tensor(1.1971, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105978175578697\n",
      "loss: 0.13105978175578697\n",
      "h_val:  tensor(1.1972, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105972451967393\n",
      "loss: 0.13105972451967393\n",
      "h_val:  tensor(1.1972, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310596956616915\n",
      "loss: 0.1310596956616915\n",
      "h_val:  tensor(1.1974, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105965789333424\n",
      "loss: 0.13105965789333424\n",
      "h_val:  tensor(1.1975, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105962167345547\n",
      "loss: 0.13105962167345547\n",
      "h_val:  tensor(1.1975, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105958397186854\n",
      "loss: 0.13105958397186854\n",
      "h_val:  tensor(1.1977, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105951602757082\n",
      "loss: 0.13105951602757082\n",
      "h_val:  tensor(1.1979, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105945687371243\n",
      "loss: 0.13105945687371243\n",
      "h_val:  tensor(1.1981, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105939870831415\n",
      "loss: 0.13105939870831415\n",
      "h_val:  tensor(1.1985, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105930342256777\n",
      "loss: 0.13105930342256777\n",
      "h_val:  tensor(1.1989, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105929002912423\n",
      "loss: 0.13105929002912423\n",
      "h_val:  tensor(1.1989, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105922426476832\n",
      "loss: 0.13105922426476832\n",
      "h_val:  tensor(1.1989, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105920008472077\n",
      "loss: 0.13105920008472077\n",
      "h_val:  tensor(1.1990, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310591808349761\n",
      "loss: 0.1310591808349761\n",
      "h_val:  tensor(1.1991, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310591505787827\n",
      "loss: 0.1310591505787827\n",
      "h_val:  tensor(1.1993, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105911017473149\n",
      "loss: 0.13105911017473149\n",
      "h_val:  tensor(1.1993, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105904306459243\n",
      "loss: 0.13105904306459243\n",
      "h_val:  tensor(1.1993, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105901159682798\n",
      "loss: 0.13105901159682798\n",
      "h_val:  tensor(1.1992, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105896117360533\n",
      "loss: 0.13105896117360533\n",
      "h_val:  tensor(1.1992, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310589228961099\n",
      "loss: 0.1310589228961099\n",
      "h_val:  tensor(1.1994, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105887365040977\n",
      "loss: 0.13105887365040977\n",
      "h_val:  tensor(1.1996, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105881092065827\n",
      "loss: 0.13105881092065827\n",
      "h_val:  tensor(1.1997, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310587826407132\n",
      "loss: 0.1310587826407132\n",
      "h_val:  tensor(1.1998, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105875642419473\n",
      "loss: 0.13105875642419473\n",
      "h_val:  tensor(1.1997, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105873044455577\n",
      "loss: 0.13105873044455577\n",
      "h_val:  tensor(1.1997, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105871714009731\n",
      "loss: 0.13105871714009731\n",
      "h_val:  tensor(1.1996, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105869579021007\n",
      "loss: 0.13105869579021007\n",
      "h_val:  tensor(1.1996, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105875804761843\n",
      "loss: 0.13105875804761843\n",
      "h_val:  tensor(1.1996, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105868030986703\n",
      "loss: 0.13105868030986703\n",
      "h_val:  tensor(1.1995, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310586432767775\n",
      "loss: 0.1310586432767775\n",
      "h_val:  tensor(1.1995, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105857456352785\n",
      "loss: 0.13105857456352785\n",
      "h_val:  tensor(1.1994, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105848660490713\n",
      "loss: 0.13105848660490713\n",
      "h_val:  tensor(1.1994, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105839052878052\n",
      "loss: 0.13105839052878052\n",
      "h_val:  tensor(1.1995, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105831503203583\n",
      "loss: 0.13105831503203583\n",
      "h_val:  tensor(1.1993, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105814443862204\n",
      "loss: 0.13105814443862204\n",
      "h_val:  tensor(1.1994, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310580690171458\n",
      "loss: 0.1310580690171458\n",
      "h_val:  tensor(1.1992, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310579615008642\n",
      "loss: 0.1310579615008642\n",
      "h_val:  tensor(1.1990, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105787287729564\n",
      "loss: 0.13105787287729564\n",
      "h_val:  tensor(1.1988, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310578035592784\n",
      "loss: 0.1310578035592784\n",
      "h_val:  tensor(1.1987, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105772105676086\n",
      "loss: 0.13105772105676086\n",
      "h_val:  tensor(1.1985, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105760312920042\n",
      "loss: 0.13105760312920042\n",
      "h_val:  tensor(1.1982, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310574385679944\n",
      "loss: 0.1310574385679944\n",
      "h_val:  tensor(1.1976, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105722483323012\n",
      "loss: 0.13105722483323012\n",
      "h_val:  tensor(1.1967, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105709393261358\n",
      "loss: 0.13105709393261358\n",
      "h_val:  tensor(1.1967, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105693182855202\n",
      "loss: 0.13105693182855202\n",
      "h_val:  tensor(1.1965, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105686282526618\n",
      "loss: 0.13105686282526618\n",
      "h_val:  tensor(1.1963, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105682538664412\n",
      "loss: 0.13105682538664412\n",
      "h_val:  tensor(1.1958, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105677920890654\n",
      "loss: 0.13105677920890654\n",
      "h_val:  tensor(1.1956, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.131056716913481\n",
      "loss: 0.131056716913481\n",
      "h_val:  tensor(1.1956, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105668163850276\n",
      "loss: 0.13105668163850276\n",
      "h_val:  tensor(1.1955, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105664760006722\n",
      "loss: 0.13105664760006722\n",
      "h_val:  tensor(1.1954, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105655925372878\n",
      "loss: 0.13105655925372878\n",
      "h_val:  tensor(1.1947, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105653923456087\n",
      "loss: 0.13105653923456087\n",
      "h_val:  tensor(1.1949, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105641939353588\n",
      "loss: 0.13105641939353588\n",
      "h_val:  tensor(1.1949, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310563925973643\n",
      "loss: 0.1310563925973643\n",
      "h_val:  tensor(1.1948, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105634814000322\n",
      "loss: 0.13105634814000322\n",
      "h_val:  tensor(1.1946, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105631269000428\n",
      "loss: 0.13105631269000428\n",
      "h_val:  tensor(1.1945, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310562697711054\n",
      "loss: 0.1310562697711054\n",
      "h_val:  tensor(1.1945, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105625187674463\n",
      "loss: 0.13105625187674463\n",
      "h_val:  tensor(1.1945, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310562362917303\n",
      "loss: 0.1310562362917303\n",
      "h_val:  tensor(1.1945, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310562164277559\n",
      "loss: 0.1310562164277559\n",
      "h_val:  tensor(1.1945, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105618948452746\n",
      "loss: 0.13105618948452746\n",
      "h_val:  tensor(1.1946, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105615932049733\n",
      "loss: 0.13105615932049733\n",
      "h_val:  tensor(1.1946, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310561163046432\n",
      "loss: 0.1310561163046432\n",
      "h_val:  tensor(1.1946, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310560851652159\n",
      "loss: 0.1310560851652159\n",
      "h_val:  tensor(1.1947, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310559855046467\n",
      "loss: 0.1310559855046467\n",
      "h_val:  tensor(1.1949, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105593655756564\n",
      "loss: 0.13105593655756564\n",
      "h_val:  tensor(1.1948, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105586558973817\n",
      "loss: 0.13105586558973817\n",
      "h_val:  tensor(1.1948, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105584563408879\n",
      "loss: 0.13105584563408879\n",
      "h_val:  tensor(1.1948, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105582767608032\n",
      "loss: 0.13105582767608032\n",
      "h_val:  tensor(1.1949, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105580000607442\n",
      "loss: 0.13105580000607442\n",
      "h_val:  tensor(1.1949, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105576123208576\n",
      "loss: 0.13105576123208576\n",
      "h_val:  tensor(1.1949, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105572342815627\n",
      "loss: 0.13105572342815627\n",
      "h_val:  tensor(1.1950, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310556768985482\n",
      "loss: 0.1310556768985482\n",
      "h_val:  tensor(1.1951, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105563055451222\n",
      "loss: 0.13105563055451222\n",
      "h_val:  tensor(1.1952, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310555510106482\n",
      "loss: 0.1310555510106482\n",
      "h_val:  tensor(1.1954, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105549605069058\n",
      "loss: 0.13105549605069058\n",
      "h_val:  tensor(1.1953, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105545740996566\n",
      "loss: 0.13105545740996566\n",
      "h_val:  tensor(1.1953, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105542085184294\n",
      "loss: 0.13105542085184294\n",
      "h_val:  tensor(1.1953, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105538194508698\n",
      "loss: 0.13105538194508698\n",
      "h_val:  tensor(1.1953, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105534925826512\n",
      "loss: 0.13105534925826512\n",
      "h_val:  tensor(1.1953, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105532855804575\n",
      "loss: 0.13105532855804575\n",
      "h_val:  tensor(1.1954, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105528444770934\n",
      "loss: 0.13105528444770934\n",
      "h_val:  tensor(1.1956, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310552197586972\n",
      "loss: 0.1310552197586972\n",
      "h_val:  tensor(1.1957, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105513026082602\n",
      "loss: 0.13105513026082602\n",
      "h_val:  tensor(1.1959, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105505030074727\n",
      "loss: 0.13105505030074727\n",
      "h_val:  tensor(1.1959, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105500340976436\n",
      "loss: 0.13105500340976436\n",
      "h_val:  tensor(1.1959, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310549661205918\n",
      "loss: 0.1310549661205918\n",
      "h_val:  tensor(1.1959, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310549061288225\n",
      "loss: 0.1310549061288225\n",
      "h_val:  tensor(1.1960, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105483535866483\n",
      "loss: 0.13105483535866483\n",
      "h_val:  tensor(1.1960, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105481349532933\n",
      "loss: 0.13105481349532933\n",
      "h_val:  tensor(1.1960, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105478889914457\n",
      "loss: 0.13105478889914457\n",
      "h_val:  tensor(1.1959, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105474409850276\n",
      "loss: 0.13105474409850276\n",
      "h_val:  tensor(1.1959, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105470700069835\n",
      "loss: 0.13105470700069835\n",
      "h_val:  tensor(1.1958, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310546496862026\n",
      "loss: 0.1310546496862026\n",
      "h_val:  tensor(1.1957, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310546206040369\n",
      "loss: 0.1310546206040369\n",
      "h_val:  tensor(1.1956, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105458201108733\n",
      "loss: 0.13105458201108733\n",
      "h_val:  tensor(1.1952, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105462586684893\n",
      "loss: 0.13105462586684893\n",
      "h_val:  tensor(1.1954, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105454643408623\n",
      "loss: 0.13105454643408623\n",
      "h_val:  tensor(1.1953, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.131054490716496\n",
      "loss: 0.131054490716496\n",
      "h_val:  tensor(1.1953, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105443284308732\n",
      "loss: 0.13105443284308732\n",
      "h_val:  tensor(1.1953, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310544071219223\n",
      "loss: 0.1310544071219223\n",
      "h_val:  tensor(1.1953, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105438307877307\n",
      "loss: 0.13105438307877307\n",
      "h_val:  tensor(1.1953, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105436024130113\n",
      "loss: 0.13105436024130113\n",
      "h_val:  tensor(1.1955, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105431281215185\n",
      "loss: 0.13105431281215185\n",
      "h_val:  tensor(1.1956, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105428772784197\n",
      "loss: 0.13105428772784197\n",
      "h_val:  tensor(1.1956, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105424861602763\n",
      "loss: 0.13105424861602763\n",
      "h_val:  tensor(1.1956, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310541659677185\n",
      "loss: 0.1310541659677185\n",
      "h_val:  tensor(1.1956, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105412214012246\n",
      "loss: 0.13105412214012246\n",
      "h_val:  tensor(1.1956, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310540876899201\n",
      "loss: 0.1310540876899201\n",
      "h_val:  tensor(1.1956, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105403913061398\n",
      "loss: 0.13105403913061398\n",
      "h_val:  tensor(1.1956, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105401864539026\n",
      "loss: 0.13105401864539026\n",
      "h_val:  tensor(1.1956, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310539803033669\n",
      "loss: 0.1310539803033669\n",
      "h_val:  tensor(1.1956, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310539483979967\n",
      "loss: 0.1310539483979967\n",
      "h_val:  tensor(1.1955, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105388900328985\n",
      "loss: 0.13105388900328985\n",
      "h_val:  tensor(1.1953, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105376354639958\n",
      "loss: 0.13105376354639958\n",
      "h_val:  tensor(1.1950, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105366018209152\n",
      "loss: 0.13105366018209152\n",
      "h_val:  tensor(1.1945, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105343965785307\n",
      "loss: 0.13105343965785307\n",
      "h_val:  tensor(1.1936, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105383776453436\n",
      "loss: 0.13105383776453436\n",
      "h_val:  tensor(1.1943, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105336100998083\n",
      "loss: 0.13105336100998083\n",
      "h_val:  tensor(1.1940, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310531909475554\n",
      "loss: 0.1310531909475554\n",
      "h_val:  tensor(1.1939, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310530857720413\n",
      "loss: 0.1310530857720413\n",
      "h_val:  tensor(1.1938, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310529818509892\n",
      "loss: 0.1310529818509892\n",
      "h_val:  tensor(1.1938, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105292106574\n",
      "loss: 0.13105292106574\n",
      "h_val:  tensor(1.1938, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105288669728848\n",
      "loss: 0.13105288669728848\n",
      "h_val:  tensor(1.1937, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105285337565697\n",
      "loss: 0.13105285337565697\n",
      "h_val:  tensor(1.1937, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105282434556967\n",
      "loss: 0.13105282434556967\n",
      "h_val:  tensor(1.1935, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105278846867618\n",
      "loss: 0.13105278846867618\n",
      "h_val:  tensor(1.1934, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105272166377666\n",
      "loss: 0.13105272166377666\n",
      "h_val:  tensor(1.1934, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105267821885297\n",
      "loss: 0.13105267821885297\n",
      "h_val:  tensor(1.1933, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105263182299015\n",
      "loss: 0.13105263182299015\n",
      "h_val:  tensor(1.1933, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105256666588366\n",
      "loss: 0.13105256666588366\n",
      "h_val:  tensor(1.1934, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105273285503508\n",
      "loss: 0.13105273285503508\n",
      "h_val:  tensor(1.1933, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105253111118295\n",
      "loss: 0.13105253111118295\n",
      "h_val:  tensor(1.1934, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105246087476202\n",
      "loss: 0.13105246087476202\n",
      "h_val:  tensor(1.1934, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310524208958253\n",
      "loss: 0.1310524208958253\n",
      "h_val:  tensor(1.1934, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310523789248105\n",
      "loss: 0.1310523789248105\n",
      "h_val:  tensor(1.1935, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105233095734503\n",
      "loss: 0.13105233095734503\n",
      "h_val:  tensor(1.1934, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105226303068548\n",
      "loss: 0.13105226303068548\n",
      "h_val:  tensor(1.1932, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105205640838463\n",
      "loss: 0.13105205640838463\n",
      "h_val:  tensor(1.1930, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105216758447283\n",
      "loss: 0.13105216758447283\n",
      "h_val:  tensor(1.1931, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105197724696796\n",
      "loss: 0.13105197724696796\n",
      "h_val:  tensor(1.1927, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310518224997417\n",
      "loss: 0.1310518224997417\n",
      "h_val:  tensor(1.1924, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310517629364726\n",
      "loss: 0.1310517629364726\n",
      "h_val:  tensor(1.1923, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310517162007942\n",
      "loss: 0.1310517162007942\n",
      "h_val:  tensor(1.1923, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105169296254027\n",
      "loss: 0.13105169296254027\n",
      "h_val:  tensor(1.1922, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105166633314524\n",
      "loss: 0.13105166633314524\n",
      "h_val:  tensor(1.1921, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105163600101608\n",
      "loss: 0.13105163600101608\n",
      "h_val:  tensor(1.1921, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105162118974711\n",
      "loss: 0.13105162118974711\n",
      "h_val:  tensor(1.1920, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105158225153904\n",
      "loss: 0.13105158225153904\n",
      "h_val:  tensor(1.1918, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105152618969543\n",
      "loss: 0.13105152618969543\n",
      "h_val:  tensor(1.1914, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310516231521905\n",
      "loss: 0.1310516231521905\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310515048658956\n",
      "loss: 0.1310515048658956\n",
      "h_val:  tensor(1.1916, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105146832581993\n",
      "loss: 0.13105146832581993\n",
      "h_val:  tensor(1.1915, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105144593820497\n",
      "loss: 0.13105144593820497\n",
      "h_val:  tensor(1.1915, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105141744852217\n",
      "loss: 0.13105141744852217\n",
      "h_val:  tensor(1.1915, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105142531673594\n",
      "loss: 0.13105142531673594\n",
      "h_val:  tensor(1.1915, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310514062814125\n",
      "loss: 0.1310514062814125\n",
      "h_val:  tensor(1.1915, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105138689903106\n",
      "loss: 0.13105138689903106\n",
      "h_val:  tensor(1.1915, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105136524446262\n",
      "loss: 0.13105136524446262\n",
      "h_val:  tensor(1.1916, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310513366039451\n",
      "loss: 0.1310513366039451\n",
      "h_val:  tensor(1.1916, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105129670815965\n",
      "loss: 0.13105129670815965\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310512397749328\n",
      "loss: 0.1310512397749328\n",
      "h_val:  tensor(1.1919, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105132026768773\n",
      "loss: 0.13105132026768773\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310512101750729\n",
      "loss: 0.1310512101750729\n",
      "h_val:  tensor(1.1918, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310511634630206\n",
      "loss: 0.1310511634630206\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105113663592619\n",
      "loss: 0.13105113663592619\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105111063552807\n",
      "loss: 0.13105111063552807\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105109016093391\n",
      "loss: 0.13105109016093391\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310510666082737\n",
      "loss: 0.1310510666082737\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105104199415368\n",
      "loss: 0.13105104199415368\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105101115716147\n",
      "loss: 0.13105101115716147\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105095728289987\n",
      "loss: 0.13105095728289987\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105093448711028\n",
      "loss: 0.13105093448711028\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105090610645484\n",
      "loss: 0.13105090610645484\n",
      "h_val:  tensor(1.1918, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105089816785184\n",
      "loss: 0.13105089816785184\n",
      "h_val:  tensor(1.1918, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105088103883428\n",
      "loss: 0.13105088103883428\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310508755733931\n",
      "loss: 0.1310508755733931\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105086973801633\n",
      "loss: 0.13105086973801633\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310508592681189\n",
      "loss: 0.1310508592681189\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105085982634362\n",
      "loss: 0.13105085982634362\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105085022655086\n",
      "loss: 0.13105085022655086\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105083718628066\n",
      "loss: 0.13105083718628066\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310508258365516\n",
      "loss: 0.1310508258365516\n",
      "h_val:  tensor(1.1918, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105081726124432\n",
      "loss: 0.13105081726124432\n",
      "h_val:  tensor(1.1918, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310508051072148\n",
      "loss: 0.1310508051072148\n",
      "h_val:  tensor(1.1918, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105079339964407\n",
      "loss: 0.13105079339964407\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105078501594525\n",
      "loss: 0.13105078501594525\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105077479294167\n",
      "loss: 0.13105077479294167\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310507667115992\n",
      "loss: 0.1310507667115992\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105074837040293\n",
      "loss: 0.13105074837040293\n",
      "h_val:  tensor(1.1916, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105073511773305\n",
      "loss: 0.13105073511773305\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105071638648358\n",
      "loss: 0.13105071638648358\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105070167144714\n",
      "loss: 0.13105070167144714\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310506896510087\n",
      "loss: 0.1310506896510087\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310506674873298\n",
      "loss: 0.1310506674873298\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105063799140718\n",
      "loss: 0.13105063799140718\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105061255342712\n",
      "loss: 0.13105061255342712\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310505909770764\n",
      "loss: 0.1310505909770764\n",
      "h_val:  tensor(1.1917, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105057987644317\n",
      "loss: 0.13105057987644317\n",
      "h_val:  tensor(1.1916, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105056704739015\n",
      "loss: 0.13105056704739015\n",
      "h_val:  tensor(1.1916, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105053413011153\n",
      "loss: 0.13105053413011153\n",
      "h_val:  tensor(1.1915, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310505217677622\n",
      "loss: 0.1310505217677622\n",
      "h_val:  tensor(1.1914, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105047660503677\n",
      "loss: 0.13105047660503677\n",
      "h_val:  tensor(1.1912, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105046540353218\n",
      "loss: 0.13105046540353218\n",
      "h_val:  tensor(1.1913, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310504241077265\n",
      "loss: 0.1310504241077265\n",
      "h_val:  tensor(1.1913, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105041013661567\n",
      "loss: 0.13105041013661567\n",
      "h_val:  tensor(1.1913, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105038221913196\n",
      "loss: 0.13105038221913196\n",
      "h_val:  tensor(1.1912, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105034295692833\n",
      "loss: 0.13105034295692833\n",
      "h_val:  tensor(1.1911, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105032317452817\n",
      "loss: 0.13105032317452817\n",
      "h_val:  tensor(1.1911, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310502830505479\n",
      "loss: 0.1310502830505479\n",
      "h_val:  tensor(1.1910, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105026333905267\n",
      "loss: 0.13105026333905267\n",
      "h_val:  tensor(1.1909, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105023742269553\n",
      "loss: 0.13105023742269553\n",
      "h_val:  tensor(1.1909, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105023593229023\n",
      "loss: 0.13105023593229023\n",
      "h_val:  tensor(1.1909, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105021913674433\n",
      "loss: 0.13105021913674433\n",
      "h_val:  tensor(1.1909, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310501894276376\n",
      "loss: 0.1310501894276376\n",
      "h_val:  tensor(1.1909, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105016738075778\n",
      "loss: 0.13105016738075778\n",
      "h_val:  tensor(1.1909, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105014162588985\n",
      "loss: 0.13105014162588985\n",
      "h_val:  tensor(1.1910, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105011661751942\n",
      "loss: 0.13105011661751942\n",
      "h_val:  tensor(1.1910, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105009494976266\n",
      "loss: 0.13105009494976266\n",
      "h_val:  tensor(1.1910, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105007900179325\n",
      "loss: 0.13105007900179325\n",
      "h_val:  tensor(1.1909, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105006361588387\n",
      "loss: 0.13105006361588387\n",
      "h_val:  tensor(1.1909, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105005117504065\n",
      "loss: 0.13105005117504065\n",
      "h_val:  tensor(1.1907, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105008894694922\n",
      "loss: 0.13105008894694922\n",
      "h_val:  tensor(1.1908, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105004092846187\n",
      "loss: 0.13105004092846187\n",
      "h_val:  tensor(1.1908, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105002122816545\n",
      "loss: 0.13105002122816545\n",
      "h_val:  tensor(1.1908, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13105001068233815\n",
      "loss: 0.13105001068233815\n",
      "h_val:  tensor(1.1908, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310499961386129\n",
      "loss: 0.1310499961386129\n",
      "h_val:  tensor(1.1908, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310499877449609\n",
      "loss: 0.1310499877449609\n",
      "h_val:  tensor(1.1908, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13104997690867165\n",
      "loss: 0.13104997690867165\n",
      "h_val:  tensor(1.1907, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310499664720966\n",
      "loss: 0.1310499664720966\n",
      "h_val:  tensor(1.1907, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310499582053075\n",
      "loss: 0.1310499582053075\n",
      "h_val:  tensor(1.1906, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13104995643496306\n",
      "loss: 0.13104995643496306\n",
      "h_val:  tensor(1.1906, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.1310499487902003\n",
      "loss: 0.1310499487902003\n",
      "h_val:  tensor(1.1905, grad_fn=<SubBackward0>)\n",
      "L_risk: 0.13104993714287977\n",
      "loss: 0.13104993714287977\n",
      "h_new:  1.1905448369818608\n",
      "[[0.68975498 0.31543224]\n",
      " [0.69062332 0.6404554 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x218f80d5e50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(8)\n",
    "x = np.random.uniform(low=-10, high=10, size=100)\n",
    "\n",
    "b = np.random.normal(0,1, 100) \n",
    "\n",
    "\n",
    "\n",
    "y = [x**3 + x + b for x, b in zip(x, b)]\n",
    "\n",
    "y = np.array(y)  # your y data here\n",
    "\n",
    "# Step 1: Determine the current range of y\n",
    "y_min, y_max = y.min(), y.max()\n",
    "\n",
    "# Step 2: No explicit step needed here, as we use the formula directly in step 3\n",
    "\n",
    "# Step 3: Apply the scaling transformation\n",
    "y_scaled = ((y - y_min) * (10 - (-10))) / (y_max - y_min) + (-10)\n",
    "\n",
    "X = np.column_stack((x, y_scaled))\n",
    "\n",
    "n = X.shape[0]\n",
    "d = X.shape[1]\n",
    "\n",
    "\n",
    "model = NotearsRKHS(n, d, \"gaussian\")\n",
    "output = learning(model, X)\n",
    "\n",
    "y_hat = output[:, 1].detach().numpy()\n",
    "plt.figure(figsize=(10, 6))  # Optional: specifies the figure size\n",
    "plt.scatter(x, y_hat, label='y2', color='red', marker='s') \n",
    "plt.scatter(x, y_scaled, label='y1', color='blue', marker='o')  # Plot x vs. y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NOTEARS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
