{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lbfgsb_scipy import LBFGSBScipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n   def main():\\n        torch.set_default_dtype(torch.double)\\n        np.set_printoptions(precision=3)\\n\\n        import utils as ut\\n        ut.set_random_seed(123)\\n\\n        n, d, s0, graph_type, sem_type = 200, 5, 9, 'ER', 'mim'\\n        B_true = ut.simulate_dag(d, s0, graph_type)\\n        np.savetxt('W_true.csv', B_true, delimiter=',')\\n\\n        X = ut.simulate_nonlinear_sem(B_true, n, sem_type)\\n        np.savetxt('X.csv', X, delimiter=',')\\n\\n        model = NotearsRKHS(2, 3, x)\\n        W_est = RKHS_nonlinear(model, x, lambda1=0.01, mu=0.01)\\n        assert ut.is_dag(W_est)\\n        np.savetxt('W_est.csv', W_est, delimiter=',')\\n        acc = ut.count_accuracy(B_true, W_est != 0)\\n        print(acc)\\n\\n\\n   if __name__ == '__main__':\\n        main()\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NotearsRKHS(nn.Module):\n",
    "   \"\"\"n: number of samples, d: num variables\"\"\"\n",
    "   def __init__(self, n, d, x):\n",
    "      super(NotearsRKHS, self).__init__()\n",
    "      self.d = d\n",
    "      self.n = n\n",
    "      self.x = x\n",
    "      # initialize alpha\n",
    "      self.fc1_pos = nn.Linear(n, d, bias=False)  # fc1_pos.weight = [d ,n], fc1_pos(x) = x @ fc1_pos.weight^T\n",
    "      self.fc1_neg = nn.Linear(n, d, bias=False)\n",
    "      #self.fc1_pos.weight.bounds = self._bounds()\n",
    "      #self.fc1_neg.weight.bounds = self._bounds()\n",
    "      nn.init.zeros_(self.fc1_pos.weight)\n",
    "      nn.init.zeros_(self.fc1_neg.weight)\n",
    "      self.I = torch.eye(self.d)\n",
    "   def gaussian_kernel(self, x, y, gamma=1): # [d, 1] * [d, 1] -> [1, 1]\n",
    "    #distance_squared = torch.norm(x-y, dim=1, keepdim=True)**2\n",
    "      distance_squared = torch.norm(x-y, dim=-1)**2\n",
    "      return torch.exp(-distance_squared / (gamma**2))\n",
    "\n",
    "   def forward(self,x): #[n,d] -> [n, d], forward(x)_{l,j} = estimation of x_j at lth observation \n",
    "      #K = torch.zeros((self.n, self.n))\n",
    "      x1 = x.unsqueeze(-1)\n",
    "      x1 = x1.repeat(1, 1, 2).transpose(1, 2)\n",
    "      x2 = x.unsqueeze(0)\n",
    "      x2 = x.repeat(2, 1, 1)\n",
    "      K = self.gaussian_kernel(x1, x2, gamma = 1)\n",
    "      #for i in range(self.n):\n",
    "      #   for l in range(self.n):\n",
    "      #      K[i,l] = self.kernel(x[i,:], x[l,:])\n",
    "      #print(K.shape)\n",
    "      output = self.fc1_pos(K) - self.fc1_neg(K)\n",
    "      #output = output.t()\n",
    "      return output\n",
    "   \n",
    "   def L_risk(self, output, x, penalty): # [1, 1]\n",
    "      squared_loss = 0.5 / self.n * torch.sum((output - x) ** 2)\n",
    "      x1 = x.unsqueeze(-1)\n",
    "      x1 = x1.repeat(1, 1, 2).transpose(1, 2)\n",
    "      x2 = x.unsqueeze(0)\n",
    "      x2 = x.repeat(2, 1, 1)\n",
    "      K = self.gaussian_kernel(x1, x2, gamma = 1) #[n,n]\n",
    "      fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight\n",
    "      A = torch.matmul(torch.matmul(fc1_weight, K), fc1_weight.t())\n",
    "      diagonal = torch.sum(torch.diag(A))\n",
    "      regularized = penalty*diagonal \n",
    "      loss = squared_loss + regularized\n",
    "      return loss\n",
    "   \n",
    "   def fc1_to_adj(self) -> np.ndarray: # [d, d]\n",
    "      \"\"\"Get W from fc1 weights, take L2 norm of the gradient\"\"\"\n",
    "      # get [n, n, d] gradient matrix of the kernel\n",
    "      # Initialize a tensor to store gradients\n",
    "      grad_storage = torch.zeros(self.n, self.n, self.d)\n",
    "      # Compute gradients\n",
    "      for i in range(self.n):\n",
    "         for l in range(self.n):\n",
    "            x_i = self.x[i,:]\n",
    "            x_l = self.x[l,:]\n",
    "            x_l.retain_grad()\n",
    "            K_il = self.gaussian_kernel(x_i, x_l, gamma=1)\n",
    "            # Zero out gradients in x, otherwise it accumulate the gradients\n",
    "            if x_l.grad is not None:\n",
    "                  x_l.grad.zero_()\n",
    "            # Select individual scalar element from y\n",
    "            # Perform backward pass on the scalar element\n",
    "            K_il.backward()\n",
    "            # Store the computed gradients\n",
    "            grad_storage[i, l] = x_l.grad.clone() #[n, n, d]\n",
    "      fc1_weight = self.fc1_pos.weight - self.fc1_neg.weight # [d, n]\n",
    "      grad_storage = grad_storage.transpose(0, 2)  # Shape [d, n, n]\n",
    "      grad_storage = grad_storage.unsqueeze(-1) #[d ,n, 1]\n",
    "      weight = torch.matmul(fc1_weight, grad_storage).squeeze(-1) # [d, n, d]\n",
    "      weight = torch.sum(weight ** 2, dim = 1)/self.n # [d, d]\n",
    "      return weight\n",
    "    \n",
    "   def h_func(self, t: float = 1.0) -> torch.Tensor: #[1, 1]\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        t : float, optional\n",
    "            Controls the domain of M-matrices, by default 1.0\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            A scalar value of the log-det acyclicity function :math:`h(\\Theta)`.\n",
    "        \"\"\"\n",
    "        weight = self.fc1_to_adj()\n",
    "        sign, logabsdet = torch.linalg.slogdet(t*self.I - weight)\n",
    "        h = -sign * logabsdet + self.d * np.log(t)\n",
    "        return h\n",
    "   \n",
    "   def dual_ascent_step(model, x, lambda1, mu, rho, h, rho_max):\n",
    "       \"\"\"Perform one step of dual ascent in augmented Lagrangian.\"\"\"\n",
    "       h_new = None\n",
    "       optimizer = LBFGSBScipy(model.parameters())\n",
    "       while rho < rho_max:\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            x_hat = model(x)\n",
    "            loss = L_risk(x_hat, x, penalty = lambda1)\n",
    "            h_val = model.h_func()\n",
    "            penalty = 0.5 * rho * h_val * h_val + mu * h_val\n",
    "            #l2_reg = 0.5 * lambda2 * model.l2_reg()\n",
    "            #l1_reg = lambda1 * model.fc1_l1_reg()\n",
    "            primal_obj = loss + penalty \n",
    "            primal_obj.backward()\n",
    "            return primal_obj\n",
    "        optimizer.step(closure)  # NOTE: updates model in-place\n",
    "        with torch.no_grad():\n",
    "            h_new = model.h_func().item()\n",
    "        if h_new > 0.25 * h:\n",
    "            rho *= 10\n",
    "        else:\n",
    "            break\n",
    "       mu += rho * h_new\n",
    "       return rho, mu, h_new\n",
    "\n",
    "   def RKHS_nonlinear(model: nn.Module,\n",
    "                        x: torch.Tensor,\n",
    "                        lambda1: float = 0.,\n",
    "                        mu: float = 0.,\n",
    "                        max_iter: int = 100,\n",
    "                        h_tol: float = 1e-8,\n",
    "                        rho_max: float = 1e+16,\n",
    "                        w_threshold: float = 0.3):\n",
    "        rho, mu, h = 1.0, 0.0, np.inf\n",
    "        for _ in range(max_iter):\n",
    "            rho, mu, h = dual_ascent_step(model, x, lambda1, mu,\n",
    "                                            rho, h, rho_max)\n",
    "            if h <= h_tol or rho >= rho_max:\n",
    "                break\n",
    "        W_est = model.fc1_to_adj()\n",
    "        W_est[np.abs(W_est) < w_threshold] = 0\n",
    "        return W_est\n",
    "# simulation\n",
    "\"\"\"\n",
    "   def main():\n",
    "        torch.set_default_dtype(torch.double)\n",
    "        np.set_printoptions(precision=3)\n",
    "\n",
    "        import utils as ut\n",
    "        ut.set_random_seed(123)\n",
    "\n",
    "        n, d, s0, graph_type, sem_type = 200, 5, 9, 'ER', 'mim'\n",
    "        B_true = ut.simulate_dag(d, s0, graph_type)\n",
    "        np.savetxt('W_true.csv', B_true, delimiter=',')\n",
    "\n",
    "        X = ut.simulate_nonlinear_sem(B_true, n, sem_type)\n",
    "        np.savetxt('X.csv', X, delimiter=',')\n",
    "\n",
    "        model = NotearsRKHS(2, 3, x)\n",
    "        W_est = RKHS_nonlinear(model, x, lambda1=0.01, mu=0.01)\n",
    "        assert ut.is_dag(W_est)\n",
    "        np.savetxt('W_est.csv', W_est, delimiter=',')\n",
    "        acc = ut.count_accuracy(B_true, W_est != 0)\n",
    "        print(acc)\n",
    "\n",
    "\n",
    "   if __name__ == '__main__':\n",
    "        main()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x, y, gamma): # [d, 1] * [d, 1] -> [1, 1]\n",
    "    #distance_squared = torch.norm(x-y, dim=1, keepdim=True)**2\n",
    "    distance_squared = torch.norm(x-y, dim=-1)**2\n",
    "    return torch.exp(-distance_squared / (gamma**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "NotearsRKHS.dual_ascent_step() missing 6 required positional arguments: 'lambda1', 'lambda2', 'rho', 'mu', 'h', and 'rho_max'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m RKHS \u001b[38;5;241m=\u001b[39m NotearsRKHS(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, x)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#output = RKHS.forward(x)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#RKHS.L_risk(output, x, 1)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#RKHS.fc1_to_adj()\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#RKHS.h_func()\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mRKHS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdual_ascent_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: NotearsRKHS.dual_ascent_step() missing 6 required positional arguments: 'lambda1', 'lambda2', 'rho', 'mu', 'h', and 'rho_max'"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 7]], dtype=torch.float32, requires_grad=True)\n",
    "RKHS = NotearsRKHS(2, 3, x)\n",
    "#output = RKHS.forward(x)\n",
    "#RKHS.L_risk(output, x, 1)\n",
    "#RKHS.fc1_to_adj()\n",
    "#RKHS.h_func()\n",
    "RKHS.dual_ascent_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4724, -0.4724, -0.4724])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.tensor([1.0, 2.0, 3.0], requires_grad=False)\n",
    "x2 = torch.tensor([1.5, 2.5, 3.5], requires_grad=True)\n",
    "\n",
    "\n",
    "# Apply the gaussian_kernel function\n",
    "output = gaussian_kernel(x1, x2, gamma=1)\n",
    "\n",
    "# Perform backward pass\n",
    "output.backward()\n",
    "\n",
    "# Access the gradient with respect to x2\n",
    "print(x2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.zeros(3,3)\n",
    "I = torch.eye(3)\n",
    "t = 1\n",
    "sign, logabsdet = torch.linalg.slogdet(t*I - weight)\n",
    "h = -sign * logabsdet + 3 * np.log(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NOTEARS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
